{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/madhurpt/Library/Application Support/sagemaker/config.yaml\n",
      "config file current -> configs/config-claude-models.yml, None\n",
      "Loaded config: {'general': {'name': 'fmbench-claude', 'model_name': 'claude'}, 'aws': {'region': 'us-east-1', 'sagemaker_execution_role': 'arn:aws:iam::121797993273:user/ab3', 'bucket': 'sagemaker-fmbench-write-121797993273'}, 'dir_paths': {'data_prefix': 'data', 'prompts_prefix': 'prompts', 'all_prompts_file': 'all_prompts.csv', 'metrics_dir': 'metrics', 'models_dir': 'models', 'metadata_dir': 'metadata'}, 's3_read_data': {'read_bucket': 'sagemaker-fmbench-read-121797993273', 'scripts_prefix': 'scripts', 'script_files': ['hf_token.txt'], 'source_data_prefix': 'source_data', 'tokenizer_prefix': 'tokenizer', 'prompt_template_dir': 'prompt_template', 'prompt_template_file': 'prompt_template.txt'}, 'run_steps': {'0_setup.ipynb': True, '1_generate_data.ipynb': True, '2_deploy_model.ipynb': False, '3_run_inference.ipynb': True, '4_model_metric_analysis.ipynb': True, '5_cleanup.ipynb': True}, 'datasets': {'prompt_template_keys': ['input', 'context'], 'filters': [{'language': 'en', 'min_length_in_tokens': 1, 'max_length_in_tokens': 500, 'payload_file': 'payload_en_1-500.jsonl'}, {'language': 'en', 'min_length_in_tokens': 500, 'max_length_in_tokens': 1000, 'payload_file': 'payload_en_500-1000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 1000, 'max_length_in_tokens': 2000, 'payload_file': 'payload_en_1000-2000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 2000, 'max_length_in_tokens': 3000, 'payload_file': 'payload_en_2000-3000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 3000, 'max_length_in_tokens': 4000, 'payload_file': 'payload_en_3000-4000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 305, 'max_length_in_tokens': 3997, 'payload_file': 'payload_en_305-3997.jsonl'}]}, 'metrics': {'dataset_of_interest': 'en_3000-4000', 'weights': {'price_per_tx_wt': 0.65, 'latenct_wt': 0.35}}, 'pricing': {'ml.m5.xlarge': 0.23, 'ml.g5.xlarge': 1.006, 'ml.g5.2xlarge': 1.212, 'ml.g5.12xlarge': 7.09, 'ml.g5.24xlarge': 10.18, 'ml.g5.48xlarge': 20.36, 'ml.inf2.24xlarge': 7.79, 'ml.inf2.48xlarge': 15.58, 'ml.p4d.24xlarge': 37.688, 'ml.p3.2xlarge': 3.825, 'Claudev3-Haiku-ODT': 0, 'ClaudeV2-ODT': 0, 'ClaudeV2:1-ODT': 0, 'ClaudeInstant-ODT': [{'input-per-1k-tokens': 0.0008}, {'output-per-1k-tokens': 0.0024}], 'titan-emb-text-ODT': 0, 'titan-text-lite-ODT': 0, 'titan-text-express-ODT': 0, 'Sonnet-ODT': 0, 'Mistral-ODT': 0, 'Mixtral-ODT': 0, 'Llama13b-ODT': 0, 'Llama70b-ODT': 0, 'AI21mid-ODT': 0, 'AI21ultra-ODT': 0, 'CohereCommand-ODT': 0, 'CohereCommLight-ODT': 0}, 'inference_parameters': {'ContentType': 'application/json', 'Accept': 'application/json'}, 'experiments': [{'name': 'claude-instant-v1', 'model_id': 'anthropic.claude-instant-v1', 'model_version': '*', 'model_name': 'claude-instant-v1', 'ep_name': 'anthropic.claude-instant-v1', 'instance_type': 'ClaudeInstant-ODT', 'image_uri': None, 'deploy': False, 'instance_count': 1, 'deployment_script': None, 'inference_script': 'bedrock_predictor.py', 'payload_files': ['payload_en_3000-4000.jsonl'], 'concurrency_levels': [1], 'env': None}], 'report': {'per_inference_request_file': 'per_inference_request_results.csv', 'all_metrics_file': 'all_metrics.csv', 'txn_count_for_showing_cost': 100000, 'v_shift_w_single_instance': 0.025, 'v_shift_w_gt_one_instance': 0.025}}\n",
      "CustomTokenizer, based on HF transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files found in S3 Bucket: 'sagemaker-fmbench-read-121797993273' with Prefix: 'tokenizer'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import botocore\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import * \n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "import importlib.resources as pkg_resources\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from fmbench.scripts.bedrock_predictor import BedrockPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:{\n",
      "  \"general\": {\n",
      "    \"name\": \"fmbench-claude\",\n",
      "    \"model_name\": \"claude\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::121797993273:user/ab3\",\n",
      "    \"bucket\": \"sagemaker-fmbench-write-121797993273\"\n",
      "  },\n",
      "  \"dir_paths\": {\n",
      "    \"data_prefix\": \"data\",\n",
      "    \"prompts_prefix\": \"prompts\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\",\n",
      "    \"metrics_dir\": \"metrics\",\n",
      "    \"models_dir\": \"models\",\n",
      "    \"metadata_dir\": \"metadata\"\n",
      "  },\n",
      "  \"s3_read_data\": {\n",
      "    \"read_bucket\": \"sagemaker-fmbench-read-121797993273\",\n",
      "    \"scripts_prefix\": \"scripts\",\n",
      "    \"script_files\": [\n",
      "      \"hf_token.txt\"\n",
      "    ],\n",
      "    \"source_data_prefix\": \"source_data\",\n",
      "    \"tokenizer_prefix\": \"tokenizer\",\n",
      "    \"prompt_template_dir\": \"prompt_template\",\n",
      "    \"prompt_template_file\": \"prompt_template.txt\"\n",
      "  },\n",
      "  \"run_steps\": {\n",
      "    \"0_setup.ipynb\": true,\n",
      "    \"1_generate_data.ipynb\": true,\n",
      "    \"2_deploy_model.ipynb\": false,\n",
      "    \"3_run_inference.ipynb\": true,\n",
      "    \"4_model_metric_analysis.ipynb\": true,\n",
      "    \"5_cleanup.ipynb\": true\n",
      "  },\n",
      "  \"datasets\": {\n",
      "    \"prompt_template_keys\": [\n",
      "      \"input\",\n",
      "      \"context\"\n",
      "    ],\n",
      "    \"filters\": [\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1,\n",
      "        \"max_length_in_tokens\": 500,\n",
      "        \"payload_file\": \"payload_en_1-500.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 500,\n",
      "        \"max_length_in_tokens\": 1000,\n",
      "        \"payload_file\": \"payload_en_500-1000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1000,\n",
      "        \"max_length_in_tokens\": 2000,\n",
      "        \"payload_file\": \"payload_en_1000-2000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 2000,\n",
      "        \"max_length_in_tokens\": 3000,\n",
      "        \"payload_file\": \"payload_en_2000-3000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 3000,\n",
      "        \"max_length_in_tokens\": 4000,\n",
      "        \"payload_file\": \"payload_en_3000-4000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 305,\n",
      "        \"max_length_in_tokens\": 3997,\n",
      "        \"payload_file\": \"payload_en_305-3997.jsonl\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_3000-4000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.m5.xlarge\": 0.23,\n",
      "    \"ml.g5.xlarge\": 1.006,\n",
      "    \"ml.g5.2xlarge\": 1.212,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688,\n",
      "    \"ml.p3.2xlarge\": 3.825,\n",
      "    \"Claudev3-Haiku-ODT\": 0,\n",
      "    \"ClaudeV2-ODT\": 0,\n",
      "    \"ClaudeV2:1-ODT\": 0,\n",
      "    \"ClaudeInstant-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      }\n",
      "    ],\n",
      "    \"titan-emb-text-ODT\": 0,\n",
      "    \"titan-text-lite-ODT\": 0,\n",
      "    \"titan-text-express-ODT\": 0,\n",
      "    \"Sonnet-ODT\": 0,\n",
      "    \"Mistral-ODT\": 0,\n",
      "    \"Mixtral-ODT\": 0,\n",
      "    \"Llama13b-ODT\": 0,\n",
      "    \"Llama70b-ODT\": 0,\n",
      "    \"AI21mid-ODT\": 0,\n",
      "    \"AI21ultra-ODT\": 0,\n",
      "    \"CohereCommand-ODT\": 0,\n",
      "    \"CohereCommLight-ODT\": 0\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"ContentType\": \"application/json\",\n",
      "    \"Accept\": \"application/json\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"claude-instant-v1\",\n",
      "      \"model_id\": \"anthropic.claude-instant-v1\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"claude-instant-v1\",\n",
      "      \"ep_name\": \"anthropic.claude-instant-v1\",\n",
      "      \"instance_type\": \"ClaudeInstant-ODT\",\n",
      "      \"image_uri\": null,\n",
      "      \"deploy\": false,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": null,\n",
      "      \"inference_script\": \"bedrock_predictor.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"env\": null\n",
      "    }\n",
      "  ],\n",
      "  \"report\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\",\n",
      "    \"txn_count_for_showing_cost\": 100000,\n",
      "    \"v_shift_w_single_instance\": 0.025,\n",
      "    \"v_shift_w_gt_one_instance\": 0.025\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## getting access to the s3 bucket where endpoints.json for different models resides\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Key fmbench-claude-ab3/data/models/endpoints.json not found in bucket sagemaker-fmbench-write-121797993273. Using an empty list for endpoints for bedrock models.\n"
     ]
    }
   ],
   "source": [
    "## Refer to the file path for the endpoint\n",
    "## getting the endpoint as an s3 object from the deployed path\n",
    "try:\n",
    "    endpoint_info_list = json.loads(get_s3_object(config['aws']['bucket'], ENDPOINT_LIST_PATH))\n",
    "    logger.info(f\"found information for {len(endpoint_info_list)} endpoints in bucket={config['aws']['bucket']}, key={ENDPOINT_LIST_PATH}\")\n",
    "    logger.info(json.dumps(endpoint_info_list, indent=2))\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "        logger.warning(f\"Key {ENDPOINT_LIST_PATH} not found in bucket {config['aws']['bucket']}. Using an empty list for endpoints for bedrock models.\")\n",
    "        endpoint_info_list = []\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:there are 0 deployed endpoint(s), endpoint_name_list->[]\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    latency = 0\n",
    "\n",
    "    ## initializing completion tokens\n",
    "    completion_tokens = None\n",
    "    prompt_tokens = None\n",
    "\n",
    "    try:\n",
    "        # get inference      \n",
    "        resp = predictor.get_prediction(payload) \n",
    "        logger.info(f\"response={resp}\")\n",
    "        response_json = resp['response_json']\n",
    "        logger.info(f\"response_json={response_json}\")\n",
    "        latency = resp['latency']\n",
    "        logger.info(f\"latency={latency}\")\n",
    "        prompt_tokens = resp['prompt_tokens']\n",
    "        logger.info(f\"prompt_tokens={prompt_tokens}\")\n",
    "        completion_tokens = resp['completion_tokens']\n",
    "        logger.info(f\"completion_tokens={completion_tokens}\")\n",
    "\n",
    "        # Assign the generated_text value to completion\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens, \n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        \n",
    "        # logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, response={json.dumps(response, indent=2)}, latency={latency:.2f}\")\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, prompt_tokens = {prompt_tokens}, completion_tokens={completion_tokens}, latency={latency:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asynchronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: Dict, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## Iterate through the endpoint information to fetch the endpoint name\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    logger.info(f\"endpoint info found is: {ep_info}\")\n",
    "\n",
    "    if ep_info != []:\n",
    "        ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "        logger.info(f\"experiment name={experiment['name']}, ep_name={ep_name}\")\n",
    "    \n",
    "    elif ep_info == []:\n",
    "        ep_name = experiment['ep_name']\n",
    "        logger.info(f\"experiment name={experiment['name']}, custom bring your own ep_name={ep_name}\")\n",
    "\n",
    "    else:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "\n",
    "    # create predictor objects\n",
    "    # Proceed with deployment as before\n",
    "    # Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "    scripts_dir = Path(pkg_resources.files('fmbench'), 'scripts')\n",
    "    logger.info(f\"Using fmbench.scripts directory: {scripts_dir}\")\n",
    "\n",
    "    # Ensure the scripts directory exists\n",
    "    scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    module_name = Path(experiment['inference_script']).stem\n",
    "    logger.info(f\"script provided for inference from this model is --> {module_name}\")\n",
    "    script_path = scripts_dir / f\"{module_name}.py\"\n",
    "    logger.info(f\"script path is --> {script_path}\")\n",
    "\n",
    "    # Check and proceed with local script\n",
    "    if not script_path.exists():\n",
    "        logger.error(f\"script {script_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Deploying using local code: {script_path}\")\n",
    "\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(script_path))\n",
    "    inference_module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = inference_module\n",
    "    spec.loader.exec_module(inference_module)\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return inference_module.create_predictor(ep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Construct the full S3 file path\n",
    "        s3_file_path = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        logger.info(f\"s3 path where the payload files are being read from -> {s3_file_path}\")\n",
    "\n",
    "        # Read the payload file from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=config['aws']['bucket'], Key=s3_file_path)\n",
    "            payload_file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            # Create a payload list by processing each line\n",
    "            payload_list = [create_payload_dict(jline, experiment) for jline in payload_file_content.splitlines()]\n",
    "            logger.info(f\"read from s3://{config['aws']['bucket']}/{s3_file_path}, contains {len(payload_list)} lines\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file from S3: {e}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Current time recorded while running this experiment is 2024-03-22 00:45:28.225084+00:00..... deployed models are going to start inferences...\n",
      "INFO:__main__:endpoint info found is: []\n",
      "INFO:__main__:experiment name=claude-instant-v1, custom bring your own ep_name=anthropic.claude-instant-v1\n",
      "INFO:__main__:Using fmbench.scripts directory: /Users/madhurpt/Desktop/foundation-model-benchmarking-tool-5/src/fmbench/scripts\n",
      "INFO:__main__:script provided for inference from this model is --> bedrock_predictor\n",
      "INFO:__main__:script path is --> /Users/madhurpt/Desktop/foundation-model-benchmarking-tool-5/src/fmbench/scripts/bedrock_predictor.py\n",
      "INFO:__main__:Deploying using local code: /Users/madhurpt/Desktop/foundation-model-benchmarking-tool-5/src/fmbench/scripts/bedrock_predictor.py\n",
      "INFO:bedrock_predictor:__init__ self._predictor=<botocore.client.BedrockRuntime object at 0x28aade390>\n",
      "INFO:__main__:there are 1 combinations of [(1, 'payload_en_3000-4000.jsonl')] to run\n",
      "INFO:__main__:s3 path where the payload files are being read from -> fmbench-claude-ab3/data/prompts/payload_en_3000-4000.jsonl\n",
      "INFO:__main__:read from s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "INFO:__main__:creating combinations for concurrency=1, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "INFO:__main__:after only retaining chunks of length 1, we have 57 chunks, previously we had 57 chunks\n",
      "INFO:__main__:there are 1 for {'name': 'claude-instant-v1', 'model_id': 'anthropic.claude-instant-v1', 'model_version': '*', 'model_name': 'claude-instant-v1', 'ep_name': 'anthropic.claude-instant-v1', 'instance_type': 'ClaudeInstant-ODT', 'image_uri': None, 'deploy': False, 'instance_count': 1, 'deployment_script': None, 'inference_script': 'bedrock_predictor.py', 'payload_files': ['payload_en_3000-4000.jsonl'], 'concurrency_levels': [1], 'env': None}\n",
      "INFO:__main__:e_idx=1/1, chunk_index=1/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3000\n",
      "INFO:bedrock_predictor:Claude completion tokens: 73\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the context passages provided, Indradhanura Chhai was released earlier than The Death of Black King. Indradhanura Chhai is an 1993 Indian Oriya film, while The Death of Black King is a 1971 Czechoslovak film. Since 1993 is later than 1971, The Death of Black King came out earlier than Indradhanura Chhai.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context passages provided, Indradhanura Chhai was released earlier than The Death of Black King. Indradhanura Chhai is an 1993 Indian Oriya film, while The Death of Black King is a 1971 Czechoslovak film. Since 1993 is later than 1971, The Death of Black King came out earlier than Indradhanura Chhai.'}, 'latency': 1.7884314589973656, 'prompt_tokens': 3000, 'completion_tokens': 73}\n",
      "INFO:__main__:response_json={'completion': ' Based on the context passages provided, Indradhanura Chhai was released earlier than The Death of Black King. Indradhanura Chhai is an 1993 Indian Oriya film, while The Death of Black King is a 1971 Czechoslovak film. Since 1993 is later than 1971, The Death of Black King came out earlier than Indradhanura Chhai.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context passages provided, Indradhanura Chhai was released earlier than The Death of Black King. Indradhanura Chhai is an 1993 Indian Oriya film, while The Death of Black King is a 1971 Czechoslovak film. Since 1993 is later than 1971, The Death of Black King came out earlier than Indradhanura Chhai.'}\n",
      "INFO:__main__:latency=1.7884314589973656\n",
      "INFO:__main__:prompt_tokens=3000\n",
      "INFO:__main__:completion_tokens=73\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3000, completion_tokens=73, latency=1.7884\n",
      "INFO:__main__:e_idx=1/1, chunk_index=2/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3896\n",
      "INFO:bedrock_predictor:Claude completion tokens: 67\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the context provided, Peter Rosegger was younger than Ruel Redinger. Peter Rosegger was born on 31 July 1843 in Krieglach, Styria, Austria. Ruel Redinger was born on 31 December 1896 in the United States. So Rosegger would have been younger than Redinger.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, Peter Rosegger was younger than Ruel Redinger. Peter Rosegger was born on 31 July 1843 in Krieglach, Styria, Austria. Ruel Redinger was born on 31 December 1896 in the United States. So Rosegger would have been younger than Redinger.'}, 'latency': 1.9793343749988708, 'prompt_tokens': 3896, 'completion_tokens': 67}\n",
      "INFO:__main__:response_json={'completion': ' Based on the context provided, Peter Rosegger was younger than Ruel Redinger. Peter Rosegger was born on 31 July 1843 in Krieglach, Styria, Austria. Ruel Redinger was born on 31 December 1896 in the United States. So Rosegger would have been younger than Redinger.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, Peter Rosegger was younger than Ruel Redinger. Peter Rosegger was born on 31 July 1843 in Krieglach, Styria, Austria. Ruel Redinger was born on 31 December 1896 in the United States. So Rosegger would have been younger than Redinger.'}\n",
      "INFO:__main__:latency=1.9793343749988708\n",
      "INFO:__main__:prompt_tokens=3896\n",
      "INFO:__main__:completion_tokens=67\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3896, completion_tokens=67, latency=1.9793\n",
      "INFO:__main__:e_idx=1/1, chunk_index=3/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3789\n",
      "INFO:bedrock_predictor:Claude completion tokens: 49\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages provided, Erich Haenisch died later than William Pooley. William Pooley died on August 11, 1929 in West Ham, England. Erich Haenisch died on December 21, 1966 in Stuttgart, Germany.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Erich Haenisch died later than William Pooley. William Pooley died on August 11, 1929 in West Ham, England. Erich Haenisch died on December 21, 1966 in Stuttgart, Germany.'}, 'latency': 1.881439415999921, 'prompt_tokens': 3789, 'completion_tokens': 49}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages provided, Erich Haenisch died later than William Pooley. William Pooley died on August 11, 1929 in West Ham, England. Erich Haenisch died on December 21, 1966 in Stuttgart, Germany.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Erich Haenisch died later than William Pooley. William Pooley died on August 11, 1929 in West Ham, England. Erich Haenisch died on December 21, 1966 in Stuttgart, Germany.'}\n",
      "INFO:__main__:latency=1.881439415999921\n",
      "INFO:__main__:prompt_tokens=3789\n",
      "INFO:__main__:completion_tokens=49\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3789, completion_tokens=49, latency=1.8814\n",
      "INFO:__main__:e_idx=1/1, chunk_index=4/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3450\n",
      "INFO:bedrock_predictor:Claude completion tokens: 41\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided, Thomas E. Noell's father was John William Noell. Passage 5 mentions that John William Noell died in Washington D.C. on March 14, 1863.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Thomas E. Noell's father was John William Noell. Passage 5 mentions that John William Noell died in Washington D.C. on March 14, 1863.\"}, 'latency': 2.207101291001891, 'prompt_tokens': 3450, 'completion_tokens': 41}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided, Thomas E. Noell's father was John William Noell. Passage 5 mentions that John William Noell died in Washington D.C. on March 14, 1863.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Thomas E. Noell's father was John William Noell. Passage 5 mentions that John William Noell died in Washington D.C. on March 14, 1863.\"}\n",
      "INFO:__main__:latency=2.207101291001891\n",
      "INFO:__main__:prompt_tokens=3450\n",
      "INFO:__main__:completion_tokens=41\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3450, completion_tokens=41, latency=2.2071\n",
      "INFO:__main__:e_idx=1/1, chunk_index=5/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3482\n",
      "INFO:bedrock_predictor:Claude completion tokens: 60\n",
      "INFO:__main__:response={'response_json': {'completion': ' No, Ding Yaping and Johann Christian Gustav Lucae are not of the same nationality based on the passages provided. Ding Yaping is identified as a female Chinese and German former international table tennis player, while Johann Christian Gustav Lucae is identified as a German anatomist.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' No, Ding Yaping and Johann Christian Gustav Lucae are not of the same nationality based on the passages provided. Ding Yaping is identified as a female Chinese and German former international table tennis player, while Johann Christian Gustav Lucae is identified as a German anatomist.'}, 'latency': 1.8743640839966247, 'prompt_tokens': 3482, 'completion_tokens': 60}\n",
      "INFO:__main__:response_json={'completion': ' No, Ding Yaping and Johann Christian Gustav Lucae are not of the same nationality based on the passages provided. Ding Yaping is identified as a female Chinese and German former international table tennis player, while Johann Christian Gustav Lucae is identified as a German anatomist.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' No, Ding Yaping and Johann Christian Gustav Lucae are not of the same nationality based on the passages provided. Ding Yaping is identified as a female Chinese and German former international table tennis player, while Johann Christian Gustav Lucae is identified as a German anatomist.'}\n",
      "INFO:__main__:latency=1.8743640839966247\n",
      "INFO:__main__:prompt_tokens=3482\n",
      "INFO:__main__:completion_tokens=60\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3482, completion_tokens=60, latency=1.8744\n",
      "INFO:__main__:e_idx=1/1, chunk_index=6/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3144\n",
      "INFO:bedrock_predictor:Claude completion tokens: 89\n",
      "INFO:__main__:response={'response_json': {'completion': ' According to the passages provided, P. Neelakantan directed the 1961 Tamil film Nallavan Vazhvan. Passage 7 mentions that P. Neelakantan was active as a film director for nearly four decades and gives his dates as 2 October 1916 – 3 September 1992. Therefore, the date of death of the director of Nallavan Vazhvan, P. Neelakantan, was 3 September 1992.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' According to the passages provided, P. Neelakantan directed the 1961 Tamil film Nallavan Vazhvan. Passage 7 mentions that P. Neelakantan was active as a film director for nearly four decades and gives his dates as 2 October 1916 – 3 September 1992. Therefore, the date of death of the director of Nallavan Vazhvan, P. Neelakantan, was 3 September 1992.'}, 'latency': 2.888750958998571, 'prompt_tokens': 3144, 'completion_tokens': 89}\n",
      "INFO:__main__:response_json={'completion': ' According to the passages provided, P. Neelakantan directed the 1961 Tamil film Nallavan Vazhvan. Passage 7 mentions that P. Neelakantan was active as a film director for nearly four decades and gives his dates as 2 October 1916 – 3 September 1992. Therefore, the date of death of the director of Nallavan Vazhvan, P. Neelakantan, was 3 September 1992.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' According to the passages provided, P. Neelakantan directed the 1961 Tamil film Nallavan Vazhvan. Passage 7 mentions that P. Neelakantan was active as a film director for nearly four decades and gives his dates as 2 October 1916 – 3 September 1992. Therefore, the date of death of the director of Nallavan Vazhvan, P. Neelakantan, was 3 September 1992.'}\n",
      "INFO:__main__:latency=2.888750958998571\n",
      "INFO:__main__:prompt_tokens=3144\n",
      "INFO:__main__:completion_tokens=89\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3144, completion_tokens=89, latency=2.8888\n",
      "INFO:__main__:e_idx=1/1, chunk_index=7/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3639\n",
      "INFO:bedrock_predictor:Claude completion tokens: 62\n",
      "INFO:__main__:response={'response_json': {'completion': ' Duilio Coletti, who directed Folgore Division, died on May 22, 1999 at the age of 92. Lesley Selander, who directed Sandflow, died on December 5, 1979 at the age of 79. Therefore, the director of Sandflow, Lesley Selander, died first.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Duilio Coletti, who directed Folgore Division, died on May 22, 1999 at the age of 92. Lesley Selander, who directed Sandflow, died on December 5, 1979 at the age of 79. Therefore, the director of Sandflow, Lesley Selander, died first.'}, 'latency': 4.387397999998939, 'prompt_tokens': 3639, 'completion_tokens': 62}\n",
      "INFO:__main__:response_json={'completion': ' Duilio Coletti, who directed Folgore Division, died on May 22, 1999 at the age of 92. Lesley Selander, who directed Sandflow, died on December 5, 1979 at the age of 79. Therefore, the director of Sandflow, Lesley Selander, died first.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Duilio Coletti, who directed Folgore Division, died on May 22, 1999 at the age of 92. Lesley Selander, who directed Sandflow, died on December 5, 1979 at the age of 79. Therefore, the director of Sandflow, Lesley Selander, died first.'}\n",
      "INFO:__main__:latency=4.387397999998939\n",
      "INFO:__main__:prompt_tokens=3639\n",
      "INFO:__main__:completion_tokens=62\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3639, completion_tokens=62, latency=4.3874\n",
      "INFO:__main__:e_idx=1/1, chunk_index=8/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3014\n",
      "INFO:bedrock_predictor:Claude completion tokens: 45\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, Edmonton/Twin Island Airpark and Mayerthorpe Airport are located in the same country. Both airports are located in Canada, as indicated by their location descriptions that specify the provinces of Alberta for each airport.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Edmonton/Twin Island Airpark and Mayerthorpe Airport are located in the same country. Both airports are located in Canada, as indicated by their location descriptions that specify the provinces of Alberta for each airport.'}, 'latency': 1.5174654169968562, 'prompt_tokens': 3014, 'completion_tokens': 45}\n",
      "INFO:__main__:response_json={'completion': ' Yes, Edmonton/Twin Island Airpark and Mayerthorpe Airport are located in the same country. Both airports are located in Canada, as indicated by their location descriptions that specify the provinces of Alberta for each airport.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Edmonton/Twin Island Airpark and Mayerthorpe Airport are located in the same country. Both airports are located in Canada, as indicated by their location descriptions that specify the provinces of Alberta for each airport.'}\n",
      "INFO:__main__:latency=1.5174654169968562\n",
      "INFO:__main__:prompt_tokens=3014\n",
      "INFO:__main__:completion_tokens=45\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3014, completion_tokens=45, latency=1.5175\n",
      "INFO:__main__:e_idx=1/1, chunk_index=9/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3891\n",
      "INFO:bedrock_predictor:Claude completion tokens: 95\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the information provided:\\n\\n- Full Length LP is the debut album by the punk rock band Guttermouth, released in 1991. \\n\\n- Blow in the Wind is the third album by Me First and the Gimme Gimmes, released in 2001.\\n\\nTherefore, Blow in the Wind was released more recently than Full Length LP, as it came out ten years later in 2001 compared to Full Length LP's 1991 release date.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the information provided:\\n\\n- Full Length LP is the debut album by the punk rock band Guttermouth, released in 1991. \\n\\n- Blow in the Wind is the third album by Me First and the Gimme Gimmes, released in 2001.\\n\\nTherefore, Blow in the Wind was released more recently than Full Length LP, as it came out ten years later in 2001 compared to Full Length LP's 1991 release date.\"}, 'latency': 3.509887042004266, 'prompt_tokens': 3891, 'completion_tokens': 95}\n",
      "INFO:__main__:response_json={'completion': \" Based on the information provided:\\n\\n- Full Length LP is the debut album by the punk rock band Guttermouth, released in 1991. \\n\\n- Blow in the Wind is the third album by Me First and the Gimme Gimmes, released in 2001.\\n\\nTherefore, Blow in the Wind was released more recently than Full Length LP, as it came out ten years later in 2001 compared to Full Length LP's 1991 release date.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the information provided:\\n\\n- Full Length LP is the debut album by the punk rock band Guttermouth, released in 1991. \\n\\n- Blow in the Wind is the third album by Me First and the Gimme Gimmes, released in 2001.\\n\\nTherefore, Blow in the Wind was released more recently than Full Length LP, as it came out ten years later in 2001 compared to Full Length LP's 1991 release date.\"}\n",
      "INFO:__main__:latency=3.509887042004266\n",
      "INFO:__main__:prompt_tokens=3891\n",
      "INFO:__main__:completion_tokens=95\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3891, completion_tokens=95, latency=3.5099\n",
      "INFO:__main__:e_idx=1/1, chunk_index=10/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3575\n",
      "INFO:bedrock_predictor:Claude completion tokens: 99\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the context provided, the Kakwa River and Bighead River are not located in the same country. The Kakwa River is mentioned as being located in western Alberta, Canada, with details about its course through the province. Meanwhile, the Bighead River is described as being located in Grey County, southern Ontario, Canada. Since the passages specify the rivers as being in different provinces or Canada, the Kakwa River and Bighead River are not located in the same country.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, the Kakwa River and Bighead River are not located in the same country. The Kakwa River is mentioned as being located in western Alberta, Canada, with details about its course through the province. Meanwhile, the Bighead River is described as being located in Grey County, southern Ontario, Canada. Since the passages specify the rivers as being in different provinces or Canada, the Kakwa River and Bighead River are not located in the same country.'}, 'latency': 3.6817956250015413, 'prompt_tokens': 3575, 'completion_tokens': 99}\n",
      "INFO:__main__:response_json={'completion': ' Based on the context provided, the Kakwa River and Bighead River are not located in the same country. The Kakwa River is mentioned as being located in western Alberta, Canada, with details about its course through the province. Meanwhile, the Bighead River is described as being located in Grey County, southern Ontario, Canada. Since the passages specify the rivers as being in different provinces or Canada, the Kakwa River and Bighead River are not located in the same country.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, the Kakwa River and Bighead River are not located in the same country. The Kakwa River is mentioned as being located in western Alberta, Canada, with details about its course through the province. Meanwhile, the Bighead River is described as being located in Grey County, southern Ontario, Canada. Since the passages specify the rivers as being in different provinces or Canada, the Kakwa River and Bighead River are not located in the same country.'}\n",
      "INFO:__main__:latency=3.6817956250015413\n",
      "INFO:__main__:prompt_tokens=3575\n",
      "INFO:__main__:completion_tokens=99\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3575, completion_tokens=99, latency=3.6818\n",
      "INFO:__main__:e_idx=1/1, chunk_index=11/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3419\n",
      "INFO:bedrock_predictor:Claude completion tokens: 40\n",
      "INFO:__main__:response={'response_json': {'completion': \" I don't have enough information in the passages to determine whether Vrindavan Lal Verma or Kerem İnan is younger. None of the passages provide the birth years of these two individuals.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" I don't have enough information in the passages to determine whether Vrindavan Lal Verma or Kerem İnan is younger. None of the passages provide the birth years of these two individuals.\"}, 'latency': 3.2328877079999074, 'prompt_tokens': 3419, 'completion_tokens': 40}\n",
      "INFO:__main__:response_json={'completion': \" I don't have enough information in the passages to determine whether Vrindavan Lal Verma or Kerem İnan is younger. None of the passages provide the birth years of these two individuals.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" I don't have enough information in the passages to determine whether Vrindavan Lal Verma or Kerem İnan is younger. None of the passages provide the birth years of these two individuals.\"}\n",
      "INFO:__main__:latency=3.2328877079999074\n",
      "INFO:__main__:prompt_tokens=3419\n",
      "INFO:__main__:completion_tokens=40\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3419, completion_tokens=40, latency=3.2329\n",
      "INFO:__main__:e_idx=1/1, chunk_index=12/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3458\n",
      "INFO:bedrock_predictor:Claude completion tokens: 122\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the context provided, William George Hylton Jolliffe, 4th Baron Hylton's father was Hylton Jolliffe, 3rd Baron Hylton. Passage 2mentions that William George Hylton Jolliffe succeeded his father Hylton Jolliffe as the 4th Baron Hylton, but it does not explicitly state the date of death of Hylton Jolliffe, 3rd Baron Hylton. The passagesdo not contain enough information to determine the date of death of William Jolliffe, 4Th Baron Hylton's father.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the context provided, William George Hylton Jolliffe, 4th Baron Hylton's father was Hylton Jolliffe, 3rd Baron Hylton. Passage 2mentions that William George Hylton Jolliffe succeeded his father Hylton Jolliffe as the 4th Baron Hylton, but it does not explicitly state the date of death of Hylton Jolliffe, 3rd Baron Hylton. The passagesdo not contain enough information to determine the date of death of William Jolliffe, 4Th Baron Hylton's father.\"}, 'latency': 4.307974709001428, 'prompt_tokens': 3458, 'completion_tokens': 122}\n",
      "INFO:__main__:response_json={'completion': \" Based on the context provided, William George Hylton Jolliffe, 4th Baron Hylton's father was Hylton Jolliffe, 3rd Baron Hylton. Passage 2mentions that William George Hylton Jolliffe succeeded his father Hylton Jolliffe as the 4th Baron Hylton, but it does not explicitly state the date of death of Hylton Jolliffe, 3rd Baron Hylton. The passagesdo not contain enough information to determine the date of death of William Jolliffe, 4Th Baron Hylton's father.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the context provided, William George Hylton Jolliffe, 4th Baron Hylton's father was Hylton Jolliffe, 3rd Baron Hylton. Passage 2mentions that William George Hylton Jolliffe succeeded his father Hylton Jolliffe as the 4th Baron Hylton, but it does not explicitly state the date of death of Hylton Jolliffe, 3rd Baron Hylton. The passagesdo not contain enough information to determine the date of death of William Jolliffe, 4Th Baron Hylton's father.\"}\n",
      "INFO:__main__:latency=4.307974709001428\n",
      "INFO:__main__:prompt_tokens=3458\n",
      "INFO:__main__:completion_tokens=122\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3458, completion_tokens=122, latency=4.3080\n",
      "INFO:__main__:e_idx=1/1, chunk_index=13/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3508\n",
      "INFO:bedrock_predictor:Claude completion tokens: 70\n",
      "INFO:__main__:response={'response_json': {'completion': ' Oskar Roehler directed the film Sources of Life. According to the passages, Oskar Roehler was born in 1959 in Starnberg, Germany, the son of writers Gisela Elsner and Klaus Roehler. Therefore, the father of the director of Sources of Life is Klaus Roehler.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Oskar Roehler directed the film Sources of Life. According to the passages, Oskar Roehler was born in 1959 in Starnberg, Germany, the son of writers Gisela Elsner and Klaus Roehler. Therefore, the father of the director of Sources of Life is Klaus Roehler.'}, 'latency': 3.23749429199961, 'prompt_tokens': 3508, 'completion_tokens': 70}\n",
      "INFO:__main__:response_json={'completion': ' Oskar Roehler directed the film Sources of Life. According to the passages, Oskar Roehler was born in 1959 in Starnberg, Germany, the son of writers Gisela Elsner and Klaus Roehler. Therefore, the father of the director of Sources of Life is Klaus Roehler.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Oskar Roehler directed the film Sources of Life. According to the passages, Oskar Roehler was born in 1959 in Starnberg, Germany, the son of writers Gisela Elsner and Klaus Roehler. Therefore, the father of the director of Sources of Life is Klaus Roehler.'}\n",
      "INFO:__main__:latency=3.23749429199961\n",
      "INFO:__main__:prompt_tokens=3508\n",
      "INFO:__main__:completion_tokens=70\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3508, completion_tokens=70, latency=3.2375\n",
      "INFO:__main__:e_idx=1/1, chunk_index=14/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3436\n",
      "INFO:bedrock_predictor:Claude completion tokens: 71\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided, Patnam Vachina Pativrathalu came out earlier than Where Are You? I'M Here. Passage 8 indicates that Patnam Vachina Pativrathalu is a 1982 Telugu film, while Passage 1 states that Where Are You? I'M Here is a 1993 Italian film.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Patnam Vachina Pativrathalu came out earlier than Where Are You? I'M Here. Passage 8 indicates that Patnam Vachina Pativrathalu is a 1982 Telugu film, while Passage 1 states that Where Are You? I'M Here is a 1993 Italian film.\"}, 'latency': 1.7217319999981555, 'prompt_tokens': 3436, 'completion_tokens': 71}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided, Patnam Vachina Pativrathalu came out earlier than Where Are You? I'M Here. Passage 8 indicates that Patnam Vachina Pativrathalu is a 1982 Telugu film, while Passage 1 states that Where Are You? I'M Here is a 1993 Italian film.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Patnam Vachina Pativrathalu came out earlier than Where Are You? I'M Here. Passage 8 indicates that Patnam Vachina Pativrathalu is a 1982 Telugu film, while Passage 1 states that Where Are You? I'M Here is a 1993 Italian film.\"}\n",
      "INFO:__main__:latency=1.7217319999981555\n",
      "INFO:__main__:prompt_tokens=3436\n",
      "INFO:__main__:completion_tokens=71\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3436, completion_tokens=71, latency=1.7217\n",
      "INFO:__main__:e_idx=1/1, chunk_index=15/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3249\n",
      "INFO:bedrock_predictor:Claude completion tokens: 61\n",
      "INFO:__main__:response={'response_json': {'completion': \" Charles James Irwin Grant, 6th Baron de Longueuil's paternal grandfather was Charles William Grant, 5th Baron de Longueuil. According to passage 1, Charles James Irwin Grant was the only son of Charles William Grant, 5th Baron de Longueuil and Caroline Coffin.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Charles James Irwin Grant, 6th Baron de Longueuil's paternal grandfather was Charles William Grant, 5th Baron de Longueuil. According to passage 1, Charles James Irwin Grant was the only son of Charles William Grant, 5th Baron de Longueuil and Caroline Coffin.\"}, 'latency': 1.6837510420009494, 'prompt_tokens': 3249, 'completion_tokens': 61}\n",
      "INFO:__main__:response_json={'completion': \" Charles James Irwin Grant, 6th Baron de Longueuil's paternal grandfather was Charles William Grant, 5th Baron de Longueuil. According to passage 1, Charles James Irwin Grant was the only son of Charles William Grant, 5th Baron de Longueuil and Caroline Coffin.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Charles James Irwin Grant, 6th Baron de Longueuil's paternal grandfather was Charles William Grant, 5th Baron de Longueuil. According to passage 1, Charles James Irwin Grant was the only son of Charles William Grant, 5th Baron de Longueuil and Caroline Coffin.\"}\n",
      "INFO:__main__:latency=1.6837510420009494\n",
      "INFO:__main__:prompt_tokens=3249\n",
      "INFO:__main__:completion_tokens=61\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3249, completion_tokens=61, latency=1.6838\n",
      "INFO:__main__:e_idx=1/1, chunk_index=16/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3087\n",
      "INFO:bedrock_predictor:Claude completion tokens: 60\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the context passages provided, Helen Clifton was born earlier than Steve Barancik. Helen Clifton was born on 4 May 1948, while Steve Barancik was born on 23 September 1961. So Helen Clifton was born approximately 13 years earlier than Steve Barancik.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context passages provided, Helen Clifton was born earlier than Steve Barancik. Helen Clifton was born on 4 May 1948, while Steve Barancik was born on 23 September 1961. So Helen Clifton was born approximately 13 years earlier than Steve Barancik.'}, 'latency': 1.7779075829967041, 'prompt_tokens': 3087, 'completion_tokens': 60}\n",
      "INFO:__main__:response_json={'completion': ' Based on the context passages provided, Helen Clifton was born earlier than Steve Barancik. Helen Clifton was born on 4 May 1948, while Steve Barancik was born on 23 September 1961. So Helen Clifton was born approximately 13 years earlier than Steve Barancik.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context passages provided, Helen Clifton was born earlier than Steve Barancik. Helen Clifton was born on 4 May 1948, while Steve Barancik was born on 23 September 1961. So Helen Clifton was born approximately 13 years earlier than Steve Barancik.'}\n",
      "INFO:__main__:latency=1.7779075829967041\n",
      "INFO:__main__:prompt_tokens=3087\n",
      "INFO:__main__:completion_tokens=60\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3087, completion_tokens=60, latency=1.7779\n",
      "INFO:__main__:e_idx=1/1, chunk_index=17/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3680\n",
      "INFO:bedrock_predictor:Claude completion tokens: 81\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided:\\n\\n1. Leka, Crown Prince Of Albania (Born 1982)'s mother was Susan, Crown Princess of Albania.  \\n2. Susan, Crown Princess of Albania died of lung cancer on July 17, 2004 in Tirana, Albania.\\n3. She was buried next to her mother-in-law, Queen Geraldine.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided:\\n\\n1. Leka, Crown Prince Of Albania (Born 1982)'s mother was Susan, Crown Princess of Albania.  \\n2. Susan, Crown Princess of Albania died of lung cancer on July 17, 2004 in Tirana, Albania.\\n3. She was buried next to her mother-in-law, Queen Geraldine.\"}, 'latency': 2.457002666997141, 'prompt_tokens': 3680, 'completion_tokens': 81}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided:\\n\\n1. Leka, Crown Prince Of Albania (Born 1982)'s mother was Susan, Crown Princess of Albania.  \\n2. Susan, Crown Princess of Albania died of lung cancer on July 17, 2004 in Tirana, Albania.\\n3. She was buried next to her mother-in-law, Queen Geraldine.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided:\\n\\n1. Leka, Crown Prince Of Albania (Born 1982)'s mother was Susan, Crown Princess of Albania.  \\n2. Susan, Crown Princess of Albania died of lung cancer on July 17, 2004 in Tirana, Albania.\\n3. She was buried next to her mother-in-law, Queen Geraldine.\"}\n",
      "INFO:__main__:latency=2.457002666997141\n",
      "INFO:__main__:prompt_tokens=3680\n",
      "INFO:__main__:completion_tokens=81\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3680, completion_tokens=81, latency=2.4570\n",
      "INFO:__main__:e_idx=1/1, chunk_index=18/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3950\n",
      "INFO:bedrock_predictor:Claude completion tokens: 88\n",
      "INFO:__main__:response={'response_json': {'completion': \" Band-e-Amir Dragons is named after Band-e Amir, a series of six deep blue lakes in Bamyan Province of Afghanistan. Band-e Amir is the country's first national park, established in 2009 to protect the natural beauty of the lakes that were formed by mineral-rich water deposits over time. The Band-e-Amir Dragons cricket team represents the central Afghan provinces near this national park.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Band-e-Amir Dragons is named after Band-e Amir, a series of six deep blue lakes in Bamyan Province of Afghanistan. Band-e Amir is the country's first national park, established in 2009 to protect the natural beauty of the lakes that were formed by mineral-rich water deposits over time. The Band-e-Amir Dragons cricket team represents the central Afghan provinces near this national park.\"}, 'latency': 3.5910073749982985, 'prompt_tokens': 3950, 'completion_tokens': 88}\n",
      "INFO:__main__:response_json={'completion': \" Band-e-Amir Dragons is named after Band-e Amir, a series of six deep blue lakes in Bamyan Province of Afghanistan. Band-e Amir is the country's first national park, established in 2009 to protect the natural beauty of the lakes that were formed by mineral-rich water deposits over time. The Band-e-Amir Dragons cricket team represents the central Afghan provinces near this national park.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Band-e-Amir Dragons is named after Band-e Amir, a series of six deep blue lakes in Bamyan Province of Afghanistan. Band-e Amir is the country's first national park, established in 2009 to protect the natural beauty of the lakes that were formed by mineral-rich water deposits over time. The Band-e-Amir Dragons cricket team represents the central Afghan provinces near this national park.\"}\n",
      "INFO:__main__:latency=3.5910073749982985\n",
      "INFO:__main__:prompt_tokens=3950\n",
      "INFO:__main__:completion_tokens=88\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3950, completion_tokens=88, latency=3.5910\n",
      "INFO:__main__:e_idx=1/1, chunk_index=19/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3463\n",
      "INFO:bedrock_predictor:Claude completion tokens: 53\n",
      "INFO:__main__:response={'response_json': {'completion': ' The radio station that serves the area comprising all of Belknap County, among others, is not mentioned in the provided passages. The passages do not contain enough information to answer the question. I do not know the call letters of the radio station serving that area.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The radio station that serves the area comprising all of Belknap County, among others, is not mentioned in the provided passages. The passages do not contain enough information to answer the question. I do not know the call letters of the radio station serving that area.'}, 'latency': 2.252195875000325, 'prompt_tokens': 3463, 'completion_tokens': 53}\n",
      "INFO:__main__:response_json={'completion': ' The radio station that serves the area comprising all of Belknap County, among others, is not mentioned in the provided passages. The passages do not contain enough information to answer the question. I do not know the call letters of the radio station serving that area.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The radio station that serves the area comprising all of Belknap County, among others, is not mentioned in the provided passages. The passages do not contain enough information to answer the question. I do not know the call letters of the radio station serving that area.'}\n",
      "INFO:__main__:latency=2.252195875000325\n",
      "INFO:__main__:prompt_tokens=3463\n",
      "INFO:__main__:completion_tokens=53\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3463, completion_tokens=53, latency=2.2522\n",
      "INFO:__main__:e_idx=1/1, chunk_index=20/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3996\n",
      "INFO:bedrock_predictor:Claude completion tokens: 33\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages, East Mamprusi District includes the village of Shienga. East Mamprusi District has Gambaga as its capital town.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages, East Mamprusi District includes the village of Shienga. East Mamprusi District has Gambaga as its capital town.'}, 'latency': 1.407112540997332, 'prompt_tokens': 3996, 'completion_tokens': 33}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages, East Mamprusi District includes the village of Shienga. East Mamprusi District has Gambaga as its capital town.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages, East Mamprusi District includes the village of Shienga. East Mamprusi District has Gambaga as its capital town.'}\n",
      "INFO:__main__:latency=1.407112540997332\n",
      "INFO:__main__:prompt_tokens=3996\n",
      "INFO:__main__:completion_tokens=33\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3996, completion_tokens=33, latency=1.4071\n",
      "INFO:__main__:e_idx=1/1, chunk_index=21/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3050\n",
      "INFO:bedrock_predictor:Claude completion tokens: 103\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided, Tim Cluess has led Iona for a longer period of time than Steve Prohm has led any one team. Cluess is mentioned as being in his sixth and seventh years as Iona's head coach in the 2015-16 and 2016-17 seasons. Prohm's longest tenure is mentioned as being in his first season with Iowa State in 2015-16. Therefore, the answer is that Tim Cluess has led his team for a longer period of time than Steve Prohm.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Tim Cluess has led Iona for a longer period of time than Steve Prohm has led any one team. Cluess is mentioned as being in his sixth and seventh years as Iona's head coach in the 2015-16 and 2016-17 seasons. Prohm's longest tenure is mentioned as being in his first season with Iowa State in 2015-16. Therefore, the answer is that Tim Cluess has led his team for a longer period of time than Steve Prohm.\"}, 'latency': 4.622739291000471, 'prompt_tokens': 3050, 'completion_tokens': 103}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided, Tim Cluess has led Iona for a longer period of time than Steve Prohm has led any one team. Cluess is mentioned as being in his sixth and seventh years as Iona's head coach in the 2015-16 and 2016-17 seasons. Prohm's longest tenure is mentioned as being in his first season with Iowa State in 2015-16. Therefore, the answer is that Tim Cluess has led his team for a longer period of time than Steve Prohm.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Tim Cluess has led Iona for a longer period of time than Steve Prohm has led any one team. Cluess is mentioned as being in his sixth and seventh years as Iona's head coach in the 2015-16 and 2016-17 seasons. Prohm's longest tenure is mentioned as being in his first season with Iowa State in 2015-16. Therefore, the answer is that Tim Cluess has led his team for a longer period of time than Steve Prohm.\"}\n",
      "INFO:__main__:latency=4.622739291000471\n",
      "INFO:__main__:prompt_tokens=3050\n",
      "INFO:__main__:completion_tokens=103\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3050, completion_tokens=103, latency=4.6227\n",
      "INFO:__main__:e_idx=1/1, chunk_index=22/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3737\n",
      "INFO:bedrock_predictor:Claude completion tokens: 88\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages provided, Jim Rasenberger has contributed to publications like the New York Times, Vanity Fair, and Smithsonian. Passage 2 indicates that Smithsonian is the official journal published by the Smithsonian Institution in Washington, D.C. Therefore, the journal Jim Rasenberger contributes to that is the official journal of an institute located in Washington, D.C. is Smithsonian.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Jim Rasenberger has contributed to publications like the New York Times, Vanity Fair, and Smithsonian. Passage 2 indicates that Smithsonian is the official journal published by the Smithsonian Institution in Washington, D.C. Therefore, the journal Jim Rasenberger contributes to that is the official journal of an institute located in Washington, D.C. is Smithsonian.'}, 'latency': 1.986900042000343, 'prompt_tokens': 3737, 'completion_tokens': 88}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages provided, Jim Rasenberger has contributed to publications like the New York Times, Vanity Fair, and Smithsonian. Passage 2 indicates that Smithsonian is the official journal published by the Smithsonian Institution in Washington, D.C. Therefore, the journal Jim Rasenberger contributes to that is the official journal of an institute located in Washington, D.C. is Smithsonian.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Jim Rasenberger has contributed to publications like the New York Times, Vanity Fair, and Smithsonian. Passage 2 indicates that Smithsonian is the official journal published by the Smithsonian Institution in Washington, D.C. Therefore, the journal Jim Rasenberger contributes to that is the official journal of an institute located in Washington, D.C. is Smithsonian.'}\n",
      "INFO:__main__:latency=1.986900042000343\n",
      "INFO:__main__:prompt_tokens=3737\n",
      "INFO:__main__:completion_tokens=88\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3737, completion_tokens=88, latency=1.9869\n",
      "INFO:__main__:e_idx=1/1, chunk_index=23/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3410\n",
      "INFO:bedrock_predictor:Claude completion tokens: 72\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages provided, Cassiope has fewer species than Deutzia. Passage 6 states that Cassiope is a genus of 9-12 small shrubby species, while Passage 5 mentions that Deutzia is a genus of about 60 species of flowering plants. Therefore, the answer is that Deutzia has more species than Cassiope.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Cassiope has fewer species than Deutzia. Passage 6 states that Cassiope is a genus of 9-12 small shrubby species, while Passage 5 mentions that Deutzia is a genus of about 60 species of flowering plants. Therefore, the answer is that Deutzia has more species than Cassiope.'}, 'latency': 2.6706151670005056, 'prompt_tokens': 3410, 'completion_tokens': 72}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages provided, Cassiope has fewer species than Deutzia. Passage 6 states that Cassiope is a genus of 9-12 small shrubby species, while Passage 5 mentions that Deutzia is a genus of about 60 species of flowering plants. Therefore, the answer is that Deutzia has more species than Cassiope.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Cassiope has fewer species than Deutzia. Passage 6 states that Cassiope is a genus of 9-12 small shrubby species, while Passage 5 mentions that Deutzia is a genus of about 60 species of flowering plants. Therefore, the answer is that Deutzia has more species than Cassiope.'}\n",
      "INFO:__main__:latency=2.6706151670005056\n",
      "INFO:__main__:prompt_tokens=3410\n",
      "INFO:__main__:completion_tokens=72\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3410, completion_tokens=72, latency=2.6706\n",
      "INFO:__main__:e_idx=1/1, chunk_index=24/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3979\n",
      "INFO:bedrock_predictor:Claude completion tokens: 72\n",
      "INFO:__main__:response={'response_json': {'completion': ' Peter Curtis and Scott Draper are both tennis players. Specifically:\\n\\n- Peter Curtis was a former British professional tennis player who won one Grand Slam title in mixed doubles. \\n\\n- Scott Draper was an Australian former tennis player who won the Australian Open Mixed Doubles in 2005 and reached the fourth round of singles at some Grand Slam events.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Peter Curtis and Scott Draper are both tennis players. Specifically:\\n\\n- Peter Curtis was a former British professional tennis player who won one Grand Slam title in mixed doubles. \\n\\n- Scott Draper was an Australian former tennis player who won the Australian Open Mixed Doubles in 2005 and reached the fourth round of singles at some Grand Slam events.'}, 'latency': 2.9538008749950677, 'prompt_tokens': 3979, 'completion_tokens': 72}\n",
      "INFO:__main__:response_json={'completion': ' Peter Curtis and Scott Draper are both tennis players. Specifically:\\n\\n- Peter Curtis was a former British professional tennis player who won one Grand Slam title in mixed doubles. \\n\\n- Scott Draper was an Australian former tennis player who won the Australian Open Mixed Doubles in 2005 and reached the fourth round of singles at some Grand Slam events.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Peter Curtis and Scott Draper are both tennis players. Specifically:\\n\\n- Peter Curtis was a former British professional tennis player who won one Grand Slam title in mixed doubles. \\n\\n- Scott Draper was an Australian former tennis player who won the Australian Open Mixed Doubles in 2005 and reached the fourth round of singles at some Grand Slam events.'}\n",
      "INFO:__main__:latency=2.9538008749950677\n",
      "INFO:__main__:prompt_tokens=3979\n",
      "INFO:__main__:completion_tokens=72\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3979, completion_tokens=72, latency=2.9538\n",
      "INFO:__main__:e_idx=1/1, chunk_index=25/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3171\n",
      "INFO:bedrock_predictor:Claude completion tokens: 40\n",
      "INFO:__main__:response={'response_json': {'completion': ' Jacob Minah had his personal best in the decathlon event, setting a score of 8099 points at the 2007 Summer Universiade in Bangkok, Thailand, where he won the gold medal.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Jacob Minah had his personal best in the decathlon event, setting a score of 8099 points at the 2007 Summer Universiade in Bangkok, Thailand, where he won the gold medal.'}, 'latency': 1.800431916002708, 'prompt_tokens': 3171, 'completion_tokens': 40}\n",
      "INFO:__main__:response_json={'completion': ' Jacob Minah had his personal best in the decathlon event, setting a score of 8099 points at the 2007 Summer Universiade in Bangkok, Thailand, where he won the gold medal.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Jacob Minah had his personal best in the decathlon event, setting a score of 8099 points at the 2007 Summer Universiade in Bangkok, Thailand, where he won the gold medal.'}\n",
      "INFO:__main__:latency=1.800431916002708\n",
      "INFO:__main__:prompt_tokens=3171\n",
      "INFO:__main__:completion_tokens=40\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3171, completion_tokens=40, latency=1.8004\n",
      "INFO:__main__:e_idx=1/1, chunk_index=26/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3655\n",
      "INFO:bedrock_predictor:Claude completion tokens: 160\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, Euptelea and Muehlenbeckia are both genuses. In the passages provided:\\n\\n1) Passage 5 describes Euptelea as \"a genus of two species of flowering plants\". \\n\\n2) Passage 9 describes Muehlenbeckia as \"a genus of flowering plants\". \\n\\n3) Multiple passages, such as Passages 1, 2, 3, 4, 6, 7, 8, and 10 refer to species within the genus Muehlenbeckia such as M. australis, M. axillaris, etc. \\n\\nTherefore, based on the information given in the passages, both Euptelea and Muehlenbeckia are genuses, as they are used to categorize multiple plant species.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Euptelea and Muehlenbeckia are both genuses. In the passages provided:\\n\\n1) Passage 5 describes Euptelea as \"a genus of two species of flowering plants\". \\n\\n2) Passage 9 describes Muehlenbeckia as \"a genus of flowering plants\". \\n\\n3) Multiple passages, such as Passages 1, 2, 3, 4, 6, 7, 8, and 10 refer to species within the genus Muehlenbeckia such as M. australis, M. axillaris, etc. \\n\\nTherefore, based on the information given in the passages, both Euptelea and Muehlenbeckia are genuses, as they are used to categorize multiple plant species.'}, 'latency': 4.419979334001255, 'prompt_tokens': 3655, 'completion_tokens': 160}\n",
      "INFO:__main__:response_json={'completion': ' Yes, Euptelea and Muehlenbeckia are both genuses. In the passages provided:\\n\\n1) Passage 5 describes Euptelea as \"a genus of two species of flowering plants\". \\n\\n2) Passage 9 describes Muehlenbeckia as \"a genus of flowering plants\". \\n\\n3) Multiple passages, such as Passages 1, 2, 3, 4, 6, 7, 8, and 10 refer to species within the genus Muehlenbeckia such as M. australis, M. axillaris, etc. \\n\\nTherefore, based on the information given in the passages, both Euptelea and Muehlenbeckia are genuses, as they are used to categorize multiple plant species.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Euptelea and Muehlenbeckia are both genuses. In the passages provided:\\n\\n1) Passage 5 describes Euptelea as \"a genus of two species of flowering plants\". \\n\\n2) Passage 9 describes Muehlenbeckia as \"a genus of flowering plants\". \\n\\n3) Multiple passages, such as Passages 1, 2, 3, 4, 6, 7, 8, and 10 refer to species within the genus Muehlenbeckia such as M. australis, M. axillaris, etc. \\n\\nTherefore, based on the information given in the passages, both Euptelea and Muehlenbeckia are genuses, as they are used to categorize multiple plant species.'}\n",
      "INFO:__main__:latency=4.419979334001255\n",
      "INFO:__main__:prompt_tokens=3655\n",
      "INFO:__main__:completion_tokens=160\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3655, completion_tokens=160, latency=4.4200\n",
      "INFO:__main__:e_idx=1/1, chunk_index=27/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3662\n",
      "INFO:bedrock_predictor:Claude completion tokens: 44\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the context provided, Bennett Hill Farm is located in New Scotland, Albany County, New York. Passage 9 provides demographic information about New Scotland and states that the population was 8,648 at the 2010 census.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, Bennett Hill Farm is located in New Scotland, Albany County, New York. Passage 9 provides demographic information about New Scotland and states that the population was 8,648 at the 2010 census.'}, 'latency': 2.5016751249931986, 'prompt_tokens': 3662, 'completion_tokens': 44}\n",
      "INFO:__main__:response_json={'completion': ' Based on the context provided, Bennett Hill Farm is located in New Scotland, Albany County, New York. Passage 9 provides demographic information about New Scotland and states that the population was 8,648 at the 2010 census.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, Bennett Hill Farm is located in New Scotland, Albany County, New York. Passage 9 provides demographic information about New Scotland and states that the population was 8,648 at the 2010 census.'}\n",
      "INFO:__main__:latency=2.5016751249931986\n",
      "INFO:__main__:prompt_tokens=3662\n",
      "INFO:__main__:completion_tokens=44\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3662, completion_tokens=44, latency=2.5017\n",
      "INFO:__main__:e_idx=1/1, chunk_index=28/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3098\n",
      "INFO:bedrock_predictor:Claude completion tokens: 72\n",
      "INFO:__main__:response={'response_json': {'completion': \" Sergio Casal and Manuel Orantes both played professional tennis. Specifically, they both had success in both singles and doubles competitions. Orantes won the 1975 US Open singles title and multiple doubles titles, while Casal won three Grand Slam doubles titles and the 1988 Olympic silver medal in men's doubles. Tennis is the type of sport they have in common.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Sergio Casal and Manuel Orantes both played professional tennis. Specifically, they both had success in both singles and doubles competitions. Orantes won the 1975 US Open singles title and multiple doubles titles, while Casal won three Grand Slam doubles titles and the 1988 Olympic silver medal in men's doubles. Tennis is the type of sport they have in common.\"}, 'latency': 2.3162352920044214, 'prompt_tokens': 3098, 'completion_tokens': 72}\n",
      "INFO:__main__:response_json={'completion': \" Sergio Casal and Manuel Orantes both played professional tennis. Specifically, they both had success in both singles and doubles competitions. Orantes won the 1975 US Open singles title and multiple doubles titles, while Casal won three Grand Slam doubles titles and the 1988 Olympic silver medal in men's doubles. Tennis is the type of sport they have in common.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Sergio Casal and Manuel Orantes both played professional tennis. Specifically, they both had success in both singles and doubles competitions. Orantes won the 1975 US Open singles title and multiple doubles titles, while Casal won three Grand Slam doubles titles and the 1988 Olympic silver medal in men's doubles. Tennis is the type of sport they have in common.\"}\n",
      "INFO:__main__:latency=2.3162352920044214\n",
      "INFO:__main__:prompt_tokens=3098\n",
      "INFO:__main__:completion_tokens=72\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3098, completion_tokens=72, latency=2.3162\n",
      "INFO:__main__:e_idx=1/1, chunk_index=29/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3704\n",
      "INFO:bedrock_predictor:Claude completion tokens: 106\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided, Baghdad ER was released first. Baghdad ER is a documentary released by HBO on May 21, 2006 about the Iraq War from the perspective of a military hospital in Baghdad. The Ten-Year Lunch, which is about the Algonquin Round Table, premiered on PBS's American Masters on September 28, 1987 and won the Academy Award for Best Documentary Feature on April 11, 1988. So Baghdad ER was released earlier than The Ten-Year Lunch.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Baghdad ER was released first. Baghdad ER is a documentary released by HBO on May 21, 2006 about the Iraq War from the perspective of a military hospital in Baghdad. The Ten-Year Lunch, which is about the Algonquin Round Table, premiered on PBS's American Masters on September 28, 1987 and won the Academy Award for Best Documentary Feature on April 11, 1988. So Baghdad ER was released earlier than The Ten-Year Lunch.\"}, 'latency': 3.0082425410000724, 'prompt_tokens': 3704, 'completion_tokens': 106}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided, Baghdad ER was released first. Baghdad ER is a documentary released by HBO on May 21, 2006 about the Iraq War from the perspective of a military hospital in Baghdad. The Ten-Year Lunch, which is about the Algonquin Round Table, premiered on PBS's American Masters on September 28, 1987 and won the Academy Award for Best Documentary Feature on April 11, 1988. So Baghdad ER was released earlier than The Ten-Year Lunch.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Baghdad ER was released first. Baghdad ER is a documentary released by HBO on May 21, 2006 about the Iraq War from the perspective of a military hospital in Baghdad. The Ten-Year Lunch, which is about the Algonquin Round Table, premiered on PBS's American Masters on September 28, 1987 and won the Academy Award for Best Documentary Feature on April 11, 1988. So Baghdad ER was released earlier than The Ten-Year Lunch.\"}\n",
      "INFO:__main__:latency=3.0082425410000724\n",
      "INFO:__main__:prompt_tokens=3704\n",
      "INFO:__main__:completion_tokens=106\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3704, completion_tokens=106, latency=3.0082\n",
      "INFO:__main__:e_idx=1/1, chunk_index=30/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3400\n",
      "INFO:bedrock_predictor:Claude completion tokens: 156\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages provided:\\n\\n- Lee Young-Sun achieved her personal best throw of 58.87 metres in the javelin throw at the 2002 Asian Games in Busan, South Korea (Passage 1).\\n\\n- 459 athletes from 39 nations took part in the athletics competition at the 2002 Asian Games, including Afghanistan, Bhutan, Brunei, Laos and Maldives who did not have any representatives (Passage 2). \\n\\n- A total of 459 athletes from 39 nations competed in athletics at the 2002 Asian Games (Passage 2).\\n\\nTherefore, the number of women who competed at the 2002 Asian Games, including Lee Young-Sun, was likely around half of the 459 total athletes, or approximately 230 women.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided:\\n\\n- Lee Young-Sun achieved her personal best throw of 58.87 metres in the javelin throw at the 2002 Asian Games in Busan, South Korea (Passage 1).\\n\\n- 459 athletes from 39 nations took part in the athletics competition at the 2002 Asian Games, including Afghanistan, Bhutan, Brunei, Laos and Maldives who did not have any representatives (Passage 2). \\n\\n- A total of 459 athletes from 39 nations competed in athletics at the 2002 Asian Games (Passage 2).\\n\\nTherefore, the number of women who competed at the 2002 Asian Games, including Lee Young-Sun, was likely around half of the 459 total athletes, or approximately 230 women.'}, 'latency': 4.647688208002364, 'prompt_tokens': 3400, 'completion_tokens': 156}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages provided:\\n\\n- Lee Young-Sun achieved her personal best throw of 58.87 metres in the javelin throw at the 2002 Asian Games in Busan, South Korea (Passage 1).\\n\\n- 459 athletes from 39 nations took part in the athletics competition at the 2002 Asian Games, including Afghanistan, Bhutan, Brunei, Laos and Maldives who did not have any representatives (Passage 2). \\n\\n- A total of 459 athletes from 39 nations competed in athletics at the 2002 Asian Games (Passage 2).\\n\\nTherefore, the number of women who competed at the 2002 Asian Games, including Lee Young-Sun, was likely around half of the 459 total athletes, or approximately 230 women.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided:\\n\\n- Lee Young-Sun achieved her personal best throw of 58.87 metres in the javelin throw at the 2002 Asian Games in Busan, South Korea (Passage 1).\\n\\n- 459 athletes from 39 nations took part in the athletics competition at the 2002 Asian Games, including Afghanistan, Bhutan, Brunei, Laos and Maldives who did not have any representatives (Passage 2). \\n\\n- A total of 459 athletes from 39 nations competed in athletics at the 2002 Asian Games (Passage 2).\\n\\nTherefore, the number of women who competed at the 2002 Asian Games, including Lee Young-Sun, was likely around half of the 459 total athletes, or approximately 230 women.'}\n",
      "INFO:__main__:latency=4.647688208002364\n",
      "INFO:__main__:prompt_tokens=3400\n",
      "INFO:__main__:completion_tokens=156\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3400, completion_tokens=156, latency=4.6477\n",
      "INFO:__main__:e_idx=1/1, chunk_index=31/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3098\n",
      "INFO:bedrock_predictor:Claude completion tokens: 75\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the contextual clues provided, \\'Back to Bedlam\\' was the debut album for British singer James Blunt.\\n\\nThe passage specifically mentions \"Feature Album - Back To Bedlam by James Blunt\" so it directly links the album \\'Back to Bedlam\\' to the British singer James Blunt. I have answered concisely using one sentence as instructed.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the contextual clues provided, \\'Back to Bedlam\\' was the debut album for British singer James Blunt.\\n\\nThe passage specifically mentions \"Feature Album - Back To Bedlam by James Blunt\" so it directly links the album \\'Back to Bedlam\\' to the British singer James Blunt. I have answered concisely using one sentence as instructed.'}, 'latency': 2.4954297919975943, 'prompt_tokens': 3098, 'completion_tokens': 75}\n",
      "INFO:__main__:response_json={'completion': ' Based on the contextual clues provided, \\'Back to Bedlam\\' was the debut album for British singer James Blunt.\\n\\nThe passage specifically mentions \"Feature Album - Back To Bedlam by James Blunt\" so it directly links the album \\'Back to Bedlam\\' to the British singer James Blunt. I have answered concisely using one sentence as instructed.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the contextual clues provided, \\'Back to Bedlam\\' was the debut album for British singer James Blunt.\\n\\nThe passage specifically mentions \"Feature Album - Back To Bedlam by James Blunt\" so it directly links the album \\'Back to Bedlam\\' to the British singer James Blunt. I have answered concisely using one sentence as instructed.'}\n",
      "INFO:__main__:latency=2.4954297919975943\n",
      "INFO:__main__:prompt_tokens=3098\n",
      "INFO:__main__:completion_tokens=75\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3098, completion_tokens=75, latency=2.4954\n",
      "INFO:__main__:e_idx=1/1, chunk_index=32/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3909\n",
      "INFO:bedrock_predictor:Claude completion tokens: 26\n",
      "INFO:__main__:response={'response_json': {'completion': ' A saluki is a breed of dog. The passage mentions that Salukis are considered one of the oldest dog breeds in existence.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' A saluki is a breed of dog. The passage mentions that Salukis are considered one of the oldest dog breeds in existence.'}, 'latency': 1.5512140829960117, 'prompt_tokens': 3909, 'completion_tokens': 26}\n",
      "INFO:__main__:response_json={'completion': ' A saluki is a breed of dog. The passage mentions that Salukis are considered one of the oldest dog breeds in existence.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' A saluki is a breed of dog. The passage mentions that Salukis are considered one of the oldest dog breeds in existence.'}\n",
      "INFO:__main__:latency=1.5512140829960117\n",
      "INFO:__main__:prompt_tokens=3909\n",
      "INFO:__main__:completion_tokens=26\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3909, completion_tokens=26, latency=1.5512\n",
      "INFO:__main__:e_idx=1/1, chunk_index=33/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3971\n",
      "INFO:bedrock_predictor:Claude completion tokens: 68\n",
      "INFO:__main__:response={'response_json': {'completion': ' The Naismith Legacy Award is presented to individuals from the game of basketball for furthering the values of honor, respect and integrity, both on and off the court, according to the passage. The passage discusses the Naismith Legacy Award and provides details about its purpose, selection process, design, and usage to honor individuals in basketball.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The Naismith Legacy Award is presented to individuals from the game of basketball for furthering the values of honor, respect and integrity, both on and off the court, according to the passage. The passage discusses the Naismith Legacy Award and provides details about its purpose, selection process, design, and usage to honor individuals in basketball.'}, 'latency': 2.0682135419992846, 'prompt_tokens': 3971, 'completion_tokens': 68}\n",
      "INFO:__main__:response_json={'completion': ' The Naismith Legacy Award is presented to individuals from the game of basketball for furthering the values of honor, respect and integrity, both on and off the court, according to the passage. The passage discusses the Naismith Legacy Award and provides details about its purpose, selection process, design, and usage to honor individuals in basketball.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The Naismith Legacy Award is presented to individuals from the game of basketball for furthering the values of honor, respect and integrity, both on and off the court, according to the passage. The passage discusses the Naismith Legacy Award and provides details about its purpose, selection process, design, and usage to honor individuals in basketball.'}\n",
      "INFO:__main__:latency=2.0682135419992846\n",
      "INFO:__main__:prompt_tokens=3971\n",
      "INFO:__main__:completion_tokens=68\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3971, completion_tokens=68, latency=2.0682\n",
      "INFO:__main__:e_idx=1/1, chunk_index=34/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3132\n",
      "INFO:bedrock_predictor:Claude completion tokens: 42\n",
      "INFO:__main__:response={'response_json': {'completion': \" The passage does not provide enough information to definitively answer which events took place at Lord's Cricket Ground. While it mentions that Lord's was used for archery, it does not list all the events there.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" The passage does not provide enough information to definitively answer which events took place at Lord's Cricket Ground. While it mentions that Lord's was used for archery, it does not list all the events there.\"}, 'latency': 1.7358617919962853, 'prompt_tokens': 3132, 'completion_tokens': 42}\n",
      "INFO:__main__:response_json={'completion': \" The passage does not provide enough information to definitively answer which events took place at Lord's Cricket Ground. While it mentions that Lord's was used for archery, it does not list all the events there.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" The passage does not provide enough information to definitively answer which events took place at Lord's Cricket Ground. While it mentions that Lord's was used for archery, it does not list all the events there.\"}\n",
      "INFO:__main__:latency=1.7358617919962853\n",
      "INFO:__main__:prompt_tokens=3132\n",
      "INFO:__main__:completion_tokens=42\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3132, completion_tokens=42, latency=1.7359\n",
      "INFO:__main__:e_idx=1/1, chunk_index=35/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3135\n",
      "INFO:bedrock_predictor:Claude completion tokens: 11\n",
      "INFO:__main__:response={'response_json': {'completion': ' Huey, Dewey, and Louie.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Huey, Dewey, and Louie.'}, 'latency': 1.0696644999989076, 'prompt_tokens': 3135, 'completion_tokens': 11}\n",
      "INFO:__main__:response_json={'completion': ' Huey, Dewey, and Louie.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Huey, Dewey, and Louie.'}\n",
      "INFO:__main__:latency=1.0696644999989076\n",
      "INFO:__main__:prompt_tokens=3135\n",
      "INFO:__main__:completion_tokens=11\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3135, completion_tokens=11, latency=1.0697\n",
      "INFO:__main__:e_idx=1/1, chunk_index=36/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3030\n",
      "INFO:bedrock_predictor:Claude completion tokens: 45\n",
      "INFO:__main__:response={'response_json': {'completion': ' Muhammad Ali. The passage explicitly states that Muhammad Ali (Cassius Clay) was stripped of his world heavyweight title and had his boxing license suspended after he refused to be inducted into the United States Army in April 1967.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Muhammad Ali. The passage explicitly states that Muhammad Ali (Cassius Clay) was stripped of his world heavyweight title and had his boxing license suspended after he refused to be inducted into the United States Army in April 1967.'}, 'latency': 1.6166856249983539, 'prompt_tokens': 3030, 'completion_tokens': 45}\n",
      "INFO:__main__:response_json={'completion': ' Muhammad Ali. The passage explicitly states that Muhammad Ali (Cassius Clay) was stripped of his world heavyweight title and had his boxing license suspended after he refused to be inducted into the United States Army in April 1967.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Muhammad Ali. The passage explicitly states that Muhammad Ali (Cassius Clay) was stripped of his world heavyweight title and had his boxing license suspended after he refused to be inducted into the United States Army in April 1967.'}\n",
      "INFO:__main__:latency=1.6166856249983539\n",
      "INFO:__main__:prompt_tokens=3030\n",
      "INFO:__main__:completion_tokens=45\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3030, completion_tokens=45, latency=1.6167\n",
      "INFO:__main__:e_idx=1/1, chunk_index=37/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3848\n",
      "INFO:bedrock_predictor:Claude completion tokens: 28\n",
      "INFO:__main__:response={'response_json': {'completion': ' Aldo Moro was the former Italian Prime Minister who was kidnapped by The Red Brigade in 1978, and killed after 55 days in captivity.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Aldo Moro was the former Italian Prime Minister who was kidnapped by The Red Brigade in 1978, and killed after 55 days in captivity.'}, 'latency': 1.4363538340039668, 'prompt_tokens': 3848, 'completion_tokens': 28}\n",
      "INFO:__main__:response_json={'completion': ' Aldo Moro was the former Italian Prime Minister who was kidnapped by The Red Brigade in 1978, and killed after 55 days in captivity.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Aldo Moro was the former Italian Prime Minister who was kidnapped by The Red Brigade in 1978, and killed after 55 days in captivity.'}\n",
      "INFO:__main__:latency=1.4363538340039668\n",
      "INFO:__main__:prompt_tokens=3848\n",
      "INFO:__main__:completion_tokens=28\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3848, completion_tokens=28, latency=1.4364\n",
      "INFO:__main__:e_idx=1/1, chunk_index=38/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3208\n",
      "INFO:bedrock_predictor:Claude completion tokens: 40\n",
      "INFO:__main__:response={'response_json': {'completion': ' Cary Grant\\n\\nThe passage provides biographical information about the British-born American film actor Archibald Alexander Leach. It states that he was known by his stage name Cary Grant.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Cary Grant\\n\\nThe passage provides biographical information about the British-born American film actor Archibald Alexander Leach. It states that he was known by his stage name Cary Grant.'}, 'latency': 1.3913269159966148, 'prompt_tokens': 3208, 'completion_tokens': 40}\n",
      "INFO:__main__:response_json={'completion': ' Cary Grant\\n\\nThe passage provides biographical information about the British-born American film actor Archibald Alexander Leach. It states that he was known by his stage name Cary Grant.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Cary Grant\\n\\nThe passage provides biographical information about the British-born American film actor Archibald Alexander Leach. It states that he was known by his stage name Cary Grant.'}\n",
      "INFO:__main__:latency=1.3913269159966148\n",
      "INFO:__main__:prompt_tokens=3208\n",
      "INFO:__main__:completion_tokens=40\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3208, completion_tokens=40, latency=1.3913\n",
      "INFO:__main__:e_idx=1/1, chunk_index=39/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3475\n",
      "INFO:bedrock_predictor:Claude completion tokens: 60\n",
      "INFO:__main__:response={'response_json': {'completion': \" I do not have enough context from the passages provided to determine Blondie's last UK No. 1 song of the 1980s. The passages discuss general information about Blondie and list some of their top songs, but do not specify their last UK No. 1 hit from the 1980s.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" I do not have enough context from the passages provided to determine Blondie's last UK No. 1 song of the 1980s. The passages discuss general information about Blondie and list some of their top songs, but do not specify their last UK No. 1 hit from the 1980s.\"}, 'latency': 2.508380582999962, 'prompt_tokens': 3475, 'completion_tokens': 60}\n",
      "INFO:__main__:response_json={'completion': \" I do not have enough context from the passages provided to determine Blondie's last UK No. 1 song of the 1980s. The passages discuss general information about Blondie and list some of their top songs, but do not specify their last UK No. 1 hit from the 1980s.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" I do not have enough context from the passages provided to determine Blondie's last UK No. 1 song of the 1980s. The passages discuss general information about Blondie and list some of their top songs, but do not specify their last UK No. 1 hit from the 1980s.\"}\n",
      "INFO:__main__:latency=2.508380582999962\n",
      "INFO:__main__:prompt_tokens=3475\n",
      "INFO:__main__:completion_tokens=60\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3475, completion_tokens=60, latency=2.5084\n",
      "INFO:__main__:e_idx=1/1, chunk_index=40/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3681\n",
      "INFO:bedrock_predictor:Claude completion tokens: 142\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the context provided, a traditional hoy boat is powered by sail. Some key details:\\n\\n- The passage describes hoy boats as being sloop-rigged, meaning they had a single mast with a fore-and-aft sail. \\n\\n- It mentions they were used for passenger or cargo transport before the development of steam engines, requiring skillful use of tides and wind. \\n\\n- The introduction of early steamers expanded coastal trade that hoys had previously engaged in. \\n\\n- So in summary, as sailing vessels that predated steam power, traditional hoy boats would have been powered by wind using their fore-and-aft rigged sails.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, a traditional hoy boat is powered by sail. Some key details:\\n\\n- The passage describes hoy boats as being sloop-rigged, meaning they had a single mast with a fore-and-aft sail. \\n\\n- It mentions they were used for passenger or cargo transport before the development of steam engines, requiring skillful use of tides and wind. \\n\\n- The introduction of early steamers expanded coastal trade that hoys had previously engaged in. \\n\\n- So in summary, as sailing vessels that predated steam power, traditional hoy boats would have been powered by wind using their fore-and-aft rigged sails.'}, 'latency': 7.091424540994922, 'prompt_tokens': 3681, 'completion_tokens': 142}\n",
      "INFO:__main__:response_json={'completion': ' Based on the context provided, a traditional hoy boat is powered by sail. Some key details:\\n\\n- The passage describes hoy boats as being sloop-rigged, meaning they had a single mast with a fore-and-aft sail. \\n\\n- It mentions they were used for passenger or cargo transport before the development of steam engines, requiring skillful use of tides and wind. \\n\\n- The introduction of early steamers expanded coastal trade that hoys had previously engaged in. \\n\\n- So in summary, as sailing vessels that predated steam power, traditional hoy boats would have been powered by wind using their fore-and-aft rigged sails.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, a traditional hoy boat is powered by sail. Some key details:\\n\\n- The passage describes hoy boats as being sloop-rigged, meaning they had a single mast with a fore-and-aft sail. \\n\\n- It mentions they were used for passenger or cargo transport before the development of steam engines, requiring skillful use of tides and wind. \\n\\n- The introduction of early steamers expanded coastal trade that hoys had previously engaged in. \\n\\n- So in summary, as sailing vessels that predated steam power, traditional hoy boats would have been powered by wind using their fore-and-aft rigged sails.'}\n",
      "INFO:__main__:latency=7.091424540994922\n",
      "INFO:__main__:prompt_tokens=3681\n",
      "INFO:__main__:completion_tokens=142\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3681, completion_tokens=142, latency=7.0914\n",
      "INFO:__main__:e_idx=1/1, chunk_index=41/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3581\n",
      "INFO:bedrock_predictor:Claude completion tokens: 29\n",
      "INFO:__main__:response={'response_json': {'completion': ' Matthew Webb swam the English Channel and then drowned swimming the Whirlpool Rapids of the Niagara River below Niagara Falls.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Matthew Webb swam the English Channel and then drowned swimming the Whirlpool Rapids of the Niagara River below Niagara Falls.'}, 'latency': 2.075493500000448, 'prompt_tokens': 3581, 'completion_tokens': 29}\n",
      "INFO:__main__:response_json={'completion': ' Matthew Webb swam the English Channel and then drowned swimming the Whirlpool Rapids of the Niagara River below Niagara Falls.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Matthew Webb swam the English Channel and then drowned swimming the Whirlpool Rapids of the Niagara River below Niagara Falls.'}\n",
      "INFO:__main__:latency=2.075493500000448\n",
      "INFO:__main__:prompt_tokens=3581\n",
      "INFO:__main__:completion_tokens=29\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3581, completion_tokens=29, latency=2.0755\n",
      "INFO:__main__:e_idx=1/1, chunk_index=42/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3302\n",
      "INFO:bedrock_predictor:Claude completion tokens: 25\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the provided context, Alexander Graham Bell first demonstrated his telephone invention at the Centennial Exhibition in Philadelphia in 1876.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the provided context, Alexander Graham Bell first demonstrated his telephone invention at the Centennial Exhibition in Philadelphia in 1876.'}, 'latency': 1.4060810000009951, 'prompt_tokens': 3302, 'completion_tokens': 25}\n",
      "INFO:__main__:response_json={'completion': ' Based on the provided context, Alexander Graham Bell first demonstrated his telephone invention at the Centennial Exhibition in Philadelphia in 1876.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the provided context, Alexander Graham Bell first demonstrated his telephone invention at the Centennial Exhibition in Philadelphia in 1876.'}\n",
      "INFO:__main__:latency=1.4060810000009951\n",
      "INFO:__main__:prompt_tokens=3302\n",
      "INFO:__main__:completion_tokens=25\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3302, completion_tokens=25, latency=1.4061\n",
      "INFO:__main__:e_idx=1/1, chunk_index=43/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3614\n",
      "INFO:bedrock_predictor:Claude completion tokens: 61\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages provided, I do not have enough information to determine which city is known as the \"Pearl of the Danube\". The passages are about a Danube River cruise but do not mention any cities by that name. I apologize, but I do not know the answer to the question.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, I do not have enough information to determine which city is known as the \"Pearl of the Danube\". The passages are about a Danube River cruise but do not mention any cities by that name. I apologize, but I do not know the answer to the question.'}, 'latency': 3.010034957995231, 'prompt_tokens': 3614, 'completion_tokens': 61}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages provided, I do not have enough information to determine which city is known as the \"Pearl of the Danube\". The passages are about a Danube River cruise but do not mention any cities by that name. I apologize, but I do not know the answer to the question.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, I do not have enough information to determine which city is known as the \"Pearl of the Danube\". The passages are about a Danube River cruise but do not mention any cities by that name. I apologize, but I do not know the answer to the question.'}\n",
      "INFO:__main__:latency=3.010034957995231\n",
      "INFO:__main__:prompt_tokens=3614\n",
      "INFO:__main__:completion_tokens=61\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3614, completion_tokens=61, latency=3.0100\n",
      "INFO:__main__:e_idx=1/1, chunk_index=44/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3537\n",
      "INFO:bedrock_predictor:Claude completion tokens: 27\n",
      "INFO:__main__:response={'response_json': {'completion': ' In the passage about the 1956 film \"The Ten Commandments\", it states that Charlton Heston played the role of Moses.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' In the passage about the 1956 film \"The Ten Commandments\", it states that Charlton Heston played the role of Moses.'}, 'latency': 1.33485929199378, 'prompt_tokens': 3537, 'completion_tokens': 27}\n",
      "INFO:__main__:response_json={'completion': ' In the passage about the 1956 film \"The Ten Commandments\", it states that Charlton Heston played the role of Moses.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' In the passage about the 1956 film \"The Ten Commandments\", it states that Charlton Heston played the role of Moses.'}\n",
      "INFO:__main__:latency=1.33485929199378\n",
      "INFO:__main__:prompt_tokens=3537\n",
      "INFO:__main__:completion_tokens=27\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3537, completion_tokens=27, latency=1.3349\n",
      "INFO:__main__:e_idx=1/1, chunk_index=45/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3165\n",
      "INFO:bedrock_predictor:Claude completion tokens: 59\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passage information provided, Gary Busey played Buddy Holly in the 1978 film The Buddy Holly Story. The reviews mention that Gary Busey did his own singing for the role and lost 32 lbs to effectively portray Buddy Holly, for which he received an Oscar nomination.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passage information provided, Gary Busey played Buddy Holly in the 1978 film The Buddy Holly Story. The reviews mention that Gary Busey did his own singing for the role and lost 32 lbs to effectively portray Buddy Holly, for which he received an Oscar nomination.'}, 'latency': 2.137779999997292, 'prompt_tokens': 3165, 'completion_tokens': 59}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passage information provided, Gary Busey played Buddy Holly in the 1978 film The Buddy Holly Story. The reviews mention that Gary Busey did his own singing for the role and lost 32 lbs to effectively portray Buddy Holly, for which he received an Oscar nomination.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passage information provided, Gary Busey played Buddy Holly in the 1978 film The Buddy Holly Story. The reviews mention that Gary Busey did his own singing for the role and lost 32 lbs to effectively portray Buddy Holly, for which he received an Oscar nomination.'}\n",
      "INFO:__main__:latency=2.137779999997292\n",
      "INFO:__main__:prompt_tokens=3165\n",
      "INFO:__main__:completion_tokens=59\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3165, completion_tokens=59, latency=2.1378\n",
      "INFO:__main__:e_idx=1/1, chunk_index=46/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3476\n",
      "INFO:bedrock_predictor:Claude completion tokens: 73\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages provided, Hattie Jacques played Miss Pugh on the Tony Hancock radio show. The cast list mentions \"Tony Hancock, Bill Kerr, Sidney James, Kenneth Williams and Hattie Jacques\" without specifying their roles, but other details in the passages refer to \"Miss Pugh\" as a character, indicating that Hattie Jacques portrayed Miss Pugh.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Hattie Jacques played Miss Pugh on the Tony Hancock radio show. The cast list mentions \"Tony Hancock, Bill Kerr, Sidney James, Kenneth Williams and Hattie Jacques\" without specifying their roles, but other details in the passages refer to \"Miss Pugh\" as a character, indicating that Hattie Jacques portrayed Miss Pugh.'}, 'latency': 2.9520329170045443, 'prompt_tokens': 3476, 'completion_tokens': 73}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages provided, Hattie Jacques played Miss Pugh on the Tony Hancock radio show. The cast list mentions \"Tony Hancock, Bill Kerr, Sidney James, Kenneth Williams and Hattie Jacques\" without specifying their roles, but other details in the passages refer to \"Miss Pugh\" as a character, indicating that Hattie Jacques portrayed Miss Pugh.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided, Hattie Jacques played Miss Pugh on the Tony Hancock radio show. The cast list mentions \"Tony Hancock, Bill Kerr, Sidney James, Kenneth Williams and Hattie Jacques\" without specifying their roles, but other details in the passages refer to \"Miss Pugh\" as a character, indicating that Hattie Jacques portrayed Miss Pugh.'}\n",
      "INFO:__main__:latency=2.9520329170045443\n",
      "INFO:__main__:prompt_tokens=3476\n",
      "INFO:__main__:completion_tokens=73\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3476, completion_tokens=73, latency=2.9520\n",
      "INFO:__main__:e_idx=1/1, chunk_index=47/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3235\n",
      "INFO:bedrock_predictor:Claude completion tokens: 14\n",
      "INFO:__main__:response={'response_json': {'completion': ' Nadine Coyle was born in Derry, Northern Ireland.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Nadine Coyle was born in Derry, Northern Ireland.'}, 'latency': 1.2684298329986632, 'prompt_tokens': 3235, 'completion_tokens': 14}\n",
      "INFO:__main__:response_json={'completion': ' Nadine Coyle was born in Derry, Northern Ireland.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Nadine Coyle was born in Derry, Northern Ireland.'}\n",
      "INFO:__main__:latency=1.2684298329986632\n",
      "INFO:__main__:prompt_tokens=3235\n",
      "INFO:__main__:completion_tokens=14\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3235, completion_tokens=14, latency=1.2684\n",
      "INFO:__main__:e_idx=1/1, chunk_index=48/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3363\n",
      "INFO:bedrock_predictor:Claude completion tokens: 38\n",
      "INFO:__main__:response={'response_json': {'completion': ' The passage provides context about the Bargello museum in Florence, Italy. Based on this context, the Bargello and Uffizi are both major art museums located in Florence.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The passage provides context about the Bargello museum in Florence, Italy. Based on this context, the Bargello and Uffizi are both major art museums located in Florence.'}, 'latency': 4.149226583998825, 'prompt_tokens': 3363, 'completion_tokens': 38}\n",
      "INFO:__main__:response_json={'completion': ' The passage provides context about the Bargello museum in Florence, Italy. Based on this context, the Bargello and Uffizi are both major art museums located in Florence.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The passage provides context about the Bargello museum in Florence, Italy. Based on this context, the Bargello and Uffizi are both major art museums located in Florence.'}\n",
      "INFO:__main__:latency=4.149226583998825\n",
      "INFO:__main__:prompt_tokens=3363\n",
      "INFO:__main__:completion_tokens=38\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3363, completion_tokens=38, latency=4.1492\n",
      "INFO:__main__:e_idx=1/1, chunk_index=49/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3882\n",
      "INFO:bedrock_predictor:Claude completion tokens: 72\n",
      "INFO:__main__:response={'response_json': {'completion': ' Guillemots are seabirds in the auk family. The passage discusses two genera of guillemots - Uria and Cepphus. Uria species are also called \"murres\" in North America, while only Cepphus species are called \"guillemots\". So guillemots are a type of small seabird.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Guillemots are seabirds in the auk family. The passage discusses two genera of guillemots - Uria and Cepphus. Uria species are also called \"murres\" in North America, while only Cepphus species are called \"guillemots\". So guillemots are a type of small seabird.'}, 'latency': 1.9089534169979743, 'prompt_tokens': 3882, 'completion_tokens': 72}\n",
      "INFO:__main__:response_json={'completion': ' Guillemots are seabirds in the auk family. The passage discusses two genera of guillemots - Uria and Cepphus. Uria species are also called \"murres\" in North America, while only Cepphus species are called \"guillemots\". So guillemots are a type of small seabird.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Guillemots are seabirds in the auk family. The passage discusses two genera of guillemots - Uria and Cepphus. Uria species are also called \"murres\" in North America, while only Cepphus species are called \"guillemots\". So guillemots are a type of small seabird.'}\n",
      "INFO:__main__:latency=1.9089534169979743\n",
      "INFO:__main__:prompt_tokens=3882\n",
      "INFO:__main__:completion_tokens=72\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3882, completion_tokens=72, latency=1.9090\n",
      "INFO:__main__:e_idx=1/1, chunk_index=50/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3616\n",
      "INFO:bedrock_predictor:Claude completion tokens: 74\n",
      "INFO:__main__:response={'response_json': {'completion': ' The passage mentions that \"Dakota\" was the first Stereophonics single to reach #1 on the UK Singles Chart. \"Dakota\" is a song by the Welsh band Stereophonics. It was the first single from their fifth studio album Language. Sex. Violence. Other? and was released on February 28, 2005.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The passage mentions that \"Dakota\" was the first Stereophonics single to reach #1 on the UK Singles Chart. \"Dakota\" is a song by the Welsh band Stereophonics. It was the first single from their fifth studio album Language. Sex. Violence. Other? and was released on February 28, 2005.'}, 'latency': 2.0856549169984646, 'prompt_tokens': 3616, 'completion_tokens': 74}\n",
      "INFO:__main__:response_json={'completion': ' The passage mentions that \"Dakota\" was the first Stereophonics single to reach #1 on the UK Singles Chart. \"Dakota\" is a song by the Welsh band Stereophonics. It was the first single from their fifth studio album Language. Sex. Violence. Other? and was released on February 28, 2005.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The passage mentions that \"Dakota\" was the first Stereophonics single to reach #1 on the UK Singles Chart. \"Dakota\" is a song by the Welsh band Stereophonics. It was the first single from their fifth studio album Language. Sex. Violence. Other? and was released on February 28, 2005.'}\n",
      "INFO:__main__:latency=2.0856549169984646\n",
      "INFO:__main__:prompt_tokens=3616\n",
      "INFO:__main__:completion_tokens=74\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3616, completion_tokens=74, latency=2.0857\n",
      "INFO:__main__:e_idx=1/1, chunk_index=51/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3567\n",
      "INFO:bedrock_predictor:Claude completion tokens: 145\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passage, the counterintelligence agency that had a name meaning \\'death to spies\\' in English was SMERSH, a Soviet counterintelligence agency that the main character Semion Strogov was a captain in. The passage states that \"Strogov is able to run, jump, crouch, crawl and climb over obstacles, and he is also able to drive vehicles. Taking place in World War II, the player is required to complete various tasks in enemy territory, such as killing targets and stealing documents. Emphasis is placed on stealth, as the player is greatly outnumbered by enemy personnel, and some missions will be failed if the alarm is raised.\" So the agency was SMERSH.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passage, the counterintelligence agency that had a name meaning \\'death to spies\\' in English was SMERSH, a Soviet counterintelligence agency that the main character Semion Strogov was a captain in. The passage states that \"Strogov is able to run, jump, crouch, crawl and climb over obstacles, and he is also able to drive vehicles. Taking place in World War II, the player is required to complete various tasks in enemy territory, such as killing targets and stealing documents. Emphasis is placed on stealth, as the player is greatly outnumbered by enemy personnel, and some missions will be failed if the alarm is raised.\" So the agency was SMERSH.'}, 'latency': 3.1010692499985453, 'prompt_tokens': 3567, 'completion_tokens': 145}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passage, the counterintelligence agency that had a name meaning \\'death to spies\\' in English was SMERSH, a Soviet counterintelligence agency that the main character Semion Strogov was a captain in. The passage states that \"Strogov is able to run, jump, crouch, crawl and climb over obstacles, and he is also able to drive vehicles. Taking place in World War II, the player is required to complete various tasks in enemy territory, such as killing targets and stealing documents. Emphasis is placed on stealth, as the player is greatly outnumbered by enemy personnel, and some missions will be failed if the alarm is raised.\" So the agency was SMERSH.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passage, the counterintelligence agency that had a name meaning \\'death to spies\\' in English was SMERSH, a Soviet counterintelligence agency that the main character Semion Strogov was a captain in. The passage states that \"Strogov is able to run, jump, crouch, crawl and climb over obstacles, and he is also able to drive vehicles. Taking place in World War II, the player is required to complete various tasks in enemy territory, such as killing targets and stealing documents. Emphasis is placed on stealth, as the player is greatly outnumbered by enemy personnel, and some missions will be failed if the alarm is raised.\" So the agency was SMERSH.'}\n",
      "INFO:__main__:latency=3.1010692499985453\n",
      "INFO:__main__:prompt_tokens=3567\n",
      "INFO:__main__:completion_tokens=145\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3567, completion_tokens=145, latency=3.1011\n",
      "INFO:__main__:e_idx=1/1, chunk_index=52/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3330\n",
      "INFO:bedrock_predictor:Claude completion tokens: 18\n",
      "INFO:__main__:response={'response_json': {'completion': ' The passage states that King George II was born at Herrenhausen, Hanover.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The passage states that King George II was born at Herrenhausen, Hanover.'}, 'latency': 1.585772457998246, 'prompt_tokens': 3330, 'completion_tokens': 18}\n",
      "INFO:__main__:response_json={'completion': ' The passage states that King George II was born at Herrenhausen, Hanover.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The passage states that King George II was born at Herrenhausen, Hanover.'}\n",
      "INFO:__main__:latency=1.585772457998246\n",
      "INFO:__main__:prompt_tokens=3330\n",
      "INFO:__main__:completion_tokens=18\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3330, completion_tokens=18, latency=1.5858\n",
      "INFO:__main__:e_idx=1/1, chunk_index=53/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3566\n",
      "INFO:bedrock_predictor:Claude completion tokens: 22\n",
      "INFO:__main__:response={'response_json': {'completion': ' Grace Slick was the main female singer with the groups Jefferson Airplane, Jefferson Starship and Starship.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Grace Slick was the main female singer with the groups Jefferson Airplane, Jefferson Starship and Starship.'}, 'latency': 1.248238082996977, 'prompt_tokens': 3566, 'completion_tokens': 22}\n",
      "INFO:__main__:response_json={'completion': ' Grace Slick was the main female singer with the groups Jefferson Airplane, Jefferson Starship and Starship.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Grace Slick was the main female singer with the groups Jefferson Airplane, Jefferson Starship and Starship.'}\n",
      "INFO:__main__:latency=1.248238082996977\n",
      "INFO:__main__:prompt_tokens=3566\n",
      "INFO:__main__:completion_tokens=22\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3566, completion_tokens=22, latency=1.2482\n",
      "INFO:__main__:e_idx=1/1, chunk_index=54/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3497\n",
      "INFO:bedrock_predictor:Claude completion tokens: 100\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the context provided, the English cities of Gloucester and Worcester are on the River Severn. The passage mentions that the Worcester & Birmingham Canal provides access to the Severn at Diglis Basins and Worcester. It also mentions that the Gloucester & Berkeley Canal opening in 1827 improved traffic on the Worcester & Birmingham Canal. As the Gloucester & Berkeley Canal connects Gloucester to the River Severn, this implies that Gloucester is also on the River Severn.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, the English cities of Gloucester and Worcester are on the River Severn. The passage mentions that the Worcester & Birmingham Canal provides access to the Severn at Diglis Basins and Worcester. It also mentions that the Gloucester & Berkeley Canal opening in 1827 improved traffic on the Worcester & Birmingham Canal. As the Gloucester & Berkeley Canal connects Gloucester to the River Severn, this implies that Gloucester is also on the River Severn.'}, 'latency': 2.8398512920030043, 'prompt_tokens': 3497, 'completion_tokens': 100}\n",
      "INFO:__main__:response_json={'completion': ' Based on the context provided, the English cities of Gloucester and Worcester are on the River Severn. The passage mentions that the Worcester & Birmingham Canal provides access to the Severn at Diglis Basins and Worcester. It also mentions that the Gloucester & Berkeley Canal opening in 1827 improved traffic on the Worcester & Birmingham Canal. As the Gloucester & Berkeley Canal connects Gloucester to the River Severn, this implies that Gloucester is also on the River Severn.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the context provided, the English cities of Gloucester and Worcester are on the River Severn. The passage mentions that the Worcester & Birmingham Canal provides access to the Severn at Diglis Basins and Worcester. It also mentions that the Gloucester & Berkeley Canal opening in 1827 improved traffic on the Worcester & Birmingham Canal. As the Gloucester & Berkeley Canal connects Gloucester to the River Severn, this implies that Gloucester is also on the River Severn.'}\n",
      "INFO:__main__:latency=2.8398512920030043\n",
      "INFO:__main__:prompt_tokens=3497\n",
      "INFO:__main__:completion_tokens=100\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3497, completion_tokens=100, latency=2.8399\n",
      "INFO:__main__:e_idx=1/1, chunk_index=55/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3003\n",
      "INFO:bedrock_predictor:Claude completion tokens: 70\n",
      "INFO:__main__:response={'response_json': {'completion': ' The bassoon. The passage mentions that in music, the obsolete term \"fagot\" referred to a \"fagotto, or bassoon\". The bassoon gets its name \"fagotto\" in Italian which translates to \"bundle of sticks\", referring to how the bassoon is constructed from a bundle of reed pipes.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The bassoon. The passage mentions that in music, the obsolete term \"fagot\" referred to a \"fagotto, or bassoon\". The bassoon gets its name \"fagotto\" in Italian which translates to \"bundle of sticks\", referring to how the bassoon is constructed from a bundle of reed pipes.'}, 'latency': 2.7011902910016943, 'prompt_tokens': 3003, 'completion_tokens': 70}\n",
      "INFO:__main__:response_json={'completion': ' The bassoon. The passage mentions that in music, the obsolete term \"fagot\" referred to a \"fagotto, or bassoon\". The bassoon gets its name \"fagotto\" in Italian which translates to \"bundle of sticks\", referring to how the bassoon is constructed from a bundle of reed pipes.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The bassoon. The passage mentions that in music, the obsolete term \"fagot\" referred to a \"fagotto, or bassoon\". The bassoon gets its name \"fagotto\" in Italian which translates to \"bundle of sticks\", referring to how the bassoon is constructed from a bundle of reed pipes.'}\n",
      "INFO:__main__:latency=2.7011902910016943\n",
      "INFO:__main__:prompt_tokens=3003\n",
      "INFO:__main__:completion_tokens=70\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3003, completion_tokens=70, latency=2.7012\n",
      "INFO:__main__:e_idx=1/1, chunk_index=56/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3783\n",
      "INFO:bedrock_predictor:Claude completion tokens: 146\n",
      "INFO:__main__:response={'response_json': {'completion': ' Based on the passages provided:\\n\\n1. Gordon Jackson, a Scottish actor, was known for playing the role of the butler Hudson in the television series \"Upstairs, Downstairs\". \\n2. The passages describe Gordon Jackson as the actor who portrayed the crotchety butler Hudson in \"Upstairs, Downstairs\", which brought him wide fame. \\n3. As Hudson, Gordon Jackson presided over the downstairs servants\\' world with a stern hand but soft heart, providing a sense of duty and propriety for the aristocratic Bellamy family upstairs.\\n\\nTherefore, the Scottish actor who played the butler Hudson in the original series of ITV\\'s Upstairs, Downstairs was Gordon Jackson.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided:\\n\\n1. Gordon Jackson, a Scottish actor, was known for playing the role of the butler Hudson in the television series \"Upstairs, Downstairs\". \\n2. The passages describe Gordon Jackson as the actor who portrayed the crotchety butler Hudson in \"Upstairs, Downstairs\", which brought him wide fame. \\n3. As Hudson, Gordon Jackson presided over the downstairs servants\\' world with a stern hand but soft heart, providing a sense of duty and propriety for the aristocratic Bellamy family upstairs.\\n\\nTherefore, the Scottish actor who played the butler Hudson in the original series of ITV\\'s Upstairs, Downstairs was Gordon Jackson.'}, 'latency': 3.201815250002255, 'prompt_tokens': 3783, 'completion_tokens': 146}\n",
      "INFO:__main__:response_json={'completion': ' Based on the passages provided:\\n\\n1. Gordon Jackson, a Scottish actor, was known for playing the role of the butler Hudson in the television series \"Upstairs, Downstairs\". \\n2. The passages describe Gordon Jackson as the actor who portrayed the crotchety butler Hudson in \"Upstairs, Downstairs\", which brought him wide fame. \\n3. As Hudson, Gordon Jackson presided over the downstairs servants\\' world with a stern hand but soft heart, providing a sense of duty and propriety for the aristocratic Bellamy family upstairs.\\n\\nTherefore, the Scottish actor who played the butler Hudson in the original series of ITV\\'s Upstairs, Downstairs was Gordon Jackson.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Based on the passages provided:\\n\\n1. Gordon Jackson, a Scottish actor, was known for playing the role of the butler Hudson in the television series \"Upstairs, Downstairs\". \\n2. The passages describe Gordon Jackson as the actor who portrayed the crotchety butler Hudson in \"Upstairs, Downstairs\", which brought him wide fame. \\n3. As Hudson, Gordon Jackson presided over the downstairs servants\\' world with a stern hand but soft heart, providing a sense of duty and propriety for the aristocratic Bellamy family upstairs.\\n\\nTherefore, the Scottish actor who played the butler Hudson in the original series of ITV\\'s Upstairs, Downstairs was Gordon Jackson.'}\n",
      "INFO:__main__:latency=3.201815250002255\n",
      "INFO:__main__:prompt_tokens=3783\n",
      "INFO:__main__:completion_tokens=146\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3783, completion_tokens=146, latency=3.2018\n",
      "INFO:__main__:e_idx=1/1, chunk_index=57/57\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 3271\n",
      "INFO:bedrock_predictor:Claude completion tokens: 38\n",
      "INFO:__main__:response={'response_json': {'completion': ' The Owl and the Pussy-Cat sailed to the land where the bong-tree grows according to the excerpt from the poem \"The Owl and the Pussy-Cat\".', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The Owl and the Pussy-Cat sailed to the land where the bong-tree grows according to the excerpt from the poem \"The Owl and the Pussy-Cat\".'}, 'latency': 1.3156322499999078, 'prompt_tokens': 3271, 'completion_tokens': 38}\n",
      "INFO:__main__:response_json={'completion': ' The Owl and the Pussy-Cat sailed to the land where the bong-tree grows according to the excerpt from the poem \"The Owl and the Pussy-Cat\".', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The Owl and the Pussy-Cat sailed to the land where the bong-tree grows according to the excerpt from the poem \"The Owl and the Pussy-Cat\".'}\n",
      "INFO:__main__:latency=1.3156322499999078\n",
      "INFO:__main__:prompt_tokens=3271\n",
      "INFO:__main__:completion_tokens=38\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 3271, completion_tokens=38, latency=1.3156\n",
      "INFO:__main__:the claude-instant-v1 ran for 181.81455199999618 seconds......\n",
      "INFO:__main__:metrics json is: {'experiment_name': 'claude-instant-v1', 'concurrency': 1, 'payload_file': 'payload_en_3000-4000.jsonl', 'errors': [], 'successes': 1, 'error_rate': 0.0, 'all_prompts_token_count': 3271, 'prompt_token_count_mean': 3271.0, 'prompt_token_throughput': 2322.99, 'all_completions_token_count': 38, 'completion_token_count_mean': 38.0, 'completion_token_throughput': 26.99, 'transactions': 1, 'transactions_per_second': 0.71, 'transactions_per_minute': 42, 'latency_mean': 1.3156322499999078}\n",
      "INFO:bedrock_predictor:pricing dict: {'input-per-1k-tokens': 0.0008}\n",
      "INFO:bedrock_predictor:input per 1k token pricing: 0.0008\n",
      "INFO:bedrock_predictor:pricing dict: {'output-per-1k-tokens': 0.0024}\n",
      "INFO:bedrock_predictor:output per 1k token pricing: 0.0024\n",
      "INFO:__main__:the rate for running claude-instant-v1 running on ClaudeInstant-ODT for 181.81455199999618 is $0.002708....\n",
      "INFO:__main__:experiment=1/1, name=claude-instant-v1, duration=181.81 seconds, done\n",
      "INFO:__main__:experiment durations:      experiment_name      instance_type duration_in_seconds  cost\n",
      "0  claude-instant-v1  ClaudeInstant-ODT              181.81  0.00\n",
      "INFO:__main__:Summary for cost of instance per endpoint per run saved to s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/endpoint_per_instance_per_run_costs.csv\n",
      "INFO:__main__:total cost of all experiments: $0.0\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "# Initializing the total model instance cost to 0\n",
    "total_model_instance_cost: int = 0\n",
    "\n",
    "## To keep track of the cost for all model endpoints\n",
    "cost_data = []\n",
    "\n",
    "## To keep track of the experiment durations and the time it takes for the model endpoint to be in service to calculate cost association\n",
    "experiment_durations = []  \n",
    "\n",
    "## start the timer before the start of inferences\n",
    "current_time = datetime.now(timezone.utc)\n",
    "logger.info(f\"Current time recorded while running this experiment is {current_time}..... deployed models are going to start inferences...\")\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    experiment_start_time = time.perf_counter()  # Start timer for the experiment\n",
    "\n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                metrics_json = json.dumps(metrics, indent=2)\n",
    "                metrics_file_name = f\"{time.time()}.json\"\n",
    "                metrics_s3_path = os.path.join(METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "                write_to_s3(metrics_json, config['aws']['bucket'], \"\", METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    response_json = json.dumps(r, indent=2)\n",
    "                    response_file_name = f\"{time.time()}.json\"\n",
    "                    response_s3_path = os.path.join(METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "                    write_to_s3(response_json, config['aws']['bucket'], \"\", METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "    \n",
    "    ## initializing the experiment cost\n",
    "    exp_cost = 0\n",
    "    \n",
    "    # Experiment done, stopping the timer for this given experiment\n",
    "    experiment_end_time = time.perf_counter()\n",
    "\n",
    "    # calculating the duration of this given endpoint inference time\n",
    "    experiment_duration = experiment_end_time - experiment_start_time\n",
    "    logger.info(f\"the {experiment['name']} ran for {experiment_duration} seconds......\")\n",
    "\n",
    "    # calculating the per second cost for this instance type\n",
    "    exp_instance_type: str = experiment['instance_type']\n",
    "\n",
    "    # ## add an if statement here for bedrock versus sagemaker [ if sagemaker call, then hourly, if bedrock, then through the number of tokens ]\n",
    "\n",
    "    # # price of the given instance for this experiment \n",
    "    # hourly_rate = config['pricing'].get(experiment['instance_type'], 0)\n",
    "    # logger.info(f\"the hourly rate for {experiment['name']} running on {exp_instance_type} is {hourly_rate}\")\n",
    "\n",
    "    # cost_per_second = hourly_rate / 3600\n",
    "    # logger.info(f\"the rate for {experiment['name']} running on {exp_instance_type} is {cost_per_second} per second\")\n",
    "    \n",
    "    #cost for this given exp\n",
    "    logger.info(f\"metrics json is: {metrics}\")\n",
    "    # exp_cost = experiment_duration * cost_per_second\n",
    "    exp_cost = predictor.calculate_cost(exp_instance_type, config, experiment_duration, metrics)\n",
    "    logger.info(f\"the rate for running {experiment['name']} running on {exp_instance_type} for {experiment_duration} is ${exp_cost}....\")\n",
    "\n",
    "    ## tracking the total cost\n",
    "    total_model_instance_cost += exp_cost\n",
    "\n",
    "    experiment_durations.append({\n",
    "        'experiment_name': experiment['name'],\n",
    "        'instance_type': exp_instance_type, \n",
    "        'duration_in_seconds': f\"{experiment_duration:.2f}\", \n",
    "        'cost': f\"{exp_cost:.2f}\", \n",
    "    })\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, duration={experiment_duration:.2f} seconds, done\")\n",
    "\n",
    "# experiment_durations.append({'total_cost': f\"${total_model_instance_cost:.2f}\"})\n",
    "\n",
    "# After all experiments are done, summarize and optionally save experiment durations along with costs\n",
    "df_durations = pd.DataFrame(experiment_durations)\n",
    "logger.info(f\"experiment durations: {df_durations}\")\n",
    "\n",
    "# Convert the DataFrame to CSV and write it to S3 or wherever you prefer\n",
    "csv_buffer_cost = io.StringIO()\n",
    "df_durations.to_csv(csv_buffer_cost, index=False)\n",
    "experiment_associated_cost = csv_buffer_cost.getvalue()\n",
    "\n",
    "# Assuming write_to_s3() is already defined and configured correctly\n",
    "write_to_s3(experiment_associated_cost, config['aws']['bucket'], \"\", METRICS_DIR, SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE)\n",
    "logger.info(f\"Summary for cost of instance per endpoint per run saved to s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE}\")\n",
    "\n",
    "logger.info(f\"total cost of all experiments: ${sum(df_durations.cost.astype(float))}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fmbench.utils:found 57 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/per_inference, suffix=.json\n",
      "INFO:fmbench.utils:there are total of 57 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/per_inference, suffix=.json\n",
      "INFO:__main__:created dataframe of shape (57, 10) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>ContentType</th>\n",
       "      <th>Accept</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the context passages provided, Indra...</td>\n",
       "      <td>3000</td>\n",
       "      <td>73</td>\n",
       "      <td>1.788431</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the context provided, Peter Rosegger...</td>\n",
       "      <td>3896</td>\n",
       "      <td>67</td>\n",
       "      <td>1.979334</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the passages provided, Erich Haenisc...</td>\n",
       "      <td>3789</td>\n",
       "      <td>49</td>\n",
       "      <td>1.881439</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the passages provided, Thomas E. Noe...</td>\n",
       "      <td>3450</td>\n",
       "      <td>41</td>\n",
       "      <td>2.207101</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Ding Yaping and Johann Christian Gustav L...</td>\n",
       "      <td>3482</td>\n",
       "      <td>60</td>\n",
       "      <td>1.874364</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 endpoint_name  \\\n",
       "0  anthropic.claude-instant-v1   \n",
       "1  anthropic.claude-instant-v1   \n",
       "2  anthropic.claude-instant-v1   \n",
       "3  anthropic.claude-instant-v1   \n",
       "4  anthropic.claude-instant-v1   \n",
       "\n",
       "                                              prompt       ContentType  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "\n",
       "             Accept                                         completion  \\\n",
       "0  application/json   Based on the context passages provided, Indra...   \n",
       "1  application/json   Based on the context provided, Peter Rosegger...   \n",
       "2  application/json   Based on the passages provided, Erich Haenisc...   \n",
       "3  application/json   Based on the passages provided, Thomas E. Noe...   \n",
       "4  application/json   No, Ding Yaping and Johann Christian Gustav L...   \n",
       "\n",
       "   prompt_tokens  completion_tokens   latency    experiment_name  concurrency  \n",
       "0           3000                 73  1.788431  claude-instant-v1            1  \n",
       "1           3896                 67  1.979334  claude-instant-v1            1  \n",
       "2           3789                 49  1.881439  claude-instant-v1            1  \n",
       "3           3450                 41  2.207101  claude-instant-v1            1  \n",
       "4           3482                 60  1.874364  claude-instant-v1            1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_INFERENCE_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "                     s3_files))\n",
    "\n",
    "# Create DataFrame\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fmbench.utils:found 57 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/per_chunk, suffix=.json\n",
      "INFO:fmbench.utils:there are total of 57 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/per_chunk, suffix=.json\n",
      "INFO:__main__:created dataframe of shape (57, 16) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1550.24</td>\n",
       "      <td>73</td>\n",
       "      <td>73.0</td>\n",
       "      <td>37.72</td>\n",
       "      <td>1</td>\n",
       "      <td>0.52</td>\n",
       "      <td>31</td>\n",
       "      <td>1.788431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3896</td>\n",
       "      <td>3896.0</td>\n",
       "      <td>1883.15</td>\n",
       "      <td>67</td>\n",
       "      <td>67.0</td>\n",
       "      <td>32.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>28</td>\n",
       "      <td>1.979334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3789</td>\n",
       "      <td>3789.0</td>\n",
       "      <td>1925.30</td>\n",
       "      <td>49</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>30</td>\n",
       "      <td>1.881439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3450</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>1503.28</td>\n",
       "      <td>41</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.44</td>\n",
       "      <td>26</td>\n",
       "      <td>2.207101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3482</td>\n",
       "      <td>3482.0</td>\n",
       "      <td>1770.74</td>\n",
       "      <td>60</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>30</td>\n",
       "      <td>1.874364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     experiment_name  concurrency                payload_file errors  \\\n",
       "0  claude-instant-v1            1  payload_en_3000-4000.jsonl     []   \n",
       "1  claude-instant-v1            1  payload_en_3000-4000.jsonl     []   \n",
       "2  claude-instant-v1            1  payload_en_3000-4000.jsonl     []   \n",
       "3  claude-instant-v1            1  payload_en_3000-4000.jsonl     []   \n",
       "4  claude-instant-v1            1  payload_en_3000-4000.jsonl     []   \n",
       "\n",
       "   successes  error_rate  all_prompts_token_count  prompt_token_count_mean  \\\n",
       "0          1         0.0                     3000                   3000.0   \n",
       "1          1         0.0                     3896                   3896.0   \n",
       "2          1         0.0                     3789                   3789.0   \n",
       "3          1         0.0                     3450                   3450.0   \n",
       "4          1         0.0                     3482                   3482.0   \n",
       "\n",
       "   prompt_token_throughput  all_completions_token_count  \\\n",
       "0                  1550.24                           73   \n",
       "1                  1883.15                           67   \n",
       "2                  1925.30                           49   \n",
       "3                  1503.28                           41   \n",
       "4                  1770.74                           60   \n",
       "\n",
       "   completion_token_count_mean  completion_token_throughput  transactions  \\\n",
       "0                         73.0                        37.72             1   \n",
       "1                         67.0                        32.38             1   \n",
       "2                         49.0                        24.90             1   \n",
       "3                         41.0                        17.87             1   \n",
       "4                         60.0                        30.51             1   \n",
       "\n",
       "   transactions_per_second  transactions_per_minute  latency_mean  \n",
       "0                     0.52                       31      1.788431  \n",
       "1                     0.48                       28      1.979334  \n",
       "2                     0.51                       30      1.881439  \n",
       "3                     0.44                       26      2.207101  \n",
       "4                     0.51                       30      1.874364  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_CHUNK_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "                     s3_files))\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:the instance type: ClaudeInstant-ODT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'ContentType', 'Accept', 'completion',\n",
      "       'prompt_tokens', 'completion_tokens', 'latency', 'experiment_name',\n",
      "       'concurrency'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>ContentType</th>\n",
       "      <th>Accept</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the context passages provided, Indra...</td>\n",
       "      <td>3000</td>\n",
       "      <td>73</td>\n",
       "      <td>1.788431</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the context provided, Peter Rosegger...</td>\n",
       "      <td>3896</td>\n",
       "      <td>67</td>\n",
       "      <td>1.979334</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the passages provided, Erich Haenisc...</td>\n",
       "      <td>3789</td>\n",
       "      <td>49</td>\n",
       "      <td>1.881439</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the passages provided, Thomas E. Noe...</td>\n",
       "      <td>3450</td>\n",
       "      <td>41</td>\n",
       "      <td>2.207101</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Ding Yaping and Johann Christian Gustav L...</td>\n",
       "      <td>3482</td>\n",
       "      <td>60</td>\n",
       "      <td>1.874364</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 endpoint_name  \\\n",
       "0  anthropic.claude-instant-v1   \n",
       "1  anthropic.claude-instant-v1   \n",
       "2  anthropic.claude-instant-v1   \n",
       "3  anthropic.claude-instant-v1   \n",
       "4  anthropic.claude-instant-v1   \n",
       "\n",
       "                                              prompt       ContentType  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "\n",
       "             Accept                                         completion  \\\n",
       "0  application/json   Based on the context passages provided, Indra...   \n",
       "1  application/json   Based on the context provided, Peter Rosegger...   \n",
       "2  application/json   Based on the passages provided, Erich Haenisc...   \n",
       "3  application/json   Based on the passages provided, Thomas E. Noe...   \n",
       "4  application/json   No, Ding Yaping and Johann Christian Gustav L...   \n",
       "\n",
       "   prompt_tokens  completion_tokens   latency    experiment_name  concurrency  \\\n",
       "0           3000                 73  1.788431  claude-instant-v1            1   \n",
       "1           3896                 67  1.979334  claude-instant-v1            1   \n",
       "2           3789                 49  1.881439  claude-instant-v1            1   \n",
       "3           3450                 41  2.207101  claude-instant-v1            1   \n",
       "4           3482                 60  1.874364  claude-instant-v1            1   \n",
       "\n",
       "       instance_type EndpointName ModelName Image S3Uri  \n",
       "0  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "1  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "2  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "3  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "4  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if endpoint_info_list:\n",
    "    df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "    df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "    cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "    print(cols_for_env)\n",
    "    cols_of_interest = ['experiment_name',\n",
    "                        'instance_type',\n",
    "                        'endpoint.EndpointName',\n",
    "                        'model_config.ModelName',\n",
    "                        'model_config.PrimaryContainer.Image',\n",
    "                        'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "    cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "    df_endpoints = df_endpoints[cols_of_interest]\n",
    "    cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "    df_endpoints.columns = cols_of_interest_renamed\n",
    "else:\n",
    "    # Create an empty DataFrame with the desired columns\n",
    "    df_endpoints = pd.DataFrame(columns=['experiment_name',\n",
    "                                         'instance_type',\n",
    "                                         'EndpointName',\n",
    "                                         'ModelName',\n",
    "                                         'Image',\n",
    "                                         'S3Uri'])\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "for e, experiment in enumerate(config['experiments']):\n",
    "    experiment_name = experiment['name']\n",
    "    instance_type = experiment['instance_type']\n",
    "    \n",
    "    logger.info(f\"the instance type: {instance_type}\")\n",
    "    # Update the instance_type column in df_results where the EndpointName matches\n",
    "    df_results.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:results s3 path for per inference csv --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/per_inference_request_results.csv\n",
      "INFO:__main__:saved results dataframe of shape=(57, 15) in s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert df_results to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_results.to_csv(csv_buffer, index=False)\n",
    "csv_data_results = csv_buffer.getvalue()\n",
    "results_file_name = config['report']['per_inference_request_file'].format(datetime=date_time)\n",
    "results_s3_path = os.path.join(METRICS_DIR, results_file_name)\n",
    "logger.info(f\"results s3 path for per inference csv --> {results_s3_path}\")\n",
    "write_to_s3(csv_data_results, config['aws']['bucket'], \"\", METRICS_DIR, results_file_name)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in s3://{BUCKET_NAME}/{results_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:the metrics metadata path is saved here --> metadata/metrics_path.txt\n",
      "INFO:__main__:the information on the defined path for results on these metrics are given in this --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45\n"
     ]
    }
   ],
   "source": [
    "# Ensure the metadata directory exists\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "# Path for the metrics_path.txt file\n",
    "metrics_path_file = os.path.join(METADATA_DIR, 'metrics_path.txt')\n",
    "logger.info(f\"the metrics metadata path is saved here --> {metrics_path_file}\")\n",
    "\n",
    "# Write the METRICS_DIR to metrics_path.txt\n",
    "with open(metrics_path_file, 'w') as file:\n",
    "    file.write(METRICS_DIR)\n",
    "\n",
    "## Write this data to S3\n",
    "write_to_s3(METRICS_DIR, config['aws']['bucket'], \"\", DATA_DIR, 'metrics_path.txt')\n",
    "\n",
    "logger.info(f\"the information on the defined path for results on these metrics are given in this --> {METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:df_metrics cols = Index(['experiment_name', 'concurrency', 'payload_file', 'errors', 'successes',\n",
      "       'error_rate', 'all_prompts_token_count', 'prompt_token_count_mean',\n",
      "       'prompt_token_throughput', 'all_completions_token_count',\n",
      "       'completion_token_count_mean', 'completion_token_throughput',\n",
      "       'transactions', 'transactions_per_second', 'transactions_per_minute',\n",
      "       'latency_mean'],\n",
      "      dtype='object')\n",
      "INFO:__main__:df_endpoints cols = Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri'],\n",
      "      dtype='object')\n",
      "INFO:__main__:the instance type: ClaudeInstant-ODT\n",
      "INFO:__main__:results s3 path for metrics csv --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/all_metrics.csv\n",
      "INFO:__main__:saved metrics results dataframe of shape=(57, 21) in s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=21/hh=20/mm=45/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# logger.info(f\"df_metrics cols = {df_metrics.columns}\")\n",
    "# logger.info(f\"df_endpoints cols = {df_endpoints.columns}\")\n",
    "# df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "# for e, experiment in enumerate(config['experiments']):\n",
    "#     experiment_name = experiment['name']\n",
    "#     instance_type = experiment['instance_type']\n",
    "    \n",
    "#     logger.info(f\"the instance type: {instance_type}\")\n",
    "#     # Update the instance_type column in df_results where the EndpointName matches\n",
    "#     df_metrics.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "\n",
    "# df_metrics.head()\n",
    "\n",
    "# # Convert df_metrics to CSV and write to S3\n",
    "# csv_buffer = io.StringIO()\n",
    "# df_metrics.to_csv(csv_buffer, index=False)\n",
    "# csv_data_metrics = csv_buffer.getvalue()\n",
    "# metrics_file_name = config['report']['all_metrics_file'].format(datetime=date_time)\n",
    "# metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "# logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "# write_to_s3(csv_data_metrics, config['aws']['bucket'], \"\", METRICS_DIR, metrics_file_name)\n",
    "# logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{config['aws']['bucket']}/{metrics_s3_path}\")\n",
    "\n",
    "logger.info(f\"df_metrics cols = {df_metrics.columns}\")\n",
    "logger.info(f\"df_endpoints cols = {df_endpoints.columns}\")\n",
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "for e, experiment in enumerate(config['experiments']):\n",
    "    experiment_name = experiment['name']\n",
    "    instance_type = experiment['instance_type']\n",
    "    \n",
    "    logger.info(f\"the instance type: {instance_type}\")\n",
    "    # Update the instance_type column in df_results where the EndpointName matches\n",
    "    df_metrics.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "df_metrics.head()\n",
    "\n",
    "# Convert df_metrics to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_metrics.to_csv(csv_buffer, index=False)\n",
    "csv_data_metrics = csv_buffer.getvalue()\n",
    "metrics_file_name = config['report']['all_metrics_file'].format(datetime=date_time)\n",
    "metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "write_to_s3(csv_data_metrics, config['aws']['bucket'], \"\", METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{config['aws']['bucket']}/{metrics_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
