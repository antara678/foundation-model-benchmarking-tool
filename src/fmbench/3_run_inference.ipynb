{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/madhurpt/Library/Application Support/sagemaker/config.yaml\n",
      "config file current -> configs/config-claude-models.yml, None\n",
      "Loaded config: {'general': {'name': 'fmbench-claude', 'model_name': 'claude'}, 'aws': {'region': 'us-east-1', 'sagemaker_execution_role': 'arn:aws:iam::121797993273:user/ab3', 'bucket': 'sagemaker-fmbench-write-121797993273'}, 'dir_paths': {'data_prefix': 'data', 'prompts_prefix': 'prompts', 'all_prompts_file': 'all_prompts.csv', 'metrics_dir': 'metrics', 'models_dir': 'models', 'metadata_dir': 'metadata'}, 's3_read_data': {'read_bucket': 'sagemaker-fmbench-read-121797993273', 'scripts_prefix': 'scripts', 'script_files': ['hf_token.txt'], 'source_data_prefix': 'source_data', 'tokenizer_prefix': 'tokenizer', 'prompt_template_dir': 'prompt_template', 'prompt_template_file': 'prompt_template.txt'}, 'run_steps': {'0_setup.ipynb': True, '1_generate_data.ipynb': True, '2_deploy_model.ipynb': False, '3_run_inference.ipynb': True, '4_model_metric_analysis.ipynb': True, '5_cleanup.ipynb': True}, 'datasets': {'prompt_template_keys': ['input', 'context'], 'filters': [{'language': 'en', 'min_length_in_tokens': 1, 'max_length_in_tokens': 500, 'payload_file': 'payload_en_1-500.jsonl'}, {'language': 'en', 'min_length_in_tokens': 500, 'max_length_in_tokens': 1000, 'payload_file': 'payload_en_500-1000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 1000, 'max_length_in_tokens': 2000, 'payload_file': 'payload_en_1000-2000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 2000, 'max_length_in_tokens': 3000, 'payload_file': 'payload_en_2000-3000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 3000, 'max_length_in_tokens': 4000, 'payload_file': 'payload_en_3000-4000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 305, 'max_length_in_tokens': 3997, 'payload_file': 'payload_en_305-3997.jsonl'}]}, 'metrics': {'dataset_of_interest': 'en_1000-2000', 'weights': {'price_per_tx_wt': 0.65, 'latenct_wt': 0.35}}, 'pricing': {'ml.m5.xlarge': 0.23, 'ml.g5.xlarge': 1.006, 'ml.g5.2xlarge': 1.212, 'ml.g5.12xlarge': 7.09, 'ml.g5.24xlarge': 10.18, 'ml.g5.48xlarge': 20.36, 'ml.inf2.24xlarge': 7.79, 'ml.inf2.48xlarge': 15.58, 'ml.p4d.24xlarge': 37.688, 'ml.p3.2xlarge': 3.825, 'Claudev3-Haiku-ODT': [{'input-per-1k-tokens': 0.00025}, {'output-per-1k-tokens': 0.00125}], 'Claudev3-Sonnet-ODT': [{'input-per-1k-tokens': 0.003}, {'output-per-1k-tokens': 0.015}], 'ClaudeV2-ODT': [{'input-per-1k-tokens': 0.0008}, {'output-per-1k-tokens': 0.0024}], 'ClaudeV2:1-ODT': [{'input-per-1k-tokens': 0.0008}, {'output-per-1k-tokens': 0.0024}], 'ClaudeInstant-ODT': [{'input-per-1k-tokens': 0.0008}, {'output-per-1k-tokens': 0.0024}], 'titan-emb-text-ODT': [{'input-per-1k-tokens': 0.0001}, {'output-per-1k-tokens': 0}], 'titan-text-lite-ODT': [{'input-per-1k-tokens': 0.0003}, {'output-per-1k-tokens': 0.0004}], 'titan-text-express-ODT': [{'input-per-1k-tokens': 0.0008}, {'output-per-1k-tokens': 0.0016}], 'Mistral-ODT': [{'input-per-1k-tokens': 0.00015}, {'output-per-1k-tokens': 0.0002}], 'Mixtral-ODT': [{'input-per-1k-tokens': 0.00045}, {'output-per-1k-tokens': 0.0007}], 'Llama13b-ODT': [{'input-per-1k-tokens': 0.00075}, {'output-per-1k-tokens': 0.001}], 'Llama70b-ODT': [{'input-per-1k-tokens': 0.00195}, {'output-per-1k-tokens': 0.00195}], 'AI21mid-ODT': [{'input-per-1k-tokens': 0.0125}, {'output-per-1k-tokens': 0.0125}], 'AI21ultra-ODT': [{'input-per-1k-tokens': 0.0188}, {'output-per-1k-tokens': 0.0188}], 'CohereCommand-ODT': [{'input-per-1k-tokens': 0.0015}, {'output-per-1k-tokens': 0.002}], 'CohereCommLight-ODT': [{'input-per-1k-tokens': 0.0003}, {'output-per-1k-tokens': 0.0006}]}, 'inference_parameters': {'ContentType': 'application/json', 'Accept': 'application/json'}, 'experiments': [{'name': 'claude-instant-v1', 'model_id': 'anthropic.claude-instant-v1', 'model_version': '*', 'model_name': 'claude-instant-v1', 'ep_name': 'anthropic.claude-instant-v1', 'instance_type': 'ClaudeInstant-ODT', 'image_uri': None, 'deploy': False, 'instance_count': 1, 'deployment_script': None, 'inference_script': 'bedrock_predictor.py', 'inference_spec': {'split_input_and_parameters': False}, 'payload_files': ['payload_en_1000-2000.jsonl'], 'concurrency_levels': [1], 'env': None}], 'report': {'per_inference_request_file': 'per_inference_request_results.csv', 'all_metrics_file': 'all_metrics.csv', 'txn_count_for_showing_cost': 100000, 'v_shift_w_single_instance': 0.025, 'v_shift_w_gt_one_instance': 0.025}}\n",
      "CustomTokenizer, based on HF transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files found in S3 Bucket: 'sagemaker-fmbench-read-121797993273' with Prefix: 'tokenizer'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import botocore\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import * \n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "import importlib.resources as pkg_resources\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from fmbench.scripts.bedrock_predictor import BedrockPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:{\n",
      "  \"general\": {\n",
      "    \"name\": \"fmbench-claude\",\n",
      "    \"model_name\": \"claude\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::121797993273:user/ab3\",\n",
      "    \"bucket\": \"sagemaker-fmbench-write-121797993273\"\n",
      "  },\n",
      "  \"dir_paths\": {\n",
      "    \"data_prefix\": \"data\",\n",
      "    \"prompts_prefix\": \"prompts\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\",\n",
      "    \"metrics_dir\": \"metrics\",\n",
      "    \"models_dir\": \"models\",\n",
      "    \"metadata_dir\": \"metadata\"\n",
      "  },\n",
      "  \"s3_read_data\": {\n",
      "    \"read_bucket\": \"sagemaker-fmbench-read-121797993273\",\n",
      "    \"scripts_prefix\": \"scripts\",\n",
      "    \"script_files\": [\n",
      "      \"hf_token.txt\"\n",
      "    ],\n",
      "    \"source_data_prefix\": \"source_data\",\n",
      "    \"tokenizer_prefix\": \"tokenizer\",\n",
      "    \"prompt_template_dir\": \"prompt_template\",\n",
      "    \"prompt_template_file\": \"prompt_template.txt\"\n",
      "  },\n",
      "  \"run_steps\": {\n",
      "    \"0_setup.ipynb\": true,\n",
      "    \"1_generate_data.ipynb\": true,\n",
      "    \"2_deploy_model.ipynb\": false,\n",
      "    \"3_run_inference.ipynb\": true,\n",
      "    \"4_model_metric_analysis.ipynb\": true,\n",
      "    \"5_cleanup.ipynb\": true\n",
      "  },\n",
      "  \"datasets\": {\n",
      "    \"prompt_template_keys\": [\n",
      "      \"input\",\n",
      "      \"context\"\n",
      "    ],\n",
      "    \"filters\": [\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1,\n",
      "        \"max_length_in_tokens\": 500,\n",
      "        \"payload_file\": \"payload_en_1-500.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 500,\n",
      "        \"max_length_in_tokens\": 1000,\n",
      "        \"payload_file\": \"payload_en_500-1000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1000,\n",
      "        \"max_length_in_tokens\": 2000,\n",
      "        \"payload_file\": \"payload_en_1000-2000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 2000,\n",
      "        \"max_length_in_tokens\": 3000,\n",
      "        \"payload_file\": \"payload_en_2000-3000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 3000,\n",
      "        \"max_length_in_tokens\": 4000,\n",
      "        \"payload_file\": \"payload_en_3000-4000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 305,\n",
      "        \"max_length_in_tokens\": 3997,\n",
      "        \"payload_file\": \"payload_en_305-3997.jsonl\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_1000-2000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.m5.xlarge\": 0.23,\n",
      "    \"ml.g5.xlarge\": 1.006,\n",
      "    \"ml.g5.2xlarge\": 1.212,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688,\n",
      "    \"ml.p3.2xlarge\": 3.825,\n",
      "    \"Claudev3-Haiku-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00025\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.00125\n",
      "      }\n",
      "    ],\n",
      "    \"Claudev3-Sonnet-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.003\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.015\n",
      "      }\n",
      "    ],\n",
      "    \"ClaudeV2-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      }\n",
      "    ],\n",
      "    \"ClaudeV2:1-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      }\n",
      "    ],\n",
      "    \"ClaudeInstant-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      }\n",
      "    ],\n",
      "    \"titan-emb-text-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0001\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0\n",
      "      }\n",
      "    ],\n",
      "    \"titan-text-lite-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0003\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0004\n",
      "      }\n",
      "    ],\n",
      "    \"titan-text-express-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0016\n",
      "      }\n",
      "    ],\n",
      "    \"Mistral-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00015\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0002\n",
      "      }\n",
      "    ],\n",
      "    \"Mixtral-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00045\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0007\n",
      "      }\n",
      "    ],\n",
      "    \"Llama13b-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00075\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.001\n",
      "      }\n",
      "    ],\n",
      "    \"Llama70b-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00195\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.00195\n",
      "      }\n",
      "    ],\n",
      "    \"AI21mid-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0125\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0125\n",
      "      }\n",
      "    ],\n",
      "    \"AI21ultra-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0188\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0188\n",
      "      }\n",
      "    ],\n",
      "    \"CohereCommand-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0015\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.002\n",
      "      }\n",
      "    ],\n",
      "    \"CohereCommLight-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0003\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0006\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"ContentType\": \"application/json\",\n",
      "    \"Accept\": \"application/json\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"claude-instant-v1\",\n",
      "      \"model_id\": \"anthropic.claude-instant-v1\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"claude-instant-v1\",\n",
      "      \"ep_name\": \"anthropic.claude-instant-v1\",\n",
      "      \"instance_type\": \"ClaudeInstant-ODT\",\n",
      "      \"image_uri\": null,\n",
      "      \"deploy\": false,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": null,\n",
      "      \"inference_script\": \"bedrock_predictor.py\",\n",
      "      \"inference_spec\": {\n",
      "        \"split_input_and_parameters\": false\n",
      "      },\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1000-2000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"env\": null\n",
      "    }\n",
      "  ],\n",
      "  \"report\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\",\n",
      "    \"txn_count_for_showing_cost\": 100000,\n",
      "    \"v_shift_w_single_instance\": 0.025,\n",
      "    \"v_shift_w_gt_one_instance\": 0.025\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## getting access to the s3 bucket where endpoints.json for different models resides\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:found information for 0 endpoints in bucket=sagemaker-fmbench-write-121797993273, key=fmbench-claude-ab3/data/models/endpoints.json\n",
      "INFO:__main__:[]\n"
     ]
    }
   ],
   "source": [
    "## Refer to the file path for the endpoint\n",
    "## getting the endpoint as an s3 object from the deployed path\n",
    "try:\n",
    "    endpoint_info_list = json.loads(get_s3_object(config['aws']['bucket'], ENDPOINT_LIST_PATH))\n",
    "    logger.info(f\"found information for {len(endpoint_info_list)} endpoints in bucket={config['aws']['bucket']}, key={ENDPOINT_LIST_PATH}\")\n",
    "    logger.info(json.dumps(endpoint_info_list, indent=2))\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "        logger.warning(f\"Key {ENDPOINT_LIST_PATH} not found in bucket {config['aws']['bucket']}. Using an empty list for endpoints for bedrock models.\")\n",
    "        endpoint_info_list = []\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:there are 0 deployed endpoint(s), endpoint_name_list->[]\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    latency = 0\n",
    "\n",
    "    ## initializing completion tokens\n",
    "    completion_tokens = None\n",
    "    prompt_tokens = None\n",
    "\n",
    "    try:\n",
    "        # get inference      \n",
    "        resp = predictor.get_prediction(payload) \n",
    "        logger.info(f\"response={resp}\")\n",
    "        response_json = resp['response_json']\n",
    "        logger.info(f\"response_json={response_json}\")\n",
    "        latency = resp['latency']\n",
    "        logger.info(f\"latency={latency}\")\n",
    "        prompt_tokens = resp['prompt_tokens']\n",
    "        logger.info(f\"prompt_tokens={prompt_tokens}\")\n",
    "        completion_tokens = resp['completion_tokens']\n",
    "        logger.info(f\"completion_tokens={completion_tokens}\")\n",
    "\n",
    "        # Assign the generated_text value to completion\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens, \n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        \n",
    "        # logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, response={json.dumps(response, indent=2)}, latency={latency:.2f}\")\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, prompt_tokens = {prompt_tokens}, completion_tokens={completion_tokens}, latency={latency:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asynchronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: Dict, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## initializing the inference_spec\n",
    "    inference_spec = None\n",
    "\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    logger.info(f\"endpoint info found is: {ep_info}\")\n",
    "\n",
    "    if ep_info != []:\n",
    "        ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "        inference_spec = experiment.get(\"inference_spec\")\n",
    "        logger.info(f\"experiment name={experiment['name']}, ep_name={ep_name}, inference_spec={inference_spec}\")\n",
    "\n",
    "    elif (ep_info == []) and (experiment['inference_script'] == 'bedrock_predictor.py'):\n",
    "        ep_name = experiment['ep_name']\n",
    "        logger.info(f\"experiment name={experiment['name']}, custom bring your own ep_name={ep_name}\")\n",
    "\n",
    "    else:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "\n",
    "    # create predictor objects\n",
    "    # Proceed with deployment as before\n",
    "    # Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "    scripts_dir = Path(pkg_resources.files('fmbench'), 'scripts')\n",
    "    logger.info(f\"Using fmbench.scripts directory: {scripts_dir}\")\n",
    "\n",
    "    # Ensure the scripts directory exists\n",
    "    scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    module_name = Path(experiment['inference_script']).stem\n",
    "    logger.info(f\"script provided for inference from this model is --> {module_name}\")\n",
    "    script_path = scripts_dir / f\"{module_name}.py\"\n",
    "    logger.info(f\"script path is --> {script_path}\")\n",
    "\n",
    "    # Check and proceed with local script\n",
    "    if not script_path.exists():\n",
    "        logger.error(f\"script {script_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Deploying using local code: {script_path}\")\n",
    "\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(script_path))\n",
    "    inference_module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = inference_module\n",
    "    spec.loader.exec_module(inference_module)\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return inference_module.create_predictor(ep_name, inference_spec)\n",
    "\n",
    "endpoint_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Construct the full S3 file path\n",
    "        s3_file_path = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        logger.info(f\"s3 path where the payload files are being read from -> {s3_file_path}\")\n",
    "\n",
    "        # Read the payload file from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=config['aws']['bucket'], Key=s3_file_path)\n",
    "            payload_file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            # Create a payload list by processing each line\n",
    "            payload_list = [create_payload_dict(jline, experiment) for jline in payload_file_content.splitlines()]\n",
    "            logger.info(f\"read from s3://{config['aws']['bucket']}/{s3_file_path}, contains {len(payload_list)} lines\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file from S3: {e}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Current time recorded while running this experiment is 2024-03-23 00:17:14.668818+00:00..... deployed models are going to start inferences...\n",
      "INFO:__main__:endpoint info found is: []\n",
      "INFO:__main__:experiment name=claude-instant-v1, custom bring your own ep_name=anthropic.claude-instant-v1\n",
      "INFO:__main__:Using fmbench.scripts directory: /Users/madhurpt/Desktop/foundation-model-benchmarking-tool-5/src/fmbench/scripts\n",
      "INFO:__main__:script provided for inference from this model is --> bedrock_predictor\n",
      "INFO:__main__:script path is --> /Users/madhurpt/Desktop/foundation-model-benchmarking-tool-5/src/fmbench/scripts/bedrock_predictor.py\n",
      "INFO:__main__:Deploying using local code: /Users/madhurpt/Desktop/foundation-model-benchmarking-tool-5/src/fmbench/scripts/bedrock_predictor.py\n",
      "INFO:bedrock_predictor:__init__ self._predictor=<botocore.client.BedrockRuntime object at 0x28da7bfd0>\n",
      "INFO:__main__:there are 1 combinations of [(1, 'payload_en_1000-2000.jsonl')] to run\n",
      "INFO:__main__:s3 path where the payload files are being read from -> fmbench-claude-ab3/data/prompts/payload_en_1000-2000.jsonl\n",
      "INFO:__main__:read from s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "INFO:__main__:creating combinations for concurrency=1, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "INFO:__main__:after only retaining chunks of length 1, we have 15 chunks, previously we had 15 chunks\n",
      "INFO:__main__:there are 1 for {'name': 'claude-instant-v1', 'model_id': 'anthropic.claude-instant-v1', 'model_version': '*', 'model_name': 'claude-instant-v1', 'ep_name': 'anthropic.claude-instant-v1', 'instance_type': 'ClaudeInstant-ODT', 'image_uri': None, 'deploy': False, 'instance_count': 1, 'deployment_script': None, 'inference_script': 'bedrock_predictor.py', 'inference_spec': {'split_input_and_parameters': False}, 'payload_files': ['payload_en_1000-2000.jsonl'], 'concurrency_levels': [1], 'env': None}\n",
      "INFO:__main__:e_idx=1/1, chunk_index=1/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1339\n",
      "INFO:bedrock_predictor:Claude completion tokens: 61\n",
      "INFO:__main__:response={'response_json': {'completion': ' No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is described as being a village in Wales, which is part of the United Kingdom. Meanwhile, Qaleh-Ye Sahar is described as being a village in Iran.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is described as being a village in Wales, which is part of the United Kingdom. Meanwhile, Qaleh-Ye Sahar is described as being a village in Iran.'}, 'latency': 1.5400538749963744, 'prompt_tokens': 1339, 'completion_tokens': 61}\n",
      "INFO:__main__:response_json={'completion': ' No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is described as being a village in Wales, which is part of the United Kingdom. Meanwhile, Qaleh-Ye Sahar is described as being a village in Iran.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is described as being a village in Wales, which is part of the United Kingdom. Meanwhile, Qaleh-Ye Sahar is described as being a village in Iran.'}\n",
      "INFO:__main__:latency=1.5400538749963744\n",
      "INFO:__main__:prompt_tokens=1339\n",
      "INFO:__main__:completion_tokens=61\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1339, completion_tokens=61, latency=1.5401\n",
      "INFO:__main__:e_idx=1/1, chunk_index=2/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1932\n",
      "INFO:bedrock_predictor:Claude completion tokens: 51\n",
      "INFO:__main__:response={'response_json': {'completion': ' There is no information provided in the passages to determine which player, Guy Arvely Dolsin or Altuğ Çelikbilek, is younger. The passages do not include their dates of birth or any details to compare their ages.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' There is no information provided in the passages to determine which player, Guy Arvely Dolsin or Altuğ Çelikbilek, is younger. The passages do not include their dates of birth or any details to compare their ages.'}, 'latency': 2.289985584007809, 'prompt_tokens': 1932, 'completion_tokens': 51}\n",
      "INFO:__main__:response_json={'completion': ' There is no information provided in the passages to determine which player, Guy Arvely Dolsin or Altuğ Çelikbilek, is younger. The passages do not include their dates of birth or any details to compare their ages.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' There is no information provided in the passages to determine which player, Guy Arvely Dolsin or Altuğ Çelikbilek, is younger. The passages do not include their dates of birth or any details to compare their ages.'}\n",
      "INFO:__main__:latency=2.289985584007809\n",
      "INFO:__main__:prompt_tokens=1932\n",
      "INFO:__main__:completion_tokens=51\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1932, completion_tokens=51, latency=2.2900\n",
      "INFO:__main__:e_idx=1/1, chunk_index=3/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1154\n",
      "INFO:bedrock_predictor:Claude completion tokens: 34\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages provide information about tributaries of rivers in Romania.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages provide information about tributaries of rivers in Romania.'}, 'latency': 1.7271992080059135, 'prompt_tokens': 1154, 'completion_tokens': 34}\n",
      "INFO:__main__:response_json={'completion': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages provide information about tributaries of rivers in Romania.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages provide information about tributaries of rivers in Romania.'}\n",
      "INFO:__main__:latency=1.7271992080059135\n",
      "INFO:__main__:prompt_tokens=1154\n",
      "INFO:__main__:completion_tokens=34\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1154, completion_tokens=34, latency=1.7272\n",
      "INFO:__main__:e_idx=1/1, chunk_index=4/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1646\n",
      "INFO:bedrock_predictor:Claude completion tokens: 58\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. Howard Lake is located approximately 50 km northeast of 100 Mile House in British Columbia. North Buck Lake is a lake in Alberta, Canada. Both lakes are within the country of Canada.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. Howard Lake is located approximately 50 km northeast of 100 Mile House in British Columbia. North Buck Lake is a lake in Alberta, Canada. Both lakes are within the country of Canada.'}, 'latency': 3.2309533749939874, 'prompt_tokens': 1646, 'completion_tokens': 58}\n",
      "INFO:__main__:response_json={'completion': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. Howard Lake is located approximately 50 km northeast of 100 Mile House in British Columbia. North Buck Lake is a lake in Alberta, Canada. Both lakes are within the country of Canada.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. Howard Lake is located approximately 50 km northeast of 100 Mile House in British Columbia. North Buck Lake is a lake in Alberta, Canada. Both lakes are within the country of Canada.'}\n",
      "INFO:__main__:latency=3.2309533749939874\n",
      "INFO:__main__:prompt_tokens=1646\n",
      "INFO:__main__:completion_tokens=58\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1646, completion_tokens=58, latency=3.2310\n",
      "INFO:__main__:e_idx=1/1, chunk_index=5/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1397\n",
      "INFO:bedrock_predictor:Claude completion tokens: 97\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided, Puka Rumi and Yana Urqu (Q'Umirqucha) are not located in the same country. Puka Rumi is situated in Peru, specifically in the Huancavelica and Junín regions. Yana Urqu (Q'Umirqucha) on the other hand is located in Peru, in the Cusco region. While both mountains are situated in Peru, they are in different regions within the country.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Puka Rumi and Yana Urqu (Q'Umirqucha) are not located in the same country. Puka Rumi is situated in Peru, specifically in the Huancavelica and Junín regions. Yana Urqu (Q'Umirqucha) on the other hand is located in Peru, in the Cusco region. While both mountains are situated in Peru, they are in different regions within the country.\"}, 'latency': 1.895930833008606, 'prompt_tokens': 1397, 'completion_tokens': 97}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided, Puka Rumi and Yana Urqu (Q'Umirqucha) are not located in the same country. Puka Rumi is situated in Peru, specifically in the Huancavelica and Junín regions. Yana Urqu (Q'Umirqucha) on the other hand is located in Peru, in the Cusco region. While both mountains are situated in Peru, they are in different regions within the country.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Puka Rumi and Yana Urqu (Q'Umirqucha) are not located in the same country. Puka Rumi is situated in Peru, specifically in the Huancavelica and Junín regions. Yana Urqu (Q'Umirqucha) on the other hand is located in Peru, in the Cusco region. While both mountains are situated in Peru, they are in different regions within the country.\"}\n",
      "INFO:__main__:latency=1.895930833008606\n",
      "INFO:__main__:prompt_tokens=1397\n",
      "INFO:__main__:completion_tokens=97\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1397, completion_tokens=97, latency=1.8959\n",
      "INFO:__main__:e_idx=1/1, chunk_index=6/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1746\n",
      "INFO:bedrock_predictor:Claude completion tokens: 97\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was abducted by forest brigand Veerappan and his gang members on 25 August 2002 from the Kamagere village of Chamarajanagar district. On 8 December 2002, H. Nagappa was killed by Veerappan or his gang members or by Tamil nadu police at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was abducted by forest brigand Veerappan and his gang members on 25 August 2002 from the Kamagere village of Chamarajanagar district. On 8 December 2002, H. Nagappa was killed by Veerappan or his gang members or by Tamil nadu police at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\"}, 'latency': 6.49568762499257, 'prompt_tokens': 1746, 'completion_tokens': 97}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was abducted by forest brigand Veerappan and his gang members on 25 August 2002 from the Kamagere village of Chamarajanagar district. On 8 December 2002, H. Nagappa was killed by Veerappan or his gang members or by Tamil nadu police at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was abducted by forest brigand Veerappan and his gang members on 25 August 2002 from the Kamagere village of Chamarajanagar district. On 8 December 2002, H. Nagappa was killed by Veerappan or his gang members or by Tamil nadu police at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\"}\n",
      "INFO:__main__:latency=6.49568762499257\n",
      "INFO:__main__:prompt_tokens=1746\n",
      "INFO:__main__:completion_tokens=97\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1746, completion_tokens=97, latency=6.4957\n",
      "INFO:__main__:e_idx=1/1, chunk_index=7/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1373\n",
      "INFO:bedrock_predictor:Claude completion tokens: 84\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. The first passage describes Huernia as \"The genus Huernia\" and lists several species found in Africa and Arabia. The second passage describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and provides details about the sole species Dictyosperma album.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. The first passage describes Huernia as \"The genus Huernia\" and lists several species found in Africa and Arabia. The second passage describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and provides details about the sole species Dictyosperma album.'}, 'latency': 4.752937790995929, 'prompt_tokens': 1373, 'completion_tokens': 84}\n",
      "INFO:__main__:response_json={'completion': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. The first passage describes Huernia as \"The genus Huernia\" and lists several species found in Africa and Arabia. The second passage describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and provides details about the sole species Dictyosperma album.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. The first passage describes Huernia as \"The genus Huernia\" and lists several species found in Africa and Arabia. The second passage describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and provides details about the sole species Dictyosperma album.'}\n",
      "INFO:__main__:latency=4.752937790995929\n",
      "INFO:__main__:prompt_tokens=1373\n",
      "INFO:__main__:completion_tokens=84\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1373, completion_tokens=84, latency=4.7529\n",
      "INFO:__main__:e_idx=1/1, chunk_index=8/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1598\n",
      "INFO:bedrock_predictor:Claude completion tokens: 53\n",
      "INFO:__main__:response={'response_json': {'completion': ' Javier Frana and Thomaz Koch were both professional tennis players. According to the passages, Frana and Koch achieved success competing in tournaments around the world during their careers on the ATP tour. Both players reached high rankings and won titles on the professional tennis circuit.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Javier Frana and Thomaz Koch were both professional tennis players. According to the passages, Frana and Koch achieved success competing in tournaments around the world during their careers on the ATP tour. Both players reached high rankings and won titles on the professional tennis circuit.'}, 'latency': 2.8628912919957656, 'prompt_tokens': 1598, 'completion_tokens': 53}\n",
      "INFO:__main__:response_json={'completion': ' Javier Frana and Thomaz Koch were both professional tennis players. According to the passages, Frana and Koch achieved success competing in tournaments around the world during their careers on the ATP tour. Both players reached high rankings and won titles on the professional tennis circuit.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Javier Frana and Thomaz Koch were both professional tennis players. According to the passages, Frana and Koch achieved success competing in tournaments around the world during their careers on the ATP tour. Both players reached high rankings and won titles on the professional tennis circuit.'}\n",
      "INFO:__main__:latency=2.8628912919957656\n",
      "INFO:__main__:prompt_tokens=1598\n",
      "INFO:__main__:completion_tokens=53\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1598, completion_tokens=53, latency=2.8629\n",
      "INFO:__main__:e_idx=1/1, chunk_index=9/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1743\n",
      "INFO:bedrock_predictor:Claude completion tokens: 21\n",
      "INFO:__main__:response={'response_json': {'completion': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.'}, 'latency': 1.378054417000385, 'prompt_tokens': 1743, 'completion_tokens': 21}\n",
      "INFO:__main__:response_json={'completion': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.'}\n",
      "INFO:__main__:latency=1.378054417000385\n",
      "INFO:__main__:prompt_tokens=1743\n",
      "INFO:__main__:completion_tokens=21\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1743, completion_tokens=21, latency=1.3781\n",
      "INFO:__main__:e_idx=1/1, chunk_index=10/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1539\n",
      "INFO:bedrock_predictor:Claude completion tokens: 22\n",
      "INFO:__main__:response={'response_json': {'completion': ' According to the passage, Mary Queen of Scots was beheaded at Fotheringhay Castle in England.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' According to the passage, Mary Queen of Scots was beheaded at Fotheringhay Castle in England.'}, 'latency': 1.2101055410021218, 'prompt_tokens': 1539, 'completion_tokens': 22}\n",
      "INFO:__main__:response_json={'completion': ' According to the passage, Mary Queen of Scots was beheaded at Fotheringhay Castle in England.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' According to the passage, Mary Queen of Scots was beheaded at Fotheringhay Castle in England.'}\n",
      "INFO:__main__:latency=1.2101055410021218\n",
      "INFO:__main__:prompt_tokens=1539\n",
      "INFO:__main__:completion_tokens=22\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1539, completion_tokens=22, latency=1.2101\n",
      "INFO:__main__:e_idx=1/1, chunk_index=11/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1695\n",
      "INFO:bedrock_predictor:Claude completion tokens: 65\n",
      "INFO:__main__:response={'response_json': {'completion': \" On an old steam locomotive, a cowcatcher is a metal frame attached to the front that helps clear debris from the tracks. A cowcatcher's sloping arms would push aside obstacles like cows or tree branches to prevent the locomotive from derailing. It helps keep the tracks clear as the train travels down the line.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" On an old steam locomotive, a cowcatcher is a metal frame attached to the front that helps clear debris from the tracks. A cowcatcher's sloping arms would push aside obstacles like cows or tree branches to prevent the locomotive from derailing. It helps keep the tracks clear as the train travels down the line.\"}, 'latency': 3.358258916006889, 'prompt_tokens': 1695, 'completion_tokens': 65}\n",
      "INFO:__main__:response_json={'completion': \" On an old steam locomotive, a cowcatcher is a metal frame attached to the front that helps clear debris from the tracks. A cowcatcher's sloping arms would push aside obstacles like cows or tree branches to prevent the locomotive from derailing. It helps keep the tracks clear as the train travels down the line.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" On an old steam locomotive, a cowcatcher is a metal frame attached to the front that helps clear debris from the tracks. A cowcatcher's sloping arms would push aside obstacles like cows or tree branches to prevent the locomotive from derailing. It helps keep the tracks clear as the train travels down the line.\"}\n",
      "INFO:__main__:latency=3.358258916006889\n",
      "INFO:__main__:prompt_tokens=1695\n",
      "INFO:__main__:completion_tokens=65\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1695, completion_tokens=65, latency=3.3583\n",
      "INFO:__main__:e_idx=1/1, chunk_index=12/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1421\n",
      "INFO:bedrock_predictor:Claude completion tokens: 24\n",
      "INFO:__main__:response={'response_json': {'completion': ' Triple Sec tastes heavily of oranges and orange peel, like orange hard candy. It has a sweet orange citrus flavor.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Triple Sec tastes heavily of oranges and orange peel, like orange hard candy. It has a sweet orange citrus flavor.'}, 'latency': 0.8762147090019425, 'prompt_tokens': 1421, 'completion_tokens': 24}\n",
      "INFO:__main__:response_json={'completion': ' Triple Sec tastes heavily of oranges and orange peel, like orange hard candy. It has a sweet orange citrus flavor.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Triple Sec tastes heavily of oranges and orange peel, like orange hard candy. It has a sweet orange citrus flavor.'}\n",
      "INFO:__main__:latency=0.8762147090019425\n",
      "INFO:__main__:prompt_tokens=1421\n",
      "INFO:__main__:completion_tokens=24\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1421, completion_tokens=24, latency=0.8762\n",
      "INFO:__main__:e_idx=1/1, chunk_index=13/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1918\n",
      "INFO:bedrock_predictor:Claude completion tokens: 18\n",
      "INFO:__main__:response={'response_json': {'completion': ' John Glenn became the oldest person to fly in space at the age of 77 in 1998.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' John Glenn became the oldest person to fly in space at the age of 77 in 1998.'}, 'latency': 4.591563750000205, 'prompt_tokens': 1918, 'completion_tokens': 18}\n",
      "INFO:__main__:response_json={'completion': ' John Glenn became the oldest person to fly in space at the age of 77 in 1998.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' John Glenn became the oldest person to fly in space at the age of 77 in 1998.'}\n",
      "INFO:__main__:latency=4.591563750000205\n",
      "INFO:__main__:prompt_tokens=1918\n",
      "INFO:__main__:completion_tokens=18\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1918, completion_tokens=18, latency=4.5916\n",
      "INFO:__main__:e_idx=1/1, chunk_index=14/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1910\n",
      "INFO:bedrock_predictor:Claude completion tokens: 100\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the context provided, the Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball who exemplify the core values of Dr. James Naismith, the inventor of basketball, such as respect, teamwork, sportsmanship, honesty, integrity and excellence. Both awards are administered by the Naismith Legacy Group to honor recipients' role in furthering the values and legacy of Dr. Naismith in the game of basketball.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the context provided, the Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball who exemplify the core values of Dr. James Naismith, the inventor of basketball, such as respect, teamwork, sportsmanship, honesty, integrity and excellence. Both awards are administered by the Naismith Legacy Group to honor recipients' role in furthering the values and legacy of Dr. Naismith in the game of basketball.\"}, 'latency': 3.3780627500091214, 'prompt_tokens': 1910, 'completion_tokens': 100}\n",
      "INFO:__main__:response_json={'completion': \" Based on the context provided, the Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball who exemplify the core values of Dr. James Naismith, the inventor of basketball, such as respect, teamwork, sportsmanship, honesty, integrity and excellence. Both awards are administered by the Naismith Legacy Group to honor recipients' role in furthering the values and legacy of Dr. Naismith in the game of basketball.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the context provided, the Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball who exemplify the core values of Dr. James Naismith, the inventor of basketball, such as respect, teamwork, sportsmanship, honesty, integrity and excellence. Both awards are administered by the Naismith Legacy Group to honor recipients' role in furthering the values and legacy of Dr. Naismith in the game of basketball.\"}\n",
      "INFO:__main__:latency=3.3780627500091214\n",
      "INFO:__main__:prompt_tokens=1910\n",
      "INFO:__main__:completion_tokens=100\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1910, completion_tokens=100, latency=3.3781\n",
      "INFO:__main__:e_idx=1/1, chunk_index=15/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1939\n",
      "INFO:bedrock_predictor:Claude completion tokens: 17\n",
      "INFO:__main__:response={'response_json': {'completion': \" John Chilcot led the inquiry into the UK's involvement in the Iraq war.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" John Chilcot led the inquiry into the UK's involvement in the Iraq war.\"}, 'latency': 2.1594329999934416, 'prompt_tokens': 1939, 'completion_tokens': 17}\n",
      "INFO:__main__:response_json={'completion': \" John Chilcot led the inquiry into the UK's involvement in the Iraq war.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" John Chilcot led the inquiry into the UK's involvement in the Iraq war.\"}\n",
      "INFO:__main__:latency=2.1594329999934416\n",
      "INFO:__main__:prompt_tokens=1939\n",
      "INFO:__main__:completion_tokens=17\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1939, completion_tokens=17, latency=2.1594\n",
      "INFO:__main__:the claude-instant-v1 ran for 50.28527366700291 seconds......\n",
      "INFO:__main__:metrics json is: {'experiment_name': 'claude-instant-v1', 'concurrency': 1, 'payload_file': 'payload_en_1000-2000.jsonl', 'errors': [], 'successes': 1, 'error_rate': 0.0, 'all_prompts_token_count': 1939, 'prompt_token_count_mean': 1939.0, 'prompt_token_throughput': 872.93, 'all_completions_token_count': 17, 'completion_token_count_mean': 17.0, 'completion_token_throughput': 7.65, 'transactions': 1, 'transactions_per_second': 0.45, 'transactions_per_minute': 27, 'latency_mean': 2.1594329999934416}\n",
      "INFO:bedrock_predictor:pricing dict: {'input-per-1k-tokens': 0.0008}\n",
      "INFO:bedrock_predictor:input per 1k token pricing: 0.0008\n",
      "INFO:bedrock_predictor:pricing dict: {'output-per-1k-tokens': 0.0024}\n",
      "INFO:bedrock_predictor:output per 1k token pricing: 0.0024\n",
      "INFO:__main__:the rate for running claude-instant-v1 running on ClaudeInstant-ODT for 50.28527366700291 is $0.001592....\n",
      "INFO:__main__:experiment=1/1, name=claude-instant-v1, duration=50.29 seconds, done\n",
      "INFO:__main__:experiment durations:      experiment_name      instance_type duration_in_seconds  cost\n",
      "0  claude-instant-v1  ClaudeInstant-ODT               50.29  0.00\n",
      "INFO:__main__:Summary for cost of instance per endpoint per run saved to s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/endpoint_per_instance_per_run_costs.csv\n",
      "INFO:__main__:total cost of all experiments: $0.0\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "# Initializing the total model instance cost to 0\n",
    "total_model_instance_cost: int = 0\n",
    "\n",
    "## To keep track of the cost for all model endpoints\n",
    "cost_data = []\n",
    "\n",
    "## To keep track of the experiment durations and the time it takes for the model endpoint to be in service to calculate cost association\n",
    "experiment_durations = []  \n",
    "\n",
    "## start the timer before the start of inferences\n",
    "current_time = datetime.now(timezone.utc)\n",
    "logger.info(f\"Current time recorded while running this experiment is {current_time}..... deployed models are going to start inferences...\")\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    experiment_start_time = time.perf_counter()  # Start timer for the experiment\n",
    "\n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                metrics_json = json.dumps(metrics, indent=2)\n",
    "                metrics_file_name = f\"{time.time()}.json\"\n",
    "                metrics_s3_path = os.path.join(METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "                write_to_s3(metrics_json, config['aws']['bucket'], \"\", METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    response_json = json.dumps(r, indent=2)\n",
    "                    response_file_name = f\"{time.time()}.json\"\n",
    "                    response_s3_path = os.path.join(METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "                    write_to_s3(response_json, config['aws']['bucket'], \"\", METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "    \n",
    "    ## initializing the experiment cost\n",
    "    exp_cost = 0\n",
    "    \n",
    "    # Experiment done, stopping the timer for this given experiment\n",
    "    experiment_end_time = time.perf_counter()\n",
    "\n",
    "    # calculating the duration of this given endpoint inference time\n",
    "    experiment_duration = experiment_end_time - experiment_start_time\n",
    "    logger.info(f\"the {experiment['name']} ran for {experiment_duration} seconds......\")\n",
    "\n",
    "    # calculating the per second cost for this instance type\n",
    "    exp_instance_type: str = experiment['instance_type']\n",
    "    \n",
    "    #cost for this given exp\n",
    "    logger.info(f\"metrics json is: {metrics}\")\n",
    "    exp_cost = predictor.calculate_cost(exp_instance_type, config, experiment_duration, metrics)\n",
    "    logger.info(f\"the rate for running {experiment['name']} running on {exp_instance_type} for {experiment_duration} is ${exp_cost}....\")\n",
    "\n",
    "    ## tracking the total cost\n",
    "    total_model_instance_cost += exp_cost\n",
    "\n",
    "    experiment_durations.append({\n",
    "        'experiment_name': experiment['name'],\n",
    "        'instance_type': exp_instance_type, \n",
    "        'duration_in_seconds': f\"{experiment_duration:.2f}\", \n",
    "        'cost': f\"{exp_cost:.2f}\", \n",
    "    })\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, duration={experiment_duration:.2f} seconds, done\")\n",
    "\n",
    "# experiment_durations.append({'total_cost': f\"${total_model_instance_cost:.2f}\"})\n",
    "\n",
    "# After all experiments are done, summarize and optionally save experiment durations along with costs\n",
    "df_durations = pd.DataFrame(experiment_durations)\n",
    "logger.info(f\"experiment durations: {df_durations}\")\n",
    "\n",
    "# Convert the DataFrame to CSV and write it to S3 or wherever you prefer\n",
    "csv_buffer_cost = io.StringIO()\n",
    "df_durations.to_csv(csv_buffer_cost, index=False)\n",
    "experiment_associated_cost = csv_buffer_cost.getvalue()\n",
    "\n",
    "# Assuming write_to_s3() is already defined and configured correctly\n",
    "write_to_s3(experiment_associated_cost, config['aws']['bucket'], \"\", METRICS_DIR, SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE)\n",
    "logger.info(f\"Summary for cost of instance per endpoint per run saved to s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE}\")\n",
    "\n",
    "logger.info(f\"total cost of all experiments: ${sum(df_durations.cost.astype(float))}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fmbench.utils:found 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/per_inference, suffix=.json\n",
      "INFO:fmbench.utils:there are total of 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/per_inference, suffix=.json\n",
      "INFO:__main__:created dataframe of shape (15, 10) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>ContentType</th>\n",
       "      <th>Accept</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Rhosgoch and Qaleh-Ye Sahar are not locat...</td>\n",
       "      <td>1339</td>\n",
       "      <td>61</td>\n",
       "      <td>1.540054</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>There is no information provided in the passa...</td>\n",
       "      <td>1932</td>\n",
       "      <td>51</td>\n",
       "      <td>2.289986</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, Gut (Crișul Alb) and Gepiș are both loca...</td>\n",
       "      <td>1154</td>\n",
       "      <td>34</td>\n",
       "      <td>1.727199</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, both Howard Lake (British Columbia) and ...</td>\n",
       "      <td>1646</td>\n",
       "      <td>58</td>\n",
       "      <td>3.230953</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the passages provided, Puka Rumi and...</td>\n",
       "      <td>1397</td>\n",
       "      <td>97</td>\n",
       "      <td>1.895931</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 endpoint_name  \\\n",
       "0  anthropic.claude-instant-v1   \n",
       "1  anthropic.claude-instant-v1   \n",
       "2  anthropic.claude-instant-v1   \n",
       "3  anthropic.claude-instant-v1   \n",
       "4  anthropic.claude-instant-v1   \n",
       "\n",
       "                                              prompt       ContentType  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "\n",
       "             Accept                                         completion  \\\n",
       "0  application/json   No, Rhosgoch and Qaleh-Ye Sahar are not locat...   \n",
       "1  application/json   There is no information provided in the passa...   \n",
       "2  application/json   Yes, Gut (Crișul Alb) and Gepiș are both loca...   \n",
       "3  application/json   Yes, both Howard Lake (British Columbia) and ...   \n",
       "4  application/json   Based on the passages provided, Puka Rumi and...   \n",
       "\n",
       "   prompt_tokens  completion_tokens   latency    experiment_name  concurrency  \n",
       "0           1339                 61  1.540054  claude-instant-v1            1  \n",
       "1           1932                 51  2.289986  claude-instant-v1            1  \n",
       "2           1154                 34  1.727199  claude-instant-v1            1  \n",
       "3           1646                 58  3.230953  claude-instant-v1            1  \n",
       "4           1397                 97  1.895931  claude-instant-v1            1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_INFERENCE_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "                     s3_files))\n",
    "\n",
    "# Create DataFrame\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fmbench.utils:found 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/per_chunk, suffix=.json\n",
      "INFO:fmbench.utils:there are total of 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/per_chunk, suffix=.json\n",
      "INFO:__main__:created dataframe of shape (15, 16) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>794.93</td>\n",
       "      <td>61</td>\n",
       "      <td>61.0</td>\n",
       "      <td>36.21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>35</td>\n",
       "      <td>1.540054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1932</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>821.98</td>\n",
       "      <td>51</td>\n",
       "      <td>51.0</td>\n",
       "      <td>21.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>25</td>\n",
       "      <td>2.289986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>1154.0</td>\n",
       "      <td>656.37</td>\n",
       "      <td>34</td>\n",
       "      <td>34.0</td>\n",
       "      <td>19.34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>34</td>\n",
       "      <td>1.727199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1646</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>501.29</td>\n",
       "      <td>58</td>\n",
       "      <td>58.0</td>\n",
       "      <td>17.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>18</td>\n",
       "      <td>3.230953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1397</td>\n",
       "      <td>1397.0</td>\n",
       "      <td>715.24</td>\n",
       "      <td>97</td>\n",
       "      <td>97.0</td>\n",
       "      <td>49.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>30</td>\n",
       "      <td>1.895931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     experiment_name  concurrency                payload_file errors  \\\n",
       "0  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "1  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "2  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "3  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "4  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "\n",
       "   successes  error_rate  all_prompts_token_count  prompt_token_count_mean  \\\n",
       "0          1         0.0                     1339                   1339.0   \n",
       "1          1         0.0                     1932                   1932.0   \n",
       "2          1         0.0                     1154                   1154.0   \n",
       "3          1         0.0                     1646                   1646.0   \n",
       "4          1         0.0                     1397                   1397.0   \n",
       "\n",
       "   prompt_token_throughput  all_completions_token_count  \\\n",
       "0                   794.93                           61   \n",
       "1                   821.98                           51   \n",
       "2                   656.37                           34   \n",
       "3                   501.29                           58   \n",
       "4                   715.24                           97   \n",
       "\n",
       "   completion_token_count_mean  completion_token_throughput  transactions  \\\n",
       "0                         61.0                        36.21             1   \n",
       "1                         51.0                        21.70             1   \n",
       "2                         34.0                        19.34             1   \n",
       "3                         58.0                        17.66             1   \n",
       "4                         97.0                        49.66             1   \n",
       "\n",
       "   transactions_per_second  transactions_per_minute  latency_mean  \n",
       "0                     0.59                       35      1.540054  \n",
       "1                     0.43                       25      2.289986  \n",
       "2                     0.57                       34      1.727199  \n",
       "3                     0.30                       18      3.230953  \n",
       "4                     0.51                       30      1.895931  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_CHUNK_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "                     s3_files))\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:the instance type: ClaudeInstant-ODT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'ContentType', 'Accept', 'completion',\n",
      "       'prompt_tokens', 'completion_tokens', 'latency', 'experiment_name',\n",
      "       'concurrency'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>ContentType</th>\n",
       "      <th>Accept</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Rhosgoch and Qaleh-Ye Sahar are not locat...</td>\n",
       "      <td>1339</td>\n",
       "      <td>61</td>\n",
       "      <td>1.540054</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>There is no information provided in the passa...</td>\n",
       "      <td>1932</td>\n",
       "      <td>51</td>\n",
       "      <td>2.289986</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, Gut (Crișul Alb) and Gepiș are both loca...</td>\n",
       "      <td>1154</td>\n",
       "      <td>34</td>\n",
       "      <td>1.727199</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, both Howard Lake (British Columbia) and ...</td>\n",
       "      <td>1646</td>\n",
       "      <td>58</td>\n",
       "      <td>3.230953</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Based on the passages provided, Puka Rumi and...</td>\n",
       "      <td>1397</td>\n",
       "      <td>97</td>\n",
       "      <td>1.895931</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 endpoint_name  \\\n",
       "0  anthropic.claude-instant-v1   \n",
       "1  anthropic.claude-instant-v1   \n",
       "2  anthropic.claude-instant-v1   \n",
       "3  anthropic.claude-instant-v1   \n",
       "4  anthropic.claude-instant-v1   \n",
       "\n",
       "                                              prompt       ContentType  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "\n",
       "             Accept                                         completion  \\\n",
       "0  application/json   No, Rhosgoch and Qaleh-Ye Sahar are not locat...   \n",
       "1  application/json   There is no information provided in the passa...   \n",
       "2  application/json   Yes, Gut (Crișul Alb) and Gepiș are both loca...   \n",
       "3  application/json   Yes, both Howard Lake (British Columbia) and ...   \n",
       "4  application/json   Based on the passages provided, Puka Rumi and...   \n",
       "\n",
       "   prompt_tokens  completion_tokens   latency    experiment_name  concurrency  \\\n",
       "0           1339                 61  1.540054  claude-instant-v1            1   \n",
       "1           1932                 51  2.289986  claude-instant-v1            1   \n",
       "2           1154                 34  1.727199  claude-instant-v1            1   \n",
       "3           1646                 58  3.230953  claude-instant-v1            1   \n",
       "4           1397                 97  1.895931  claude-instant-v1            1   \n",
       "\n",
       "       instance_type EndpointName ModelName Image S3Uri  \n",
       "0  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "1  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "2  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "3  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "4  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if endpoint_info_list:\n",
    "    df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "    df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "    cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "    print(cols_for_env)\n",
    "    cols_of_interest = ['experiment_name',\n",
    "                        'instance_type',\n",
    "                        'endpoint.EndpointName',\n",
    "                        'model_config.ModelName',\n",
    "                        'model_config.PrimaryContainer.Image',\n",
    "                        'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "    cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "    df_endpoints = df_endpoints[cols_of_interest]\n",
    "    cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "    df_endpoints.columns = cols_of_interest_renamed\n",
    "else:\n",
    "    # Create an empty DataFrame with the desired columns\n",
    "    df_endpoints = pd.DataFrame(columns=['experiment_name',\n",
    "                                         'instance_type',\n",
    "                                         'EndpointName',\n",
    "                                         'ModelName',\n",
    "                                         'Image',\n",
    "                                         'S3Uri'])\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "for e, experiment in enumerate(config['experiments']):\n",
    "    experiment_name = experiment['name']\n",
    "    instance_type = experiment['instance_type']\n",
    "    \n",
    "    logger.info(f\"the instance type: {instance_type}\")\n",
    "    # Update the instance_type column in df_results where the EndpointName matches\n",
    "    df_results.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:results s3 path for per inference csv --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/per_inference_request_results.csv\n",
      "INFO:__main__:saved results dataframe of shape=(15, 15) in s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert df_results to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_results.to_csv(csv_buffer, index=False)\n",
    "csv_data_results = csv_buffer.getvalue()\n",
    "results_file_name = config['report']['per_inference_request_file'].format(datetime=date_time)\n",
    "results_s3_path = os.path.join(METRICS_DIR, results_file_name)\n",
    "logger.info(f\"results s3 path for per inference csv --> {results_s3_path}\")\n",
    "write_to_s3(csv_data_results, config['aws']['bucket'], \"\", METRICS_DIR, results_file_name)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in s3://{BUCKET_NAME}/{results_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:the metrics metadata path is saved here --> metadata/metrics_path.txt\n",
      "INFO:__main__:the information on the defined path for results on these metrics are given in this --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17\n"
     ]
    }
   ],
   "source": [
    "# Ensure the metadata directory exists\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "# Path for the metrics_path.txt file\n",
    "metrics_path_file = os.path.join(METADATA_DIR, 'metrics_path.txt')\n",
    "logger.info(f\"the metrics metadata path is saved here --> {metrics_path_file}\")\n",
    "\n",
    "# Write the METRICS_DIR to metrics_path.txt\n",
    "with open(metrics_path_file, 'w') as file:\n",
    "    file.write(METRICS_DIR)\n",
    "\n",
    "## Write this data to S3\n",
    "write_to_s3(METRICS_DIR, config['aws']['bucket'], \"\", DATA_DIR, 'metrics_path.txt')\n",
    "\n",
    "logger.info(f\"the information on the defined path for results on these metrics are given in this --> {METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:df_metrics cols = Index(['experiment_name', 'concurrency', 'payload_file', 'errors', 'successes',\n",
      "       'error_rate', 'all_prompts_token_count', 'prompt_token_count_mean',\n",
      "       'prompt_token_throughput', 'all_completions_token_count',\n",
      "       'completion_token_count_mean', 'completion_token_throughput',\n",
      "       'transactions', 'transactions_per_second', 'transactions_per_minute',\n",
      "       'latency_mean'],\n",
      "      dtype='object')\n",
      "INFO:__main__:df_endpoints cols = Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri'],\n",
      "      dtype='object')\n",
      "INFO:__main__:the instance type: ClaudeInstant-ODT\n",
      "INFO:__main__:results s3 path for metrics csv --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/all_metrics.csv\n",
      "INFO:__main__:saved metrics results dataframe of shape=(15, 21) in s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=22/hh=20/mm=17/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# logger.info(f\"df_metrics cols = {df_metrics.columns}\")\n",
    "# logger.info(f\"df_endpoints cols = {df_endpoints.columns}\")\n",
    "# df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "# for e, experiment in enumerate(config['experiments']):\n",
    "#     experiment_name = experiment['name']\n",
    "#     instance_type = experiment['instance_type']\n",
    "    \n",
    "#     logger.info(f\"the instance type: {instance_type}\")\n",
    "#     # Update the instance_type column in df_results where the EndpointName matches\n",
    "#     df_metrics.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "\n",
    "# df_metrics.head()\n",
    "\n",
    "# # Convert df_metrics to CSV and write to S3\n",
    "# csv_buffer = io.StringIO()\n",
    "# df_metrics.to_csv(csv_buffer, index=False)\n",
    "# csv_data_metrics = csv_buffer.getvalue()\n",
    "# metrics_file_name = config['report']['all_metrics_file'].format(datetime=date_time)\n",
    "# metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "# logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "# write_to_s3(csv_data_metrics, config['aws']['bucket'], \"\", METRICS_DIR, metrics_file_name)\n",
    "# logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{config['aws']['bucket']}/{metrics_s3_path}\")\n",
    "\n",
    "logger.info(f\"df_metrics cols = {df_metrics.columns}\")\n",
    "logger.info(f\"df_endpoints cols = {df_endpoints.columns}\")\n",
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "for e, experiment in enumerate(config['experiments']):\n",
    "    experiment_name = experiment['name']\n",
    "    instance_type = experiment['instance_type']\n",
    "    \n",
    "    logger.info(f\"the instance type: {instance_type}\")\n",
    "    # Update the instance_type column in df_results where the EndpointName matches\n",
    "    df_metrics.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "df_metrics.head()\n",
    "\n",
    "# Convert df_metrics to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_metrics.to_csv(csv_buffer, index=False)\n",
    "csv_data_metrics = csv_buffer.getvalue()\n",
    "metrics_file_name = config['report']['all_metrics_file'].format(datetime=date_time)\n",
    "metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "write_to_s3(csv_data_metrics, config['aws']['bucket'], \"\", METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{config['aws']['bucket']}/{metrics_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
