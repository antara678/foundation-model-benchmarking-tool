{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import botocore\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import * \n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "import importlib.resources as pkg_resources\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from fmbench.scripts.bedrock_predictor import BedrockPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:{\n",
      "  \"general\": {\n",
      "    \"name\": \"fmbench-claude\",\n",
      "    \"model_name\": \"claude\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::121797993273:user/ab3\",\n",
      "    \"bucket\": \"sagemaker-fmbench-write-121797993273\"\n",
      "  },\n",
      "  \"dir_paths\": {\n",
      "    \"data_prefix\": \"data\",\n",
      "    \"prompts_prefix\": \"prompts\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\",\n",
      "    \"metrics_dir\": \"metrics\",\n",
      "    \"models_dir\": \"models\",\n",
      "    \"metadata_dir\": \"metadata\"\n",
      "  },\n",
      "  \"s3_read_data\": {\n",
      "    \"read_bucket\": \"sagemaker-fmbench-read-121797993273\",\n",
      "    \"scripts_prefix\": \"scripts\",\n",
      "    \"script_files\": [\n",
      "      \"hf_token.txt\"\n",
      "    ],\n",
      "    \"source_data_prefix\": \"source_data\",\n",
      "    \"tokenizer_prefix\": \"tokenizer\",\n",
      "    \"prompt_template_dir\": \"prompt_template\",\n",
      "    \"prompt_template_file\": \"prompt_template.txt\"\n",
      "  },\n",
      "  \"run_steps\": {\n",
      "    \"0_setup.ipynb\": true,\n",
      "    \"1_generate_data.ipynb\": true,\n",
      "    \"2_deploy_model.ipynb\": false,\n",
      "    \"3_run_inference.ipynb\": true,\n",
      "    \"4_model_metric_analysis.ipynb\": true,\n",
      "    \"5_cleanup.ipynb\": true\n",
      "  },\n",
      "  \"datasets\": {\n",
      "    \"prompt_template_keys\": [\n",
      "      \"input\",\n",
      "      \"context\"\n",
      "    ],\n",
      "    \"filters\": [\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1,\n",
      "        \"max_length_in_tokens\": 500,\n",
      "        \"payload_file\": \"payload_en_1-500.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 500,\n",
      "        \"max_length_in_tokens\": 1000,\n",
      "        \"payload_file\": \"payload_en_500-1000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1000,\n",
      "        \"max_length_in_tokens\": 2000,\n",
      "        \"payload_file\": \"payload_en_1000-2000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 2000,\n",
      "        \"max_length_in_tokens\": 3000,\n",
      "        \"payload_file\": \"payload_en_2000-3000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 3000,\n",
      "        \"max_length_in_tokens\": 4000,\n",
      "        \"payload_file\": \"payload_en_3000-4000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 305,\n",
      "        \"max_length_in_tokens\": 3997,\n",
      "        \"payload_file\": \"payload_en_305-3997.jsonl\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_1000-2000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.m5.xlarge\": 0.23,\n",
      "    \"ml.g5.xlarge\": 1.006,\n",
      "    \"ml.g5.2xlarge\": 1.212,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688,\n",
      "    \"ml.p3.2xlarge\": 3.825,\n",
      "    \"Claudev3-Haiku-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00025\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.00125\n",
      "      }\n",
      "    ],\n",
      "    \"Claudev3-Sonnet-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.003\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.015\n",
      "      }\n",
      "    ],\n",
      "    \"ClaudeV2-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      }\n",
      "    ],\n",
      "    \"ClaudeV2:1-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      }\n",
      "    ],\n",
      "    \"ClaudeInstant-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      }\n",
      "    ],\n",
      "    \"titan-emb-text-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0001\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0\n",
      "      }\n",
      "    ],\n",
      "    \"titan-text-lite-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0003\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0004\n",
      "      }\n",
      "    ],\n",
      "    \"titan-text-express-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0008\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0016\n",
      "      }\n",
      "    ],\n",
      "    \"Mistral-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00015\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0002\n",
      "      }\n",
      "    ],\n",
      "    \"Mixtral-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00045\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0007\n",
      "      }\n",
      "    ],\n",
      "    \"Llama13b-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00075\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.001\n",
      "      }\n",
      "    ],\n",
      "    \"Llama70b-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.00195\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.00195\n",
      "      }\n",
      "    ],\n",
      "    \"AI21mid-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0125\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0125\n",
      "      }\n",
      "    ],\n",
      "    \"AI21ultra-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0188\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0188\n",
      "      }\n",
      "    ],\n",
      "    \"CohereCommand-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0015\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.002\n",
      "      }\n",
      "    ],\n",
      "    \"CohereCommLight-ODT\": [\n",
      "      {\n",
      "        \"input-per-1k-tokens\": 0.0003\n",
      "      },\n",
      "      {\n",
      "        \"output-per-1k-tokens\": 0.0006\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"ContentType\": \"application/json\",\n",
      "    \"Accept\": \"application/json\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"claude-instant-v1\",\n",
      "      \"model_id\": \"anthropic.claude-instant-v1\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"claude-instant-v1\",\n",
      "      \"ep_name\": \"anthropic.claude-instant-v1\",\n",
      "      \"instance_type\": \"ClaudeInstant-ODT\",\n",
      "      \"image_uri\": null,\n",
      "      \"deploy\": false,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": null,\n",
      "      \"inference_script\": \"bedrock_predictor.py\",\n",
      "      \"inference_spec\": {\n",
      "        \"split_input_and_parameters\": false\n",
      "      },\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1000-2000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"env\": null\n",
      "    }\n",
      "  ],\n",
      "  \"report\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\",\n",
      "    \"txn_count_for_showing_cost\": 100000,\n",
      "    \"v_shift_w_single_instance\": 0.025,\n",
      "    \"v_shift_w_gt_one_instance\": 0.025\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## getting access to the s3 bucket where endpoints.json for different models resides\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:found information for 0 endpoints in bucket=sagemaker-fmbench-write-121797993273, key=fmbench-claude-ab3/data/models/endpoints.json\n",
      "INFO:__main__:[]\n"
     ]
    }
   ],
   "source": [
    "## Refer to the file path for the endpoint\n",
    "## getting the endpoint as an s3 object from the deployed path\n",
    "try:\n",
    "    endpoint_info_list = json.loads(get_s3_object(config['aws']['bucket'], ENDPOINT_LIST_PATH))\n",
    "    logger.info(f\"found information for {len(endpoint_info_list)} endpoints in bucket={config['aws']['bucket']}, key={ENDPOINT_LIST_PATH}\")\n",
    "    logger.info(json.dumps(endpoint_info_list, indent=2))\n",
    "## if there is no endpoint information, assume that the user is using a bedrock model and send in the endpoint info list as an empty list\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "        logger.warning(f\"Key {ENDPOINT_LIST_PATH} not found in bucket {config['aws']['bucket']}. Using an empty list for endpoints for bedrock models.\")\n",
    "        endpoint_info_list = []\n",
    "    ## raise an error if the model is not deployed as an endpoint via sagemaker/eks or bedrock\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:there are 0 deployed endpoint(s), endpoint_name_list->[]\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "\n",
    "## endpoint information \n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "## function to get inference\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    ## set the latency to 0\n",
    "    latency = 0\n",
    "\n",
    "    ## initializing completion tokens\n",
    "    completion_tokens = None\n",
    "\n",
    "    ## initialize the prompt tokens that are returned as a part of the dict\n",
    "    prompt_tokens = None\n",
    "\n",
    "    try:\n",
    "        # get inference      \n",
    "        resp = predictor.get_prediction(payload) \n",
    "        logger.info(f\"response={resp}\")\n",
    "        ## return the response json with response from the model\n",
    "        response_json = resp['response_json']\n",
    "        logger.info(f\"response_json={response_json}\")\n",
    "        ## return the latency of prediction\n",
    "        latency = resp['latency']\n",
    "        logger.info(f\"latency={latency}\")\n",
    "        ## return the prompt tokens in the input payload\n",
    "        prompt_tokens = resp['prompt_tokens']\n",
    "        logger.info(f\"prompt_tokens={prompt_tokens}\")\n",
    "        ## return the completion tokens as a part of the model prediction output\n",
    "        completion_tokens = resp['completion_tokens']\n",
    "        logger.info(f\"completion_tokens={completion_tokens}\")\n",
    "\n",
    "        # Assign the generated_text value to completion\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens, \n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        \n",
    "        ## log the output of the prediction\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, prompt_tokens = {prompt_tokens}, completion_tokens={completion_tokens}, latency={latency:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asynchronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: Dict, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## initializing the inference_spec\n",
    "    inference_spec = None\n",
    "\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    logger.info(f\"endpoint info found is: {ep_info}\")\n",
    "\n",
    "    ## if the endpoint info list is not empty, the deployed model is a sagemaker endpoint and contains the endpoint config as a dict\n",
    "    if ep_info != []:\n",
    "        ## get the endpoint name from the dict created\n",
    "        ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "        ## get the inference spec based on the model type\n",
    "        inference_spec = experiment.get(\"inference_spec\")\n",
    "        logger.info(f\"experiment name={experiment['name']}, ep_name={ep_name}, inference_spec={inference_spec}\")\n",
    "\n",
    "    ## case to handle if the model is not a sagemaker endpoint and coming from the bedrock predictor file\n",
    "    elif (ep_info == []) and (experiment['inference_script'] == 'bedrock_predictor.py'):\n",
    "        ## record the bedrock model id that is converted into a REST API URL in the predictor function\n",
    "        ep_name = experiment['ep_name']\n",
    "        logger.info(f\"experiment name={experiment['name']}, custom bring your own ep_name={ep_name}\")\n",
    "\n",
    "    ## if neither are the case, then skip the endpoint since it is not found from either sagemaker or bedrock\n",
    "    else:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "\n",
    "    # create predictor objects\n",
    "    # Proceed with deployment as before\n",
    "    # Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "    scripts_dir = Path(pkg_resources.files('fmbench'), 'scripts')\n",
    "    logger.info(f\"Using fmbench.scripts directory: {scripts_dir}\")\n",
    "\n",
    "    # Ensure the scripts directory exists\n",
    "    scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    module_name = Path(experiment['inference_script']).stem\n",
    "    logger.info(f\"script provided for inference from this model is --> {module_name}\")\n",
    "    script_path = scripts_dir / f\"{module_name}.py\"\n",
    "    logger.info(f\"script path is --> {script_path}\")\n",
    "\n",
    "    # Check and proceed with local script\n",
    "    if not script_path.exists():\n",
    "        logger.error(f\"script {script_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Deploying using local code: {script_path}\")\n",
    "\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(script_path))\n",
    "    inference_module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = inference_module\n",
    "    spec.loader.exec_module(inference_module)\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return inference_module.create_predictor(ep_name, inference_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Construct the full S3 file path\n",
    "        s3_file_path = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        logger.info(f\"s3 path where the payload files are being read from -> {s3_file_path}\")\n",
    "\n",
    "        # Read the payload file from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=config['aws']['bucket'], Key=s3_file_path)\n",
    "            payload_file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            # Create a payload list by processing each line\n",
    "            payload_list = [create_payload_dict(jline, experiment) for jline in payload_file_content.splitlines()]\n",
    "            logger.info(f\"read from s3://{config['aws']['bucket']}/{s3_file_path}, contains {len(payload_list)} lines\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file from S3: {e}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Current time recorded while running this experiment is 2024-03-23 15:46:31.292450+00:00..... deployed models are going to start inferences...\n",
      "INFO:__main__:endpoint info found is: []\n",
      "INFO:__main__:experiment name=claude-instant-v1, custom bring your own ep_name=anthropic.claude-instant-v1\n",
      "INFO:__main__:Using fmbench.scripts directory: /Users/madhurpt/Documents/foundation-model-benchmarking-tool-10/src/fmbench/scripts\n",
      "INFO:__main__:script provided for inference from this model is --> bedrock_predictor\n",
      "INFO:__main__:script path is --> /Users/madhurpt/Documents/foundation-model-benchmarking-tool-10/src/fmbench/scripts/bedrock_predictor.py\n",
      "INFO:__main__:Deploying using local code: /Users/madhurpt/Documents/foundation-model-benchmarking-tool-10/src/fmbench/scripts/bedrock_predictor.py\n",
      "INFO:bedrock_predictor:__init__ self._predictor=<botocore.client.BedrockRuntime object at 0x28f711410>\n",
      "INFO:__main__:there are 1 combinations of [(1, 'payload_en_1000-2000.jsonl')] to run\n",
      "INFO:__main__:s3 path where the payload files are being read from -> fmbench-claude-ab3/data/prompts/payload_en_1000-2000.jsonl\n",
      "INFO:__main__:read from s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "INFO:__main__:creating combinations for concurrency=1, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "INFO:__main__:after only retaining chunks of length 1, we have 15 chunks, previously we had 15 chunks\n",
      "INFO:__main__:there are 1 for {'name': 'claude-instant-v1', 'model_id': 'anthropic.claude-instant-v1', 'model_version': '*', 'model_name': 'claude-instant-v1', 'ep_name': 'anthropic.claude-instant-v1', 'instance_type': 'ClaudeInstant-ODT', 'image_uri': None, 'deploy': False, 'instance_count': 1, 'deployment_script': None, 'inference_script': 'bedrock_predictor.py', 'inference_spec': {'split_input_and_parameters': False}, 'payload_files': ['payload_en_1000-2000.jsonl'], 'concurrency_levels': [1], 'env': None}\n",
      "INFO:__main__:e_idx=1/1, chunk_index=1/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1339\n",
      "INFO:bedrock_predictor:Claude completion tokens: 94\n",
      "INFO:__main__:response={'response_json': {'completion': \" No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is a village in Wales, which is part of the United Kingdom. Qaleh-Ye Sahar is a village in Iran. The passages provide context that Rhosgoch is located on the island of Anglesey in Wales, while Qaleh-Ye Sahar is a village in Iran's Khuzestan Province.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is a village in Wales, which is part of the United Kingdom. Qaleh-Ye Sahar is a village in Iran. The passages provide context that Rhosgoch is located on the island of Anglesey in Wales, while Qaleh-Ye Sahar is a village in Iran's Khuzestan Province.\"}, 'latency': 2.2888075829978334, 'prompt_tokens': 1339, 'completion_tokens': 94}\n",
      "INFO:__main__:response_json={'completion': \" No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is a village in Wales, which is part of the United Kingdom. Qaleh-Ye Sahar is a village in Iran. The passages provide context that Rhosgoch is located on the island of Anglesey in Wales, while Qaleh-Ye Sahar is a village in Iran's Khuzestan Province.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" No, Rhosgoch and Qaleh-Ye Sahar are not located in the same country. Rhosgoch is a village in Wales, which is part of the United Kingdom. Qaleh-Ye Sahar is a village in Iran. The passages provide context that Rhosgoch is located on the island of Anglesey in Wales, while Qaleh-Ye Sahar is a village in Iran's Khuzestan Province.\"}\n",
      "INFO:__main__:latency=2.2888075829978334\n",
      "INFO:__main__:prompt_tokens=1339\n",
      "INFO:__main__:completion_tokens=94\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1339, completion_tokens=94, latency=2.2888\n",
      "INFO:__main__:e_idx=1/1, chunk_index=2/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1932\n",
      "INFO:bedrock_predictor:Claude completion tokens: 47\n",
      "INFO:__main__:response={'response_json': {'completion': \" I don't have enough information in the provided context passages to determine whether Guy Arvely Dolsin or Altuğ Çelikbilek is younger. None of the passages mention their ages or which one is older.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" I don't have enough information in the provided context passages to determine whether Guy Arvely Dolsin or Altuğ Çelikbilek is younger. None of the passages mention their ages or which one is older.\"}, 'latency': 1.6497906659933506, 'prompt_tokens': 1932, 'completion_tokens': 47}\n",
      "INFO:__main__:response_json={'completion': \" I don't have enough information in the provided context passages to determine whether Guy Arvely Dolsin or Altuğ Çelikbilek is younger. None of the passages mention their ages or which one is older.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" I don't have enough information in the provided context passages to determine whether Guy Arvely Dolsin or Altuğ Çelikbilek is younger. None of the passages mention their ages or which one is older.\"}\n",
      "INFO:__main__:latency=1.6497906659933506\n",
      "INFO:__main__:prompt_tokens=1932\n",
      "INFO:__main__:completion_tokens=47\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1932, completion_tokens=47, latency=1.6498\n",
      "INFO:__main__:e_idx=1/1, chunk_index=3/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1154\n",
      "INFO:bedrock_predictor:Claude completion tokens: 42\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages describe tributaries of Crișul Alb or Crișul Repede rivers in Romania.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages describe tributaries of Crișul Alb or Crișul Repede rivers in Romania.'}, 'latency': 1.2983396670024376, 'prompt_tokens': 1154, 'completion_tokens': 42}\n",
      "INFO:__main__:response_json={'completion': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages describe tributaries of Crișul Alb or Crișul Repede rivers in Romania.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, Gut (Crișul Alb) and Gepiș are both located in Romania. All the passages describe tributaries of Crișul Alb or Crișul Repede rivers in Romania.'}\n",
      "INFO:__main__:latency=1.2983396670024376\n",
      "INFO:__main__:prompt_tokens=1154\n",
      "INFO:__main__:completion_tokens=42\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1154, completion_tokens=42, latency=1.2983\n",
      "INFO:__main__:e_idx=1/1, chunk_index=4/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1646\n",
      "INFO:bedrock_predictor:Claude completion tokens: 32\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. British Columbia and Alberta are both provinces of Canada.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. British Columbia and Alberta are both provinces of Canada.'}, 'latency': 1.1336507499945583, 'prompt_tokens': 1646, 'completion_tokens': 32}\n",
      "INFO:__main__:response_json={'completion': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. British Columbia and Alberta are both provinces of Canada.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Howard Lake (British Columbia) and North Buck Lake (Alberta) are located in Canada. British Columbia and Alberta are both provinces of Canada.'}\n",
      "INFO:__main__:latency=1.1336507499945583\n",
      "INFO:__main__:prompt_tokens=1646\n",
      "INFO:__main__:completion_tokens=32\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1646, completion_tokens=32, latency=1.1337\n",
      "INFO:__main__:e_idx=1/1, chunk_index=5/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1397\n",
      "INFO:bedrock_predictor:Claude completion tokens: 83\n",
      "INFO:__main__:response={'response_json': {'completion': \" No, Yana Urqu (Q'Umirqucha) and Puka Rumi are not located in the same country. Passage 1 indicates that Puka Rumi is located in Peru. Passage 2 indicates that Yana Urqu (Q'Umirqucha) is located in Peru. However, these passages describe two different mountains named Yana Urqu located in different areas of Peru.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" No, Yana Urqu (Q'Umirqucha) and Puka Rumi are not located in the same country. Passage 1 indicates that Puka Rumi is located in Peru. Passage 2 indicates that Yana Urqu (Q'Umirqucha) is located in Peru. However, these passages describe two different mountains named Yana Urqu located in different areas of Peru.\"}, 'latency': 3.42721141698712, 'prompt_tokens': 1397, 'completion_tokens': 83}\n",
      "INFO:__main__:response_json={'completion': \" No, Yana Urqu (Q'Umirqucha) and Puka Rumi are not located in the same country. Passage 1 indicates that Puka Rumi is located in Peru. Passage 2 indicates that Yana Urqu (Q'Umirqucha) is located in Peru. However, these passages describe two different mountains named Yana Urqu located in different areas of Peru.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" No, Yana Urqu (Q'Umirqucha) and Puka Rumi are not located in the same country. Passage 1 indicates that Puka Rumi is located in Peru. Passage 2 indicates that Yana Urqu (Q'Umirqucha) is located in Peru. However, these passages describe two different mountains named Yana Urqu located in different areas of Peru.\"}\n",
      "INFO:__main__:latency=3.42721141698712\n",
      "INFO:__main__:prompt_tokens=1397\n",
      "INFO:__main__:completion_tokens=83\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1397, completion_tokens=83, latency=3.4272\n",
      "INFO:__main__:e_idx=1/1, chunk_index=6/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1746\n",
      "INFO:bedrock_predictor:Claude completion tokens: 59\n",
      "INFO:__main__:response={'response_json': {'completion': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was killed by Veerappan or his gang members or by Tamil Nadu police on December 8, 2002 at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was killed by Veerappan or his gang members or by Tamil Nadu police on December 8, 2002 at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\"}, 'latency': 1.4532077909971122, 'prompt_tokens': 1746, 'completion_tokens': 59}\n",
      "INFO:__main__:response_json={'completion': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was killed by Veerappan or his gang members or by Tamil Nadu police on December 8, 2002 at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\", 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': \" Based on the passages provided, Parimala Nagappa's husband H. Nagappa was killed by Veerappan or his gang members or by Tamil Nadu police on December 8, 2002 at Changadi forest area near M. M. Hills bordering the state of Tamil Nadu.\"}\n",
      "INFO:__main__:latency=1.4532077909971122\n",
      "INFO:__main__:prompt_tokens=1746\n",
      "INFO:__main__:completion_tokens=59\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1746, completion_tokens=59, latency=1.4532\n",
      "INFO:__main__:e_idx=1/1, chunk_index=7/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1373\n",
      "INFO:bedrock_predictor:Claude completion tokens: 75\n",
      "INFO:__main__:response={'response_json': {'completion': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. Passage 1 describes Huernia as \"The genus Huernia\" and provides details about the genus. Passage 2 describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and also provides details about the genus.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. Passage 1 describes Huernia as \"The genus Huernia\" and provides details about the genus. Passage 2 describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and also provides details about the genus.'}, 'latency': 1.9736374579952098, 'prompt_tokens': 1373, 'completion_tokens': 75}\n",
      "INFO:__main__:response_json={'completion': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. Passage 1 describes Huernia as \"The genus Huernia\" and provides details about the genus. Passage 2 describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and also provides details about the genus.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Yes, both Dictyosperma and Huernia are described as genera in the passages. Passage 1 describes Huernia as \"The genus Huernia\" and provides details about the genus. Passage 2 describes Dictyosperma as \"Dictyosperma is a monotypic genus of flowering plant\" and also provides details about the genus.'}\n",
      "INFO:__main__:latency=1.9736374579952098\n",
      "INFO:__main__:prompt_tokens=1373\n",
      "INFO:__main__:completion_tokens=75\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1373, completion_tokens=75, latency=1.9736\n",
      "INFO:__main__:e_idx=1/1, chunk_index=8/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1598\n",
      "INFO:bedrock_predictor:Claude completion tokens: 79\n",
      "INFO:__main__:response={'response_json': {'completion': ' Both Javier Frana and Thomaz Koch were tennis players. The passages indicate that Thomaz Koch was a former Brazilian tennis player who was a quarterfinalist at several Grand Slam tournaments. Javier Frana was a former tennis player from Argentina who won the 1996 French Open mixed doubles title and had a highest singles ATP ranking of world No. 30. So both were professional tennis players.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Both Javier Frana and Thomaz Koch were tennis players. The passages indicate that Thomaz Koch was a former Brazilian tennis player who was a quarterfinalist at several Grand Slam tournaments. Javier Frana was a former tennis player from Argentina who won the 1996 French Open mixed doubles title and had a highest singles ATP ranking of world No. 30. So both were professional tennis players.'}, 'latency': 3.9371663339989027, 'prompt_tokens': 1598, 'completion_tokens': 79}\n",
      "INFO:__main__:response_json={'completion': ' Both Javier Frana and Thomaz Koch were tennis players. The passages indicate that Thomaz Koch was a former Brazilian tennis player who was a quarterfinalist at several Grand Slam tournaments. Javier Frana was a former tennis player from Argentina who won the 1996 French Open mixed doubles title and had a highest singles ATP ranking of world No. 30. So both were professional tennis players.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Both Javier Frana and Thomaz Koch were tennis players. The passages indicate that Thomaz Koch was a former Brazilian tennis player who was a quarterfinalist at several Grand Slam tournaments. Javier Frana was a former tennis player from Argentina who won the 1996 French Open mixed doubles title and had a highest singles ATP ranking of world No. 30. So both were professional tennis players.'}\n",
      "INFO:__main__:latency=3.9371663339989027\n",
      "INFO:__main__:prompt_tokens=1598\n",
      "INFO:__main__:completion_tokens=79\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1598, completion_tokens=79, latency=3.9372\n",
      "INFO:__main__:e_idx=1/1, chunk_index=9/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1743\n",
      "INFO:bedrock_predictor:Claude completion tokens: 21\n",
      "INFO:__main__:response={'response_json': {'completion': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.'}, 'latency': 1.1107988330040826, 'prompt_tokens': 1743, 'completion_tokens': 21}\n",
      "INFO:__main__:response_json={'completion': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Charlotte of Mecklenburg-Strelitz was the wife of King George III of Great Britain.'}\n",
      "INFO:__main__:latency=1.1107988330040826\n",
      "INFO:__main__:prompt_tokens=1743\n",
      "INFO:__main__:completion_tokens=21\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1743, completion_tokens=21, latency=1.1108\n",
      "INFO:__main__:e_idx=1/1, chunk_index=10/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1539\n",
      "INFO:bedrock_predictor:Claude completion tokens: 17\n",
      "INFO:__main__:response={'response_json': {'completion': ' Mary Queen of Scots was beheaded at Fotheringhay Castle in England.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Mary Queen of Scots was beheaded at Fotheringhay Castle in England.'}, 'latency': 1.0643735409976216, 'prompt_tokens': 1539, 'completion_tokens': 17}\n",
      "INFO:__main__:response_json={'completion': ' Mary Queen of Scots was beheaded at Fotheringhay Castle in England.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Mary Queen of Scots was beheaded at Fotheringhay Castle in England.'}\n",
      "INFO:__main__:latency=1.0643735409976216\n",
      "INFO:__main__:prompt_tokens=1539\n",
      "INFO:__main__:completion_tokens=17\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1539, completion_tokens=17, latency=1.0644\n",
      "INFO:__main__:e_idx=1/1, chunk_index=11/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1695\n",
      "INFO:bedrock_predictor:Claude completion tokens: 98\n",
      "INFO:__main__:response={'response_json': {'completion': ' A cowcatcher would typically be found on the front of a locomotive or train engine. Specifically, a cowcatcher is a V-shaped device at the front of a locomotive or other large vehicle that is designed to clear debris from the tracks like cows or other obstacles without derailing the vehicle. It helps push objects out of the way to prevent major damage to the train. So in short, a cowcatcher would be found on the front end of a train engine or locomotive.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' A cowcatcher would typically be found on the front of a locomotive or train engine. Specifically, a cowcatcher is a V-shaped device at the front of a locomotive or other large vehicle that is designed to clear debris from the tracks like cows or other obstacles without derailing the vehicle. It helps push objects out of the way to prevent major damage to the train. So in short, a cowcatcher would be found on the front end of a train engine or locomotive.'}, 'latency': 4.517527457996039, 'prompt_tokens': 1695, 'completion_tokens': 98}\n",
      "INFO:__main__:response_json={'completion': ' A cowcatcher would typically be found on the front of a locomotive or train engine. Specifically, a cowcatcher is a V-shaped device at the front of a locomotive or other large vehicle that is designed to clear debris from the tracks like cows or other obstacles without derailing the vehicle. It helps push objects out of the way to prevent major damage to the train. So in short, a cowcatcher would be found on the front end of a train engine or locomotive.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' A cowcatcher would typically be found on the front of a locomotive or train engine. Specifically, a cowcatcher is a V-shaped device at the front of a locomotive or other large vehicle that is designed to clear debris from the tracks like cows or other obstacles without derailing the vehicle. It helps push objects out of the way to prevent major damage to the train. So in short, a cowcatcher would be found on the front end of a train engine or locomotive.'}\n",
      "INFO:__main__:latency=4.517527457996039\n",
      "INFO:__main__:prompt_tokens=1695\n",
      "INFO:__main__:completion_tokens=98\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1695, completion_tokens=98, latency=4.5175\n",
      "INFO:__main__:e_idx=1/1, chunk_index=12/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1421\n",
      "INFO:bedrock_predictor:Claude completion tokens: 31\n",
      "INFO:__main__:response={'response_json': {'completion': ' Triple sec is described as having a heavily orange and citrus flavor, similar to orange hard candy or orange peel. Specifically, it tastes strongly of oranges.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Triple sec is described as having a heavily orange and citrus flavor, similar to orange hard candy or orange peel. Specifically, it tastes strongly of oranges.'}, 'latency': 1.199854292004602, 'prompt_tokens': 1421, 'completion_tokens': 31}\n",
      "INFO:__main__:response_json={'completion': ' Triple sec is described as having a heavily orange and citrus flavor, similar to orange hard candy or orange peel. Specifically, it tastes strongly of oranges.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' Triple sec is described as having a heavily orange and citrus flavor, similar to orange hard candy or orange peel. Specifically, it tastes strongly of oranges.'}\n",
      "INFO:__main__:latency=1.199854292004602\n",
      "INFO:__main__:prompt_tokens=1421\n",
      "INFO:__main__:completion_tokens=31\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1421, completion_tokens=31, latency=1.1999\n",
      "INFO:__main__:e_idx=1/1, chunk_index=13/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1918\n",
      "INFO:bedrock_predictor:Claude completion tokens: 108\n",
      "INFO:__main__:response={'response_json': {'completion': ' John Glenn became the oldest person to fly in space at age 77 in 1998 according to the passage. Specifically, the passage states: \"On Oct. 29, 1998, at the age of 77, he [John Glenn] became the oldest person ever to fly in space, a hero of the Cold War stepping forward this time as a role model for the geriatric generation -- and to satisfy an unending yearning.\" John Glenn suited up again as an astronaut in 1998 at age 77, becoming the oldest person to fly in space.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' John Glenn became the oldest person to fly in space at age 77 in 1998 according to the passage. Specifically, the passage states: \"On Oct. 29, 1998, at the age of 77, he [John Glenn] became the oldest person ever to fly in space, a hero of the Cold War stepping forward this time as a role model for the geriatric generation -- and to satisfy an unending yearning.\" John Glenn suited up again as an astronaut in 1998 at age 77, becoming the oldest person to fly in space.'}, 'latency': 2.1863788339978782, 'prompt_tokens': 1918, 'completion_tokens': 108}\n",
      "INFO:__main__:response_json={'completion': ' John Glenn became the oldest person to fly in space at age 77 in 1998 according to the passage. Specifically, the passage states: \"On Oct. 29, 1998, at the age of 77, he [John Glenn] became the oldest person ever to fly in space, a hero of the Cold War stepping forward this time as a role model for the geriatric generation -- and to satisfy an unending yearning.\" John Glenn suited up again as an astronaut in 1998 at age 77, becoming the oldest person to fly in space.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' John Glenn became the oldest person to fly in space at age 77 in 1998 according to the passage. Specifically, the passage states: \"On Oct. 29, 1998, at the age of 77, he [John Glenn] became the oldest person ever to fly in space, a hero of the Cold War stepping forward this time as a role model for the geriatric generation -- and to satisfy an unending yearning.\" John Glenn suited up again as an astronaut in 1998 at age 77, becoming the oldest person to fly in space.'}\n",
      "INFO:__main__:latency=2.1863788339978782\n",
      "INFO:__main__:prompt_tokens=1918\n",
      "INFO:__main__:completion_tokens=108\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1918, completion_tokens=108, latency=2.1864\n",
      "INFO:__main__:e_idx=1/1, chunk_index=14/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1910\n",
      "INFO:bedrock_predictor:Claude completion tokens: 54\n",
      "INFO:__main__:response={'response_json': {'completion': ' The Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball for furthering the values of honor, respect and integrity in the game. The awards are named after Dr. James Naismith, the inventor of basketball.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball for furthering the values of honor, respect and integrity in the game. The awards are named after Dr. James Naismith, the inventor of basketball.'}, 'latency': 2.4999954579980113, 'prompt_tokens': 1910, 'completion_tokens': 54}\n",
      "INFO:__main__:response_json={'completion': ' The Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball for furthering the values of honor, respect and integrity in the game. The awards are named after Dr. James Naismith, the inventor of basketball.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' The Naismith Legacy Award and Doc Naismith Award are presented to honor individuals in the sport of basketball for furthering the values of honor, respect and integrity in the game. The awards are named after Dr. James Naismith, the inventor of basketball.'}\n",
      "INFO:__main__:latency=2.4999954579980113\n",
      "INFO:__main__:prompt_tokens=1910\n",
      "INFO:__main__:completion_tokens=54\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1910, completion_tokens=54, latency=2.5000\n",
      "INFO:__main__:e_idx=1/1, chunk_index=15/15\n",
      "INFO:__main__:processing chunk with concurrency=1\n",
      "INFO:bedrock_predictor:REST API URL created based on bedrock model id 'anthropic.claude-instant-v1' : https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-instant-v1/invoke\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:bedrock_predictor:CLAUDE PROMPT TOKENS: 1939\n",
      "INFO:bedrock_predictor:Claude completion tokens: 67\n",
      "INFO:__main__:response={'response_json': {'completion': ' John Chilcot led the inquiry into the UK\\'s involvement in the Iraq war. The passage mentions that \"Chairman of UK Iraq inquiry John Chilcot says the basis for the decisions that lead Great Britain into the Iraq War were \"far from satisfactory.\"\" I\\'ve used one sentence as you instructed to keep the answer concise.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' John Chilcot led the inquiry into the UK\\'s involvement in the Iraq war. The passage mentions that \"Chairman of UK Iraq inquiry John Chilcot says the basis for the decisions that lead Great Britain into the Iraq War were \"far from satisfactory.\"\" I\\'ve used one sentence as you instructed to keep the answer concise.'}, 'latency': 2.890143500000704, 'prompt_tokens': 1939, 'completion_tokens': 67}\n",
      "INFO:__main__:response_json={'completion': ' John Chilcot led the inquiry into the UK\\'s involvement in the Iraq war. The passage mentions that \"Chairman of UK Iraq inquiry John Chilcot says the basis for the decisions that lead Great Britain into the Iraq War were \"far from satisfactory.\"\" I\\'ve used one sentence as you instructed to keep the answer concise.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:', 'generated_text': ' John Chilcot led the inquiry into the UK\\'s involvement in the Iraq war. The passage mentions that \"Chairman of UK Iraq inquiry John Chilcot says the basis for the decisions that lead Great Britain into the Iraq War were \"far from satisfactory.\"\" I\\'ve used one sentence as you instructed to keep the answer concise.'}\n",
      "INFO:__main__:latency=2.890143500000704\n",
      "INFO:__main__:prompt_tokens=1939\n",
      "INFO:__main__:completion_tokens=67\n",
      "INFO:__main__:get_inference, done, endpoint=anthropic.claude-instant-v1, prompt_tokens = 1939, completion_tokens=67, latency=2.8901\n",
      "INFO:__main__:the claude-instant-v1 ran for 42.53826425000443 seconds......\n",
      "INFO:__main__:metrics json is: {'experiment_name': 'claude-instant-v1', 'concurrency': 1, 'payload_file': 'payload_en_1000-2000.jsonl', 'errors': [], 'successes': 1, 'error_rate': 0.0, 'all_prompts_token_count': 1939, 'prompt_token_count_mean': 1939.0, 'prompt_token_throughput': 653.3, 'all_completions_token_count': 67, 'completion_token_count_mean': 67.0, 'completion_token_throughput': 22.57, 'transactions': 1, 'transactions_per_second': 0.34, 'transactions_per_minute': 20, 'latency_mean': 2.890143500000704}\n",
      "INFO:bedrock_predictor:pricing dict: {'input-per-1k-tokens': 0.0008}\n",
      "INFO:bedrock_predictor:input per 1k token pricing: 0.0008\n",
      "INFO:bedrock_predictor:pricing dict: {'output-per-1k-tokens': 0.0024}\n",
      "INFO:bedrock_predictor:output per 1k token pricing: 0.0024\n",
      "INFO:__main__:the rate for running claude-instant-v1 running on ClaudeInstant-ODT for 42.53826425000443 is $0.0017120000000000002....\n",
      "INFO:__main__:experiment=1/1, name=claude-instant-v1, duration=42.54 seconds, done\n",
      "INFO:__main__:experiment durations:      experiment_name      instance_type duration_in_seconds  cost\n",
      "0  claude-instant-v1  ClaudeInstant-ODT               42.54  0.00\n",
      "INFO:__main__:Summary for cost of instance per endpoint per run saved to s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/endpoint_per_instance_per_run_costs.csv\n",
      "INFO:__main__:total cost of all experiments: $0.0\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "# Initializing the total model instance cost to 0\n",
    "total_model_instance_cost: int = 0\n",
    "\n",
    "## To keep track of the cost for all model endpoints - this is the same for sagemaker and bedrock\n",
    "cost_data = []\n",
    "\n",
    "## To keep track of the experiment durations and the time it takes for the model endpoint to be in service to calculate cost association\n",
    "experiment_durations = []  \n",
    "\n",
    "## start the timer before the start of inferences\n",
    "current_time = datetime.now(timezone.utc)\n",
    "logger.info(f\"Current time recorded while running this experiment is {current_time}..... deployed models are going to start inferences...\")\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    experiment_start_time = time.perf_counter()  # Start timer for the experiment\n",
    "\n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                metrics_json = json.dumps(metrics, indent=2)\n",
    "                metrics_file_name = f\"{time.time()}.json\"\n",
    "                metrics_s3_path = os.path.join(METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "                write_to_s3(metrics_json, config['aws']['bucket'], \"\", METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    response_json = json.dumps(r, indent=2)\n",
    "                    response_file_name = f\"{time.time()}.json\"\n",
    "                    response_s3_path = os.path.join(METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "                    write_to_s3(response_json, config['aws']['bucket'], \"\", METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "    \n",
    "    ## initializing the experiment cost to 0\n",
    "    exp_cost = 0\n",
    "    \n",
    "    # Experiment done, stopping the timer for this given experiment\n",
    "    experiment_end_time = time.perf_counter()\n",
    "\n",
    "    # calculating the duration of this given endpoint inference time\n",
    "    experiment_duration = experiment_end_time - experiment_start_time\n",
    "    logger.info(f\"the {experiment['name']} ran for {experiment_duration} seconds......\")\n",
    "\n",
    "    # calculating the per second cost for this instance type\n",
    "    exp_instance_type: str = experiment['instance_type']\n",
    "    \n",
    "    #cost for this given exp\n",
    "    logger.info(f\"metrics json is: {metrics}\")\n",
    "\n",
    "    ## calculate the cost of run for both sagemaker and bedrock models\n",
    "    exp_cost = predictor.calculate_cost(exp_instance_type, config, experiment_duration, metrics)\n",
    "    logger.info(f\"the rate for running {experiment['name']} running on {exp_instance_type} for {experiment_duration} is ${exp_cost}....\")\n",
    "\n",
    "    ## tracking the total cost\n",
    "    total_model_instance_cost += exp_cost\n",
    "\n",
    "    experiment_durations.append({\n",
    "        'experiment_name': experiment['name'],\n",
    "        'instance_type': exp_instance_type, \n",
    "        'duration_in_seconds': f\"{experiment_duration:.2f}\", \n",
    "        'cost': f\"{exp_cost:.2f}\", \n",
    "    })\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, duration={experiment_duration:.2f} seconds, done\")\n",
    "\n",
    "# After all experiments are done, summarize and optionally save experiment durations along with costs\n",
    "df_durations = pd.DataFrame(experiment_durations)\n",
    "logger.info(f\"experiment durations: {df_durations}\")\n",
    "\n",
    "# Convert the DataFrame to CSV and write it to S3 or wherever you prefer\n",
    "csv_buffer_cost = io.StringIO()\n",
    "df_durations.to_csv(csv_buffer_cost, index=False)\n",
    "experiment_associated_cost = csv_buffer_cost.getvalue()\n",
    "\n",
    "# Assuming write_to_s3() is already defined and configured correctly\n",
    "write_to_s3(experiment_associated_cost, config['aws']['bucket'], \"\", METRICS_DIR, SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE)\n",
    "logger.info(f\"Summary for cost of instance per endpoint per run saved to s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE}\")\n",
    "\n",
    "logger.info(f\"total cost of all experiments: ${sum(df_durations.cost.astype(float))}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fmbench.utils:found 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/per_inference, suffix=.json\n",
      "INFO:fmbench.utils:there are total of 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/per_inference, suffix=.json\n",
      "INFO:__main__:created dataframe of shape (15, 10) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>ContentType</th>\n",
       "      <th>Accept</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Rhosgoch and Qaleh-Ye Sahar are not locat...</td>\n",
       "      <td>1339</td>\n",
       "      <td>94</td>\n",
       "      <td>2.288808</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>I don't have enough information in the provid...</td>\n",
       "      <td>1932</td>\n",
       "      <td>47</td>\n",
       "      <td>1.649791</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, Gut (Crișul Alb) and Gepiș are both loca...</td>\n",
       "      <td>1154</td>\n",
       "      <td>42</td>\n",
       "      <td>1.298340</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, both Howard Lake (British Columbia) and ...</td>\n",
       "      <td>1646</td>\n",
       "      <td>32</td>\n",
       "      <td>1.133651</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Yana Urqu (Q'Umirqucha) and Puka Rumi are...</td>\n",
       "      <td>1397</td>\n",
       "      <td>83</td>\n",
       "      <td>3.427211</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 endpoint_name  \\\n",
       "0  anthropic.claude-instant-v1   \n",
       "1  anthropic.claude-instant-v1   \n",
       "2  anthropic.claude-instant-v1   \n",
       "3  anthropic.claude-instant-v1   \n",
       "4  anthropic.claude-instant-v1   \n",
       "\n",
       "                                              prompt       ContentType  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "\n",
       "             Accept                                         completion  \\\n",
       "0  application/json   No, Rhosgoch and Qaleh-Ye Sahar are not locat...   \n",
       "1  application/json   I don't have enough information in the provid...   \n",
       "2  application/json   Yes, Gut (Crișul Alb) and Gepiș are both loca...   \n",
       "3  application/json   Yes, both Howard Lake (British Columbia) and ...   \n",
       "4  application/json   No, Yana Urqu (Q'Umirqucha) and Puka Rumi are...   \n",
       "\n",
       "   prompt_tokens  completion_tokens   latency    experiment_name  concurrency  \n",
       "0           1339                 94  2.288808  claude-instant-v1            1  \n",
       "1           1932                 47  1.649791  claude-instant-v1            1  \n",
       "2           1154                 42  1.298340  claude-instant-v1            1  \n",
       "3           1646                 32  1.133651  claude-instant-v1            1  \n",
       "4           1397                 83  3.427211  claude-instant-v1            1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_INFERENCE_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "                     s3_files))\n",
    "\n",
    "# Create DataFrame\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fmbench.utils:found 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/per_chunk, suffix=.json\n",
      "INFO:fmbench.utils:there are total of 15 items in bucket=sagemaker-fmbench-write-121797993273, prefix=fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/per_chunk, suffix=.json\n",
      "INFO:__main__:created dataframe of shape (15, 16) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>554.79</td>\n",
       "      <td>94</td>\n",
       "      <td>94.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>24</td>\n",
       "      <td>2.288808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1932</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>1152.01</td>\n",
       "      <td>47</td>\n",
       "      <td>47.0</td>\n",
       "      <td>28.03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>36</td>\n",
       "      <td>1.649791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>1154.0</td>\n",
       "      <td>856.43</td>\n",
       "      <td>42</td>\n",
       "      <td>42.0</td>\n",
       "      <td>31.17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>44</td>\n",
       "      <td>1.298340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1646</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>1394.11</td>\n",
       "      <td>32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>27.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>51</td>\n",
       "      <td>1.133651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1397</td>\n",
       "      <td>1397.0</td>\n",
       "      <td>401.48</td>\n",
       "      <td>83</td>\n",
       "      <td>83.0</td>\n",
       "      <td>23.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>17</td>\n",
       "      <td>3.427211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     experiment_name  concurrency                payload_file errors  \\\n",
       "0  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "1  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "2  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "3  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "4  claude-instant-v1            1  payload_en_1000-2000.jsonl     []   \n",
       "\n",
       "   successes  error_rate  all_prompts_token_count  prompt_token_count_mean  \\\n",
       "0          1         0.0                     1339                   1339.0   \n",
       "1          1         0.0                     1932                   1932.0   \n",
       "2          1         0.0                     1154                   1154.0   \n",
       "3          1         0.0                     1646                   1646.0   \n",
       "4          1         0.0                     1397                   1397.0   \n",
       "\n",
       "   prompt_token_throughput  all_completions_token_count  \\\n",
       "0                   554.79                           94   \n",
       "1                  1152.01                           47   \n",
       "2                   856.43                           42   \n",
       "3                  1394.11                           32   \n",
       "4                   401.48                           83   \n",
       "\n",
       "   completion_token_count_mean  completion_token_throughput  transactions  \\\n",
       "0                         94.0                        38.95             1   \n",
       "1                         47.0                        28.03             1   \n",
       "2                         42.0                        31.17             1   \n",
       "3                         32.0                        27.10             1   \n",
       "4                         83.0                        23.85             1   \n",
       "\n",
       "   transactions_per_second  transactions_per_minute  latency_mean  \n",
       "0                     0.41                       24      2.288808  \n",
       "1                     0.60                       36      1.649791  \n",
       "2                     0.74                       44      1.298340  \n",
       "3                     0.85                       51      1.133651  \n",
       "4                     0.29                       17      3.427211  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_CHUNK_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "                     s3_files))\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:the instance type: ClaudeInstant-ODT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'ContentType', 'Accept', 'completion',\n",
      "       'prompt_tokens', 'completion_tokens', 'latency', 'experiment_name',\n",
      "       'concurrency'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>ContentType</th>\n",
       "      <th>Accept</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Rhosgoch and Qaleh-Ye Sahar are not locat...</td>\n",
       "      <td>1339</td>\n",
       "      <td>94</td>\n",
       "      <td>2.288808</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>I don't have enough information in the provid...</td>\n",
       "      <td>1932</td>\n",
       "      <td>47</td>\n",
       "      <td>1.649791</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, Gut (Crișul Alb) and Gepiș are both loca...</td>\n",
       "      <td>1154</td>\n",
       "      <td>42</td>\n",
       "      <td>1.298340</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>Yes, both Howard Lake (British Columbia) and ...</td>\n",
       "      <td>1646</td>\n",
       "      <td>32</td>\n",
       "      <td>1.133651</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>application/json</td>\n",
       "      <td>application/json</td>\n",
       "      <td>No, Yana Urqu (Q'Umirqucha) and Puka Rumi are...</td>\n",
       "      <td>1397</td>\n",
       "      <td>83</td>\n",
       "      <td>3.427211</td>\n",
       "      <td>claude-instant-v1</td>\n",
       "      <td>1</td>\n",
       "      <td>ClaudeInstant-ODT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 endpoint_name  \\\n",
       "0  anthropic.claude-instant-v1   \n",
       "1  anthropic.claude-instant-v1   \n",
       "2  anthropic.claude-instant-v1   \n",
       "3  anthropic.claude-instant-v1   \n",
       "4  anthropic.claude-instant-v1   \n",
       "\n",
       "                                              prompt       ContentType  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...  application/json   \n",
       "\n",
       "             Accept                                         completion  \\\n",
       "0  application/json   No, Rhosgoch and Qaleh-Ye Sahar are not locat...   \n",
       "1  application/json   I don't have enough information in the provid...   \n",
       "2  application/json   Yes, Gut (Crișul Alb) and Gepiș are both loca...   \n",
       "3  application/json   Yes, both Howard Lake (British Columbia) and ...   \n",
       "4  application/json   No, Yana Urqu (Q'Umirqucha) and Puka Rumi are...   \n",
       "\n",
       "   prompt_tokens  completion_tokens   latency    experiment_name  concurrency  \\\n",
       "0           1339                 94  2.288808  claude-instant-v1            1   \n",
       "1           1932                 47  1.649791  claude-instant-v1            1   \n",
       "2           1154                 42  1.298340  claude-instant-v1            1   \n",
       "3           1646                 32  1.133651  claude-instant-v1            1   \n",
       "4           1397                 83  3.427211  claude-instant-v1            1   \n",
       "\n",
       "       instance_type EndpointName ModelName Image S3Uri  \n",
       "0  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "1  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "2  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "3  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  \n",
       "4  ClaudeInstant-ODT          NaN       NaN   NaN   NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## if the endpoint list contains elements, utilize the sagemaker properties into the df \n",
    "if endpoint_info_list:\n",
    "    df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "    df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "    cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "    print(cols_for_env)\n",
    "    ## record the column of interest for sagemaker endpoint based on the model configs\n",
    "    cols_of_interest = ['experiment_name',\n",
    "                        'instance_type',\n",
    "                        'endpoint.EndpointName',\n",
    "                        'model_config.ModelName',\n",
    "                        'model_config.PrimaryContainer.Image',\n",
    "                        'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "    cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "    df_endpoints = df_endpoints[cols_of_interest]\n",
    "    cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "    df_endpoints.columns = cols_of_interest_renamed\n",
    "\n",
    "## if the endpoint list is empty, create columns specific to the bedrock supported models, which includes the name of the endpoint, experiment name, model name, etc\n",
    "else:\n",
    "    # Create an empty DataFrame with the desired columns\n",
    "    df_endpoints = pd.DataFrame(columns=['experiment_name',\n",
    "                                         'instance_type',\n",
    "                                         'EndpointName',\n",
    "                                         'ModelName',\n",
    "                                         'Image',\n",
    "                                         'S3Uri'])\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "for e, experiment in enumerate(config['experiments']):\n",
    "    experiment_name = experiment['name']\n",
    "    instance_type = experiment['instance_type']\n",
    "    \n",
    "    logger.info(f\"the instance type: {instance_type}\")\n",
    "    # Update the instance_type column in df_results where the EndpointName matches\n",
    "    df_results.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:results s3 path for per inference csv --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/per_inference_request_results.csv\n",
      "INFO:__main__:saved results dataframe of shape=(15, 15) in s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert df_results to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_results.to_csv(csv_buffer, index=False)\n",
    "csv_data_results = csv_buffer.getvalue()\n",
    "results_file_name = config['report']['per_inference_request_file'].format(datetime=date_time)\n",
    "results_s3_path = os.path.join(METRICS_DIR, results_file_name)\n",
    "logger.info(f\"results s3 path for per inference csv --> {results_s3_path}\")\n",
    "write_to_s3(csv_data_results, config['aws']['bucket'], \"\", METRICS_DIR, results_file_name)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in s3://{BUCKET_NAME}/{results_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:the metrics metadata path is saved here --> metadata/metrics_path.txt\n",
      "INFO:__main__:the information on the defined path for results on these metrics are given in this --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33\n"
     ]
    }
   ],
   "source": [
    "# Ensure the metadata directory exists\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "# Path for the metrics_path.txt file\n",
    "metrics_path_file = os.path.join(METADATA_DIR, 'metrics_path.txt')\n",
    "logger.info(f\"the metrics metadata path is saved here --> {metrics_path_file}\")\n",
    "\n",
    "# Write the METRICS_DIR to metrics_path.txt\n",
    "with open(metrics_path_file, 'w') as file:\n",
    "    file.write(METRICS_DIR)\n",
    "\n",
    "## Write this data to S3\n",
    "write_to_s3(METRICS_DIR, config['aws']['bucket'], \"\", DATA_DIR, 'metrics_path.txt')\n",
    "\n",
    "logger.info(f\"the information on the defined path for results on these metrics are given in this --> {METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:df_metrics cols = Index(['experiment_name', 'concurrency', 'payload_file', 'errors', 'successes',\n",
      "       'error_rate', 'all_prompts_token_count', 'prompt_token_count_mean',\n",
      "       'prompt_token_throughput', 'all_completions_token_count',\n",
      "       'completion_token_count_mean', 'completion_token_throughput',\n",
      "       'transactions', 'transactions_per_second', 'transactions_per_minute',\n",
      "       'latency_mean', 'instance_type', 'EndpointName', 'ModelName', 'Image',\n",
      "       'S3Uri'],\n",
      "      dtype='object')\n",
      "INFO:__main__:df_endpoints cols = Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri'],\n",
      "      dtype='object')\n",
      "INFO:__main__:the instance type: ClaudeInstant-ODT\n",
      "INFO:__main__:results s3 path for metrics csv --> fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/all_metrics.csv\n",
      "INFO:__main__:saved metrics results dataframe of shape=(15, 27) in s3://sagemaker-fmbench-write-121797993273/fmbench-claude-ab3/data/metrics/yyyy=2024/mm=03/dd=23/hh=11/mm=33/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"df_metrics cols = {df_metrics.columns}\")\n",
    "logger.info(f\"df_endpoints cols = {df_endpoints.columns}\")\n",
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "for e, experiment in enumerate(config['experiments']):\n",
    "    experiment_name = experiment['name']\n",
    "    instance_type = experiment['instance_type']\n",
    "    \n",
    "    logger.info(f\"the instance type: {instance_type}\")\n",
    "    # Update the instance_type column in df_results where the EndpointName matches\n",
    "    ## get the metrics based on the instance types used from the experiment\n",
    "    df_metrics.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "df_metrics.head()\n",
    "\n",
    "# Convert df_metrics to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_metrics.to_csv(csv_buffer, index=False)\n",
    "csv_data_metrics = csv_buffer.getvalue()\n",
    "metrics_file_name = config['report']['all_metrics_file'].format(datetime=date_time)\n",
    "metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "write_to_s3(csv_data_metrics, config['aws']['bucket'], \"\", METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{config['aws']['bucket']}/{metrics_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
